---
title: "STAT 425 Project Final Report"
author: "Yiming Gao, Yuan Yuan"
date: "2016/12/14"
output: pdf_document
linestretch: 1.4
fontsize: 10.8pt
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r houseing, include=FALSE}
library(data.table)
library(testthat)
library(gridExtra)
library(corrplot)
library(GGally)
library(ggplot2)
library(e1071)
library(dplyr) # data manipulation
library(MASS)
library(lmtest)
library(leaps)
library(Metrics)
library(lars)
library(caret)
library(party)
library(randomForest)
library(xgboost)
library(knitr)
# Visualization
plotHist <- function(data_in, i) {
  data <- data.frame(x=data_in[[i]])
  p <- ggplot(data=data, aes(x=factor(x))) + stat_count() + xlab(colnames(data_in)[i]) + theme_light() + 
    theme(axis.text.x = element_text(angle = 90, hjust =1))
  return (p)
}

doPlots <- function(data_in, fun, ii, ncol=3) {
  pp <- list()
  for (i in ii) {
    p <- fun(data_in=data_in, i=i)
    pp <- c(pp, list(p))
  }
  do.call("grid.arrange", c(pp, ncol=ncol))
}


plotDen <- function(data_in, i){
  data <- data.frame(x=data_in[[i]], price = data_in$price)
  p <- ggplot(data= data) + geom_line(aes(x = x), stat = 'density', size = 1,alpha = 1.0) +
    xlab(paste0((colnames(data_in)[i]), '\n', 'Skewness: ',round(skewness(data_in[[i]], na.rm = TRUE), 2))) + theme_light() 
  return(p)
}

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```
\section{1 Description}
\subsection{1.1 Motivation}
According to the article "U.S. existing home sales decline, prices still strong", the demand for housing will increase due to the millennials, "a demographic group that has seen an improvement in job prospects since the end of the 2007-2009 recession". For the increasing demand in housing, predicting the house price will attract more and more attention. It is important for any actors in the property market to have an estimation for the house they are interested in. Sellers would ask for a price based on the valuation, while buyer would have to consider if that certain real estate is worth for the asked price or not, and how much they would like to pay for it. In this project, We are planning to use dataset containing house sale prices for King County, which includes Seattle from Kaggle. It includes homes sold between May 2014 and May 2015. The main response is the house price, and the predictors are house features including continuous and categorical ones. 

The potential user could be people who are interested in investing real estates, real estate agencies. The data product will help potential users to have a reference when they want to either buy or sell a house, since there are many factors for people to make a final deal. And the negotiation skills of buyers and sellers may also come into play in reality. It is said in the article of "7 Online Tools to Help You Estimate Your Home???s Value" by Teresa Mears that people have to consider the kinds of variables that went into the estimate. The variables used in different models for the data product has been showed for users' consideration. Potential users can also have an idea of the overall housing market in King County through our data product.  

There are many websites that provides house price estimation which are similar to the data product we produced such as Zillow, Redfin, Chase and Bank of America. For example, Redfin is a tool that shows you photos and listing information for the exact variables used to arrive at the value of your home. All of the online tools predict the house price by running through computer models using publicly available data. However, the exact dataset they are using is still private, same as the formulas used in modeling. And the available data also has effects on the accuracy of the prediction. As it is said in the article by Teresa Mears, it is difficult to price a house if it is an outlier. For example, a house is not similar to houses in the data will have less accurate prediction. 

\subsection{1.2 Discussion}
Considering the accuracy of prediction, the data used for making predictions should have similar distribution as the data used for training.  Since it is expected that the data distributions may move over time, deploying a model would be a continuous process. It is usually good to continuously monitor the incoming data and retrain models on new data if new incoming data is deviating vastly from the original data distribution. If it is not easy for monitoring data to detect a change in the data distribution, then a simpler strategy is to train the model periodically, for example, daily, weekly, or monthly. We will choose to retrain our model monthly, since the price of the house does not change frequently. 

The data product should be used for predicting house price in the area of King County, since the data used in training contains only house sale prices for King County. And the prediction error will be larger for houses that are not similar to the houses in the dataset. There would be no lawsuit regarding of our data product, since the house sale data we got from Kaggle is publicly available for use. And it is released under CC0: Public Domain License which means that we can copy, modify, distribute and perform on the data, even for commercial purposes, all without asking permission. 

There is still some improvement to be made. For example, try some transformations to reduce the problems in diagnostics, create more detailed, dynamic plots, or build some ensemble models.

\textbf{The dashboard could be found at:} https://yiminggao.shinyapps.io/STAT425_YimingGao/


\section{2 Exploratory Data Analysis}
\subsection{2.1 Variable Description}
The housing data set has 21613 rows and 19 features with the target feature price. For prediction purpose, I dropped some irrelevant features such as latitute, longitute, zipcode and so on. 

First we view the data, and find that some variables have many zero values, for example, waterfront. In this case, value 1/0 means whether it is waterfront or not. Some variables such as floors, condition and grade, we will treat them as categorical variables in this part, but will convert them to numeric later when we build models. We can gain some insight on the number of houses that were renovated. According to the variable description, if yr_renovated is not zero, then the house was renovated at that year. We can find out that 914 of them were renovated, and the percentage is 4.23%.

Categorical features, numeric features and percentage of renovation are as follows.
```{r, echo=FALSE}
house <- fread("/Users/Yiming/Desktop/425_Project/Raw data/LM/kc_house_data.csv",colClasses = c('condition' = "character", 
                                     'grade' = 'character'))

# categorical variables
cat_var<-names(house)[which(sapply(house, is.character))]
cat_var

# numeric variables
numeric_var <- names(house)[which(sapply(house, is.numeric))]
numeric_var

# Since there are many zeros in the data, we replace them with NA
house$waterfront[house$waterfront == 0] <- NA
house$view[house$view == 0] <- NA
 

# Find out how many houses were renovated and the percentage of them
cat('Percentage of houses renovated is',round(sum(house[,'yr_renovated', with = FALSE]
                                                  != 0)/ dim(house)[1],4)*100, '%.')

```

\subsection{2.2 Training and Test Set}
We will use a training set to discover potentially predictive relationships, and a test set to assess the strength and utility of the predictive relationship. In this report, all the models and predictions are basd on the training and test set (with seed 1234). We choose to randomly separate out 70% of observations as data in our training set.
```{r, echo=FALSE}
# Split the data set into train and test set, with 70% in a training set
set.seed(1234)
sub <- sample(nrow(house), floor(nrow(house) * 0.7))
train <- house[sub, ]
test <- house[-sub, ]

# summary training and testing data sets
cat('Training set has',dim(train)[1],'rows and',dim(train)[2],'columns.')
cat('Testing set has',dim(test)[1],'rows and',dim(test)[2],'columns.')

# Convert charater to factors
train[,(cat_var) := lapply(.SD, as.factor), .SDcols = cat_var]
train_cat <- train[,.SD, .SDcols = cat_var]
train_cont <- train[,.SD, .SDcols = numeric_var]
```

\subsection{2.3 Feature Visualization}
\textbf{- Barplots}

In order to get more insight into the data, we first create barplots for categorical features, indicating the distribution of different levels. As we can see, the majority of houses are on floor 1 or 2, with condition level at 3 or 4, and grade ranges from 6 to 9.

```{r, echo=FALSE,fig.width=9, fig.height = 5,fig.align="center"}
doPlots(train_cat[, 3:6], fun = plotHist, ii = 1:4, ncol = 2)
```

\textbf{- Density Plots}

Density plots of the features indicates that the features are skewed. I only draw the density for target feature \textbf{price} and log-transformed price here. We can see that price before log transformation is highly skewed.
```{r, echo=FALSE,fig.width=9, fig.height = 3.5,fig.align="center"}
p1 = ggplot(data= train_cont) + geom_line(aes(x = train_cont$price), stat = 'density', size = 1,alpha = 1.0) +
  xlab(paste0("price", '\n', 'Skewness: ',round(skewness(train_cont[[1]], na.rm = TRUE), 2))) + theme_light() 
p2 = ggplot(data= train_cont) + geom_line(aes(x = log(train_cont$price)), stat = 'density', size = 1,alpha = 1.0) +
  xlab(paste0("log(price)", '\n', 'Skewness: ',round(skewness(log(train_cont[[1]]), na.rm = TRUE), 2))) + theme_light() 
multiplot(p1,p2, cols = 2)
```

\textbf{- Histograms}

The histograms below show that majority of the houses have 3 or 4 bedrooms and 2.5 bathrooms (full baths and half baths).
```{r, echo=FALSE,fig.width=9, fig.height = 3.5,fig.align="center"}
doPlots(train_cont, fun = plotHist, ii = 2:3, ncol = 2)
```

\textbf{- Correlation Plot}

The plot below show that there are 4 variables are highly correlated with price. However, some of the variables are also highly correlated. We need to discover their relationships by further detailed research.
```{r, echo=FALSE, fig.height = 3.5,fig.align="center"}
# Explore the correlation
correlations <- cor(subset(train_cont, select = -c(waterfront, view)))

# correlations
row_indic <- apply(correlations, 1, function(x) sum(x > 0.3 | x < -0.3) > 1)

correlations<- correlations[row_indic ,row_indic]
corrplot(correlations, method="square")
```

\section{3 Linear Regression}
\subsection{3.1 Full Model}
\textbf{- Model Summary}

We first remove some unnecessary features, and do some data convertion. Then we want to do log transformation on those highly skewed features with skewness larger than 0.75. So we do the transformation on price, sqft\_ living, sqft\_ lot, sqft\_ above, sqft\_ basement, sqft\_ living 15 and sqft\_ lot15.
```{r, echo=FALSE}
house <- read.csv("/Users/Yiming/Desktop/425_Project/Raw data/LM/kc_house_data.csv")

# remove unnecessary columns
house.reg <- house[, -c(1:2, 16:19)]
house.reg$waterfront <- as.factor(house.reg$waterfront)

# Split the data set into train and test set, with 70% in a training set
set.seed(1234)
sub <- sample(nrow(house.reg), floor(nrow(house.reg) * 0.7))
train.reg <- house.reg[sub, ]
test.reg <- house.reg[-sub, ]
numeric_feats <-c(names(house.reg)[1],names(house.reg)[4],names(house.reg)[5],
                  names(house.reg)[11],names(house.reg)[12],names(house.reg)[14],
                  names(house.reg)[15])

# determine skew for each numeric feature
skewed_feats <- sapply(numeric_feats,function(x){skewness(house.reg[[x]],na.rm=TRUE)})

# keep only features that exceed a threshold for skewness
skewed_feats <- skewed_feats[skewed_feats > 0.75]
skewed_feats

# transform excessively skewed features with log(x + 1)
for(x in names(skewed_feats)) {
  train.reg[[x]] <- log(train.reg[[x]] + 1)
  test.reg[[x]] <- log(test.reg[[x]] + 1)
}

```

Since the output of the full model is too lengthy, I will omit it in the report. All predictors excluding sqft\_ lot are statistically significant with p-values less than 0.05. There is a completely linear relationship: sqft\_ living = sqft\_ above + sqft\_ basement, i.e. rank deficiency. However, the result will not be affected. $R^2 = 0.6585$, which is good.

The diagnostic plots of the full model are as follows, they don't seem to show any potential problems, which the assumptions are not been violated.

```{r, echo=FALSE,fig.width=9, fig.height = 6.5,fig.align="center"}
# Our response is a continuous numeric variable, we want a linear model
g.full <- lm(price ~ ., data = train.reg)

layout(matrix(c(1,2,3,4),2,2))
plot(g.full)
```

\textbf{- Prediction}

We make the prediction on the test set, and compute the root mean squared error between actual values and predicted values in package "Metrics". The root mean squared error is 0.3043 for the full model.

```{r, echo=FALSE}
# prediction with the full model
pred.full = predict(g.full, newdata = test.reg)
round(rmse(test.reg$price, pred.full),4) # 0.3043
```

\subsection{3.2 Reduced Model}
\textbf{- Variable Selection}

We use \textbf{AIC} and \textbf{BIC} criteria to determine the "best" linear model. Note that in BIC, we use the penalty of $logm$ where m is the size of the training set, in this case, $m=15129$.

Both AIC and BIC procedure give us same 8 variables: bathrooms, sqft\_ living, waterfront (with value 1), view, grade, yr\_ built, sqft\_ living15 and sqft\_ lot15. Then we will fit a linear model based on those variables.
```{r, echo=FALSE, warning=FALSE}
# Model Selection by AIC & BIC (with leaps package)
b = regsubsets(price~., data = train.reg)
rs = summary(b)
m = dim(train.reg)[1]
msize = 2:14
AIC = m*log(rs$rss/m) + 2*msize
BIC = m*log(rs$rss/m) + msize*log(m)


# AIC model
var.AIC = colnames(rs$which)[rs$which[which.min(AIC),]]

# BIC model
var.BIC = colnames(rs$which)[rs$which[which.min(BIC),]]
var.BIC
```

\textbf{- Reduced Model}

We built the model with 8 predictors selected by AIC and BIC. The model summary and diagnostic plots are as follows. We can see that $R^2$ is 0.6549, meaning 65.49% of the total variation is price can be explained by these variables. All the predictors are statistically significant. Compared to the full model, we can say that this reduced model is adequate enough, especially in term of simplicity.

The diagnostic plots don't show any potential problems either. It means the reduced model is acceptable. Then we compare its prediction strength with the full model.
```{r, echo=FALSE}
g.lm = lm(price ~ bathrooms + sqft_living + waterfront
         + view + grade + yr_built + sqft_living15 + sqft_lot15,data = train.reg)
summary(g.lm) # R^2 = 0.649, doesn't decrease too much compared to full model
```

```{r,echo=FALSE,fig.width=9, fig.height = 6,fig.align="center"}
layout(matrix(c(1,2,3,4),2,2))
plot(g.lm)
```

\textbf{- Prediction}

We perform the prediction on the test data.The root mean squared error is 0.3084 for this reduced model.
```{r, echo=FALSE}
pred.lm = predict(g.lm, newdata = test.reg)
round(rmse(test.reg$price, pred.lm),4)
```

\newpage
\section{4 XGBoost}
\subsection{4.1 Feature Importance}

XGBoost stands for Extreme Gradient Boosting, which can build a model and make predictions. It supports various objective functions, including _regression_, _classification_ and _ranking_. In particular, XGBoost can manage huge dataset very efficiently. That's why we consider using it here.

First we need to transfer dummy variable \textbf{waterfront} into numeric form. We simply transfer them by implementing "as.integer" method. 

We will build the model using the following parameters:

- objective = "reg:linear": we will train a linear regression model

- max.depth=4: the depth of trees is 4

- nthread = 2: the number of cpu threads we are going to use

- nround = 10: there will be ten passes on the data
```{r, echo=FALSE}
# transform dummy variables into numeric form
train.reg$waterfront <- as.integer(train.reg$waterfront)
test.reg$waterfront <- as.integer(test.reg$waterfront)

# transform the dataset into sparse matrix
train.tmp <- as.matrix(train.reg, rownames.force=NA)
test.tmp <- as.matrix(test.reg, rownames.force=NA)
train.xgb <- as(train.tmp, "sparseMatrix")
test.xgb <- as(test.tmp, "sparseMatrix")

train.xgb <- xgb.DMatrix(data = train.xgb[,2:15], label = train.xgb[,"price"])
```
```{r, warning=FALSE}
g.xgb <- xgboost(data = train.xgb, max.depth = 4, eta = 1, nthread = 2,
                 nround = 10, objective = "reg:linear")
importance<-xgb.importance(feature_names = names(train.reg)[2:15], 
                             model = g.xgb)
```
```{r}
as.data.frame(importance)[1:10,]
```
```{r, echo=FALSE,fig.width=9, fig.height = 5.5,fig.align="center",warning=FALSE}
# Plot the feature importance matrix
library(Ckmeans.1d.dp)

xgb.plot.importance(importance[1:10,])
```
The feature importance plot shows that in this training set (70% with seed 1234), the first 5 most important features for prediction are building grade (from 1 to 13), sqft\_ living (total square footage of the house), yr\_ built (when the house was built), sqft\_ living15 (the average square footage of the 15 closest houses) and sqft\_ lot15 (lot size of the house).

\subsection{4.2 Prediction}

To measure the model performance, we will make the prediction on the test set based on this XGBoost model, and compute the RMSE. The root mean square error is 0.3005, which is the smallest among models till now. 
```{r, echo=FALSE}
# prediction
pred.xgb = predict(g.xgb, data.matrix(test.xgb[,-1]))
round(rmse(test.reg$price, pred.xgb),4)
```

When I run the XGBoost model, I found that it wouldn't take a long time, especially in comparison with the random forest model which I'll mention later in the report. This is determined by both algorithm and implementation. We have to wait on training models on such a large data set in most cases. The training speed of XGBoost impressed me a lot. We are confident that XGBoost is one of the fastest learning algorithm of gradient boosting algorithm.


\section{5 Random Forest}
Instead of growing a single tree, we grow many, and introduce some sources of randomness. When it comes to problems where the focus is not so much in understanding the data, but in making predictions.

\subsection{5.1 Model Summary}
\textbf{- Gini Importance}

We first create a random forest with 500 trees (with "Randomforest" package). From the output below, we know that the amount of total variance of price explained by the random forest model is 72.49%. 

We also output the importance matrix, in which _IncNodePurity_ is the total decrease in node impurities, measured by the Gini Index from splitting on the variable, averaged over all trees. We reordered the importance by decreasing IncNodePurity. In this case, the first 5 most important features are grade, sqft\_ living, sqft\_ living15, yr\_ built and sqft\_ above. 

Gini importance, is based on Gini gain criterion employed in most traditional classification tree algorithms.

```{r, echo=FALSE}
library(caret)
library(party)
library(randomForest)
obj <- randomForest(price~., data = train.reg,importance=TRUE)
print(obj)

# Gini Importance
imp <- as.data.frame(importance(obj, type=2))
imp$feature <- rownames(imp)
imp <- imp[order(-imp$IncNodePurity),]

data.frame(imp$feature,imp$IncNodePurity)
```

We also plot the reordered feature importance for better understanding.

```{r, echo=FALSE,fig.width=8.5, fig.height = 5,fig.align="center",warning=FALSE}
ggplot(imp, aes(x = reorder(feature,IncNodePurity), y = IncNodePurity))+geom_bar(stat="identity",width = 0.3,fill="coral1")+coord_flip()
```

\textbf{- Permutation Importance}

The permutation importance, on the other hand, is a reliable measure of variable importance for uncorrelated predictors when subsampling without replacement - instead of bootstrap sampling - and unbiased trees are used in the construction of the forest (Strobl et al., 2007b). Such an unbiased tree algorithm is available in the \textbf{party} package. It allows us to make reliable prediction and interpretability, especially for assessing the importance of each predictor in the complex ensemble of trees.

We use the \textbf{cForest} function in party package. Similar to the importance() function in \textbf{randomForest} package, a permutation importance is available via varimp() in \textbf{party}.

In this case, the first 5 most important features picked up by permutation importance is the same as previous, despite of the order.

```{r, echo=FALSE}
obj1 <- cforest(price~.,data=train.reg) #if your predictor variables are highly correlated:

# permutation importance
imp1<-as.data.frame(varimp(obj1))
imp1$feature <- rownames(imp1)
imp1 <- imp1[order(-imp1$`varimp(obj1)`),]

data.frame(imp1$feature,imp1$`varimp(obj1)`)
```

The feature importance plot is as follows.
```{r,echo=FALSE,fig.width=8.5, fig.height = 5,fig.align="center",warning=FALSE}
ggplot(imp1, aes(x = reorder(feature,imp1$`varimp(obj1)`), y = imp1$`varimp(obj1)`))+geom_bar(stat="identity",width = 0.3,fill="coral1")+coord_flip()
```

\subsection{5.2 Prediction}
We make the predictions on test data based on this two random forest models. The RMSE are 0.2771, 0.2858, respectively. Both of them are smaller than any of the RMSE for previous models.

```{r,echo=FALSE, warning=FALSE}
pred.tree = predict(obj, newdata = test.reg)

pred.tree1 = as.vector(predict(obj1, newdata = test.reg))

c(round(rmse(test.reg$price, pred.tree),4),round(rmse(test.reg$price, pred.tree1),4))

```

\section{6 Comparison}
The summary information table of each procedures have been listed below. For XGBoost and Random forest models, they give almost same important features. Note that all the computataion are based on the training set and test set we've set at the very beginning.

We notice that those so-called "black box" models perform better in prediction. Black-box is a term used to identify certain predictive modeling techniques that are not capable of explaining their reasoning. We could justify their strong prediction power in our project.

```{r, results = "asis",echo=FALSE}
a <- c("Full model", "Reduced model", "XGBoost","Random forest (Gini importance)","Random forest (permutation importance)")
b <- c(14,8," "," "," ")
c <- c(0.3043, 0.3084, 0.3005, 0.2771, 0.2858)
my_data <- data.frame(a, b, c)
names(my_data) <- c("Approach", "Number of variables","Root mean square error")
kable(my_data, align = "c", padding=2)
```

\section{7 Work Cited}
Mears, Teresa. "7 Online Tools to Help You Estimate Your Home???s Value." U.S. News, Web. 	Dec. 15, 2016.

Lucia Mutikani. "U.S. existing home sales decline, prices still strong." Reuters, Web. Dec.15, 	2016.

Carolin Strobl, Torsten Hothorn, Achim Zelieis. "Party on! A New, Conditional Variable Importance Measure for Random Forests Available in the party Package." Department of Statistics, University of Munich. Dec.15, 2016.

Some plot functions from: https://www.kaggle.com/notaapple/house-prices-advanced-regression-techniques/detailed-exploratory-data-analysis-using-r
