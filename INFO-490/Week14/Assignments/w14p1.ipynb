{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1206a454e6c628e914177ddb325af6a9",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If you are not using the `Assignments` tab on the course JupyterHub server to read this notebook, read [Activating the assignments tab](https://github.com/lcdm-uiuc/info490-sp17/blob/master/help/act_assign_tab.md).\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_  â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "61e78c4bf3fb68605fb28cd743a75b0d",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Problem 14.1. Spark\n",
    "\n",
    "In this problem, we will perform basic data processing tasks within Spark using the concept of Resilient Distributed Datasets (RDDs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2f7ff2b12ef6fc2b3bb9285e85b15e97",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6c4e35e7e25b945eda489c7245a439f8",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We run Spark in [local mode](http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes) from within our own server Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b89f31e49fc9c87324f97bb9c08b5683",
     "grade": false,
     "grade_id": "sparkcontext",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "57737d53998d02abdcef45ca39f41b0c",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We create a new RDD by reading in the data as a text file. We use the ratings data from [MovieLens](http://grouplens.org/datasets/movielens/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1ee45b253c99ade7600d888249eddf88",
     "grade": false,
     "grade_id": "text_file",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile('/home/data_scientist/data/ml-latest-small/ratings.csv')\n",
    "\n",
    "assert_is_instance(text_file, pyspark.rdd.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4d50a33726e803a10c319e7e2bba5d33",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Part 1\n",
    "- Write a function that creates a new RDD by transforming `text_file` into an RDD with columns of appropriate data types.\n",
    "- The function accepts a `pyspark.rdd.RDD` instance (e.g., `text_file` in the above code cell) and returns another RDD instance, `pyspark.rdd.PipelinedRDD`.\n",
    "- `ratings.csv` contains a header row which should not be included in your output instance. Use the `head` command or otherwise to inspect the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a1029eb43f1b62f034a173819ed25929",
     "grade": false,
     "grade_id": "read_ratings_csv_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def read_ratings_csv(rdd):\n",
    "    '''\n",
    "    Creates an RDD by transforming `ratings.csv`\n",
    "    into columns with appropriate data types.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    rdd = rdd.map(lambda l: l.split(\",\")) \\\n",
    "            .filter(lambda line: 'userId' not in line) \\\n",
    "            .map(lambda p: (int(p[0]), int(p[1]), float(p[2]), int(p[3])))\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6ec0d0d794f477bc768b96a7be2645cd",
     "grade": false,
     "grade_id": "read_ratings_csv_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 16, 4.0, 1217897793), (1, 24, 1.5, 1217895807), (1, 32, 4.0, 1217896246)]\n"
     ]
    }
   ],
   "source": [
    "ratings = read_ratings_csv(text_file)\n",
    "print(ratings.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f35f4f662a3c11120673f5b56104a88f",
     "grade": true,
     "grade_id": "read_ratings_csv_test",
     "locked": true,
     "points": 20,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(ratings, pyspark.rdd.PipelinedRDD)\n",
    "assert_equal(ratings.count(), 105339)\n",
    "assert_equal(len(ratings.first()), 4)\n",
    "assert_equal(\n",
    "    ratings.take(5),\n",
    "    [(1, 16, 4.0, 1217897793),\n",
    "     (1, 24, 1.5, 1217895807),\n",
    "     (1, 32, 4.0, 1217896246),\n",
    "     (1, 47, 4.0, 1217896556),\n",
    "     (1, 50, 4.0, 1217896523)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "df87da206de23c22837babe7f224474e",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Part 2\n",
    "For simplicity, we might want to restrict our analysis to only favorable ratings, which, since the movies are rated on a five-star system, we take to mean ratings greater than three. So\n",
    "\n",
    "- Write a function that selects rows whose rating is greater than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "4c106f1587d11a1cf24ceb2d1a1e5045",
     "grade": false,
     "grade_id": "filter_favorable_ratings_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_favorable_ratings(rdd):\n",
    "    '''\n",
    "    Selects rows whose rating is greater than 3.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.rdd.PipelinedRDD instance.\n",
    "    '''\n",
    "    \n",
    "    # third column: ratings\n",
    "    rdd = rdd.filter(lambda x: x[2] > 3.0)\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7d616abf698d28e0494df5826c368cdc",
     "grade": false,
     "grade_id": "filter_favorable_ratings_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "favorable = filter_favorable_ratings(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "bdb44bf32b24f611446ce5e3d2f3eb33",
     "grade": true,
     "grade_id": "filter_favorable_ratings_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(favorable, pyspark.rdd.PipelinedRDD)\n",
    "assert_equal(favorable.count(), 64160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0f8ce8a58208939c906d18e546f028cc",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Part 3\n",
    "We might also want to select only those movies that have been reviewed by multiple people.\n",
    "\n",
    "- Write a function that returns the number of reviews for a given movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "98f24922fecf09cc089a9d135aa1525a",
     "grade": false,
     "grade_id": "find_n_reviews_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_n_reviews(rdd, movie_id):\n",
    "    '''\n",
    "    Finds the number of reviews for a movie.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    movie_id: An int.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An int.\n",
    "    '''\n",
    "    \n",
    "    # second column: MovieID\n",
    "    n_reviews = rdd.filter(lambda x: x[1] == movie_id).count()\n",
    "    \n",
    "    return n_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "285ee7425162cc4c868edad6edcb1715",
     "grade": false,
     "grade_id": "find_n_reviews_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "n_toy_story = find_n_reviews(favorable, 1)\n",
    "print(n_toy_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d133378c27685096c9b702154ebc148a",
     "grade": true,
     "grade_id": "find_n_review_test",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(n_toy_story, int)\n",
    "\n",
    "test = [find_n_reviews(favorable, n) for n in range(5)]\n",
    "assert_equal(test, [0, 172, 44, 18, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1f1cbc95d6cc8f4d200a9e134233cb69",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "We must stop the SparkContext in order to release the spark resources before existing this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "ec8c059e7ca934a0d1fb3342fc042680",
     "grade": false,
     "grade_id": "sc_stop",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
