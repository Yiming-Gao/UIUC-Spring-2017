{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Social Media: Email\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore email messages as a data source for text analysis. Email is among the oldest forms of social media and have a fairly long history of an application area for text analysis. While an email message might seem fairly simple, an email can actually be quite complicated. Part of this complication arises from the requirements to safely and securely transmit an electronic message through the Internet. This information, which is metadata about the message itself, is generally included in the email header. \n",
    "\n",
    "The majority of the complication, however, arises from the change of emails moving beyond simple textual content to include multiple components within an email message. To handle multiple components, which can include images, documents, or just HTML-styled versions of the text content, email messages must enable multiple parts of a message to be identified and properly encoded/decoded (for example, binary or Unicode data) for safe and secure transmission. \n",
    "\n",
    "In the rest of this Notebook, we first explore reading and parsing a simple email, including both the header, message itself, and the different parts (or payloads) within the message. Next, we develop a text classification pipeline from a public email corpus. Finally, we apply this pipeline to blind emails to quantify the accuracy of our simple classification pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email Text Parsing\n",
    "\n",
    "Python provides built-in support for [processing email messages][pem], which are an often overlooked source of information in data science projects. The library is part of the core Python distribution, and includes support for parsing email messages, as well as sending and receiving emails. For our purpose, we simply need to read in text and create an email `message`, which provides access to the basic email contents. The `message` instance provides access to the email header information as well as any payload data. \n",
    "\n",
    "Normally the payload is the email message, but with multipart messages, like HTML email messages, an email can have multiple payloads. In the next several code cells, we create an email `message` by reading an email from a file (the email should look familiar). We subsequently explore the Python email message interface to extract email headers and the message payload, before grabbing the HTML message for later parsing.\n",
    "\n",
    "First, we read in one demonstration email (that I sent out to a class) and display message headers, which are accessible via dictionary keys. In the second code cell, we explicitly retrieve message header values by using the `msg` dictionary. Note that headers can be repeated (for example, the `Received` header as different mail servers add additional header information as the message is transmitted. Finally, in the third code cell, we directly display part of the email message.\n",
    "\n",
    "-----\n",
    "\n",
    "[pem]: https://docs.python.org/3/library/email.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'Received', 'Received', 'Received', 'Received', 'Received', 'Date', 'To',\n",
      "  'From', 'Subject', 'Message-ID', 'X-Mailer', 'List-Id', 'List-Help',\n",
      "  'X-Course-Id', 'X-Course-Name', 'Precedence', 'X-Auto-Response-Suppress',\n",
      "  'Auto-Submitted', 'Content-Type', 'X-Spam-Reason', 'Return-Path',\n",
      "  'X-MS-Exchange-Organization-AuthSource', 'X-MS-Exchange-Organization-AuthAs',\n",
      "  'X-MS-Exchange-Organization-Antispam-Report',\n",
      "  'X-MS-Exchange-Organization-SCL',\n",
      "  'X-MS-Exchange-Organization-AVStamp-Mailbox', 'MIME-Version']\n"
     ]
    }
   ],
   "source": [
    "# Import email library, policy controls how to process the data\n",
    "import email as em\n",
    "from email import policy\n",
    "\n",
    "# For demonstration purposes, open one good email\n",
    "with open(\"data/ham/prg.eml\") as fin:\n",
    "    msg = em.message_from_file(fin, policy=policy.default)\n",
    "\n",
    "# Enable pretty printing of data structures\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "# Display available header keys, \n",
    "# values can be displayed via dictionary access (next cell)\n",
    "pp.pprint(msg.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To: Robert Brunner <xxxyyy@illinois.edu>\n",
      "From: Robert Brunner <xxxyyy@illinois.edu>\n",
      "Subject: INFO 490 RB2 SP16: Peer Review Grading\n"
     ]
    }
   ],
   "source": [
    "print('To:', msg['to'])\n",
    "print('From:', msg['from'])\n",
    "print('Subject:', msg['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Peer Review Grading\n",
      "by Robert Brunner - Monday, 29 February 2016, 10:22 AM\n",
      "---------------------------------------------------------------------\n",
      "I have received several emails or private Moodle messages regarding peer\n",
      "grading. The basic context seems to be concern about losing a few points\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display subset of entire message\n",
    "print(msg.as_string()[2340:2693])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "After parsing, different components of the email message can be accessed via attributes or methods. This was demonstrated previously by accessing the header values via the `msg` variable, or the message text content via the `as_string` method. These were just two of the available techniques, however, and we can display the entire suite of available access techniques by extracting and printing the contents of the email message variable.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '_add_multipart', '_body_types', '_charset', '_default_type', '_find_body',\n",
      "  '_get_params_preserve', '_headers', '_make_multipart', '_payload',\n",
      "  '_unixfrom', 'add_alternative', 'add_attachment', 'add_header', 'add_related',\n",
      "  'as_bytes', 'as_string', 'attach', 'clear', 'clear_content', 'defects',\n",
      "  'del_param', 'epilogue', 'get', 'get_all', 'get_body', 'get_boundary',\n",
      "  'get_charset', 'get_charsets', 'get_content', 'get_content_charset',\n",
      "  'get_content_disposition', 'get_content_maintype', 'get_content_subtype',\n",
      "  'get_content_type', 'get_default_type', 'get_filename', 'get_param',\n",
      "  'get_params', 'get_payload', 'get_unixfrom', 'is_attachment', 'is_multipart',\n",
      "  'items', 'iter_attachments', 'iter_parts', 'keys', 'make_alternative',\n",
      "  'make_mixed', 'make_related', 'policy', 'preamble', 'raw_items',\n",
      "  'replace_header', 'set_boundary', 'set_charset', 'set_content',\n",
      "  'set_default_type', 'set_param', 'set_payload', 'set_raw', 'set_type',\n",
      "  'set_unixfrom', 'values', 'walk']\n"
     ]
    }
   ],
   "source": [
    "# Print out message methods and attributes\n",
    "pp.pprint([att for att in dir(msg) if '__' not in att])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Email message can contain multiple parts, for example attachments to an email, or both a plain text and HTML-styled version of an email message. To identify if an email has multiple parts, we use the `is_multipart` method, after which we can extract different parts (or _payloads_) from the message. For our demonstration email, both a plain text and HTML-styled version of the text are included in the message (since the email was generated by moodle). In the following three code cells, we extract the plain text and html payloads and use them to display part (for the plain text) and entire message (for HTML form).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data:\n",
      "---------\n",
      " \n",
      "INFO 490 RB2 SP16 -> Forums -> Announcements -> Peer Review Grading\n",
      "https://learn.illinois.edu/mod/forum/discuss.php?d=938611\n",
      "---------------------------------------------------------------------\n",
      "Peer Review Grading\n",
      "by Robert Brunner - Monday, 29 February 2016, 10:22 AM\n",
      "---------------------------------------------------------------------\n",
      "I have received several emails or private Moodle messages regarding peer\n",
      "grading. The basic context seems to be concern about losing a few points\n",
      "from one reviewer. \n",
      "\n",
      "To be clear, we do not intervene in Peer Assessment unless there has ben an\n",
      "egregious violation. An example of this would be someone giving a zero and\n",
      "saying 'The code does not run' when it clearly does run. Or 'The code is\n",
      "empty' when the assignment was completed. Losing a few points occasionally\n",
      "will not lower your grade. And other than write the best code you possibly\n",
      "can and document it thoroughly, we do not have any advice!\n"
     ]
    }
   ],
   "source": [
    "# Display part of the message\n",
    "\n",
    "if msg.is_multipart() == True:\n",
    "    data = msg.get_payload(0)\n",
    "    html=msg.get_payload(1)\n",
    "\n",
    "    print(\"Text Data:\\n---------\\n\", data.get_content()[:941])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html><head>\n",
       "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"></head>\n",
       "<body id=\"email\">\n",
       "\n",
       "<div class=\"navbar\"><a target=\"_blank\" href=\"https://urldefense.proofpoint.com/v2/url?u=https-3A__learn.illinois.edu_course_view.php-3Fid-3D15109&amp;d=BQMFaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=O_6ckC2-W1REgY_jBF9kJ25-Hp-9sYdc1-HuZG5JGv8&amp;m=gYTUNA_nqJH7sLR31Nt0DbtPC08xlWozFkhx4rjmB9M&amp;s=m5T6oKzMak6YY0JWwdJLRbtHekX2plOBZVUzElIkoR8&amp;e=\">INFO 490 RB2 SP16</a> » <a target=\"_blank\" href=\"https://urldefense.proofpoint.com/v2/url?u=https-3A__learn.illinois.edu_mod_forum_index.php-3Fid-3D15109&amp;d=BQMFaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=O_6ckC2-W1REgY_jBF9kJ25-Hp-9sYdc1-HuZG5JGv8&amp;m=gYTUNA_nqJH7sLR31Nt0DbtPC08xlWozFkhx4rjmB9M&amp;s=bA6Bc25b5EUiaV1CepFrRg3vjtHgoIv2kIeTmTPXjuM&amp;e=\">Forums</a> » <a target=\"_blank\" href=\"https://urldefense.proofpoint.com/v2/url?u=https-3A__learn.illinois.edu_mod_forum_view.php-3Ff-3D109618&amp;d=BQMFaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=O_6ckC2-W1REgY_jBF9kJ25-Hp-9sYdc1-HuZG5JGv8&amp;m=gYTUNA_nqJH7sLR31Nt0DbtPC08xlWozFkhx4rjmB9M&amp;s=iQ6fSH3TuLYp-awgLY9ZRt3g-ivSKSQgIlbsCBXn5rE&amp;e=\">Announcements</a> » <a target=\"_blank\" href=\"https://urldefense.proofpoint.com/v2/url?u=https-3A__learn.illinois.edu_mod_forum_discuss.php-3Fd-3D938611&amp;d=BQMFaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=O_6ckC2-W1REgY_jBF9kJ25-Hp-9sYdc1-HuZG5JGv8&amp;m=gYTUNA_nqJH7sLR31Nt0DbtPC08xlWozFkhx4rjmB9M&amp;s=H6IS8ugW_i7ZJ0gbpiK9kt50LABnyJdFAJltftJkD04&amp;e=\">Peer Review Grading</a></div><table border=\"0\" cellpadding=\"3\" cellspacing=\"0\" class=\"forumpost\"><tr class=\"header\"><td width=\"35\" valign=\"top\" class=\"picture left\"><a href=\"https://urldefense.proofpoint.com/v2/url?u=https-3A__learn.illinois.edu_user_view.php-3Fid-3D1861-26course-3D15109&amp;d=BQMFaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=O_6ckC2-W1REgY_jBF9kJ25-Hp-9sYdc1-HuZG5JGv8&amp;m=gYTUNA_nqJH7sLR31Nt0DbtPC08xlWozFkhx4rjmB9M&amp;s=QuPMNXl9dczRUqitLIgdH7V0LWer93xdz8q1L7lw5gE&amp;e=\"><img src=\"https://learn.illinois.edu/pluginfile.php/3519/user/icon/bootstrap_atlas/f2?rev=1\" alt=\"Picture of Robert Brunner\" title=\"Picture of Robert Brunner\" class=\"userpicture\" width=\"35\" height=\"35\"></a></td><td class=\"topic starter\"><div class=\"subject\">Peer Review Grading</div><div class=\"author\">by <a href=\"https://urldefense.proofpoint.com/v2/url?u=https-3A__learn.illinois.edu_user_view.php-3Fid-3D1861-26course-3D15109&amp;d=BQMFaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=O_6ckC2-W1REgY_jBF9kJ25-Hp-9sYdc1-HuZG5JGv8&amp;m=gYTUNA_nqJH7sLR31Nt0DbtPC08xlWozFkhx4rjmB9M&amp;s=QuPMNXl9dczRUqitLIgdH7V0LWer93xdz8q1L7lw5gE&amp;e=\">Robert Brunner</a> - Monday, 29 February 2016, 10:22 AM</div></td></tr><tr><td class=\"left side\" valign=\"top\">&nbsp;</td><td class=\"content\"><p>I have received several emails or private Moodle messages regarding peer grading. The basic context seems to be concern about losing a few points from one reviewer.</p>\n",
       "<p>To be clear, we do not intervene in Peer Assessment unless there has ben an egregious violation. An example of this would be someone giving a zero and saying 'The code does not run' when it clearly does run. Or 'The code is empty' when the assignment was completed. Losing a few points occasionally will not lower your grade. And other than write the best code you possibly can and document it thoroughly, we do not have any advice!</p>\n",
       "<p>The point of peer review is two-fold:</p>\n",
       "<p>1) You get a chance to see how your peers completed an assignment. In some case, you might learn how to improve your own code, learn new coding tricks, or see ways that your coding style could improve.</p>\n",
       "<p>2) You have your code reviewed by your peers and obtain comments from them that (hopefully) will be constructive and help you improve your work. In some cases, a review might grade you lower than you like, or in seeming contrast to others.</p>\n",
       "<p>Learning to deal with situations like this is part of the course. You will (probably) experience many of these situations later in your career, either via an interview (which is a form of peer assessment), in the workplace with pair coding or code reviews, or just via general interactions at conferences or public meetings. And if you ever release code online, expect comments and criticisms!</p>\n",
       "<p>Finally, please remember that this is a large online course, which Edward and I are building as the course is being offered. Please refrain from frivolous questions or peer assessment regrade requests; otherwise, we will be forced to invoke point reductions:</p>\n",
       "<p>https://github.com/UI-DataScience/info490-sp16/blob/master/orientation/syllabus.md#point-reductions</p>\n",
       "<p>and also the special point reduction under the Peer Review section:</p>\n",
       "<p>https://github.com/UI-DataScience/info490-sp16/blob/master/orientation/syllabus.md#peer-review</p>\n",
       "<p>Having said this, please understand we encourage valid questions; we do not want to stifle the learning process.</p>\n",
       "<p>Robert</p>\n",
       "<p>&nbsp;</p><div class=\"commands\"><a target=\"_blank\" href=\"https://urldefense.proofpoint.com/v2/url?u=https-3A__learn.illinois.edu_mod_forum_post.php-3Freply-3D2477776&amp;d=BQMFaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=O_6ckC2-W1REgY_jBF9kJ25-Hp-9sYdc1-HuZG5JGv8&amp;m=gYTUNA_nqJH7sLR31Nt0DbtPC08xlWozFkhx4rjmB9M&amp;s=XC2jTGNGGVT3o48qTZ-pxUM40PJOnKjwYYQ9T1ImYqI&amp;e=\">Reply</a></div><div class=\"link\"><a target=\"_blank\" href=\"https://urldefense.proofpoint.com/v2/url?u=https-3A__learn.illinois.edu_mod_forum_discuss.php-3Fd-3D938611-23p2477776&amp;d=BQMFaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=O_6ckC2-W1REgY_jBF9kJ25-Hp-9sYdc1-HuZG5JGv8&amp;m=gYTUNA_nqJH7sLR31Nt0DbtPC08xlWozFkhx4rjmB9M&amp;s=D60VY-nQ1oQLDrx3HrjnX5vRN--IgpP43fyX60Sp_Gw&amp;e=\">See this post in context</a></div></td></tr></table>\n",
       "\n",
       "<hr><div class=\"mdl-align unsubscribelink\"><a href=\"https://urldefense.proofpoint.com/v2/url?u=https-3A__learn.illinois.edu_mod_forum_index.php-3Fid-3D15109&amp;d=BQMFaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=O_6ckC2-W1REgY_jBF9kJ25-Hp-9sYdc1-HuZG5JGv8&amp;m=gYTUNA_nqJH7sLR31Nt0DbtPC08xlWozFkhx4rjmB9M&amp;s=bA6Bc25b5EUiaV1CepFrRg3vjtHgoIv2kIeTmTPXjuM&amp;e=\">Change your forum digest preferences</a></div></body>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab content, as decoded text. Display as HTML\n",
    "from IPython.display import HTML\n",
    "HTML(html.get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can also interact with a payload, both to obtain the payload data or to enable further processing of the payload (e.g., decoding). In the following cell, we demonstrate this by extracting the payload parameters and a subset of the text version of the HTML payload.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload parameters: \n",
      " [('text/html', ''), ('charset', 'UTF-8')]\n",
      "--------------------\n",
      "<html><head>\n",
      "<meta http-equiv=3D\"Content-Type\" content=3D\"text/html; charset=3Dutf-8\"></=\n",
      "head>\n",
      "<body id=3D\"email\">\n"
     ]
    }
   ],
   "source": [
    "# Display email parameters\n",
    "print('Payload parameters: \\n', html.get_params())\n",
    "print(20*'-')\n",
    "\n",
    "# Get raw payload, undecoded\n",
    "print(html.get_payload()[:115])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we read an email message from a file, parsed the text, and extracted header information and payload content. Now that you have run the Notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Try reading other header content, such as 'Date', 'Message-ID', 'X-Mailer', and 'Precedence'. Do these values align with the message source (which you can see by viewing the file contents at the Unix command line)?\n",
    "2. Try reading one of the other emails in the data directory for this week (you can use the JupyterHub server to browse directories and open the files, or view them directly on github). Did anything change?\n",
    "3. Save one of your own emails (from your mail client) and open it with this notebook. Can you parse and view the content?\n",
    "\n",
    "-----\n",
    "\n",
    "## Email Classification\n",
    "\n",
    "To demonstrate using email data in a text analysis project, we will build a [_spam_][sc] classification pipeline to identify emails that we do not wish to receive. For training data, we will use a public corpus of good (_ham_) and bad (_spam_) emails collected by an open-source spam classification tool, known as [spam assassin][sa]. Each email is stored as a separate file in a directory that classifies the type of email. To access these data, we iterate through each file in the two directories (`ham` and `spam`)\n",
    "\n",
    "One issue when performing text analysis are the memory requirements of dealing with large text data sets. Given the shared nature of this JupyterHub server, each Docker container has a limited amount of memory. As a result, we restrict the data size in this Notebook to only the first `max_files` emails in each folder. By default, this value is set to `500`, which should enable the notebook to work effectively on the JupyterHub server. But you can, of course, change this value up and down to study the effects of changing the quantity of training and testing data. \n",
    "\n",
    "-----\n",
    "[sc]: https://en.wikipedia.org/wiki/Email_spam\n",
    "[sa]: https://spamassassin.apache.org/publiccorpus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://spamassassin.apache.org/publiccorpus/readme.html\n",
    "# Grabbed spam_2 and easyham_2\n",
    "\n",
    "import os\n",
    "\n",
    "mypath = '/home/data_scientist/data/email/'\n",
    "\n",
    "ham = []\n",
    "spam = []\n",
    "\n",
    "# Max number of files to read\n",
    "max_files = 500\n",
    "\n",
    "# Read in good (ham) emails\n",
    "for root, dirs, files in os.walk(os.path.join(mypath, 'ham')):\n",
    "    for count, file in enumerate(files):\n",
    "    \n",
    "        # To control memory usage, we limit the number of files\n",
    "        if count >= max_files:\n",
    "            break\n",
    "            \n",
    "        with open(os.path.join(root, file), encoding='ISO-8859-1') as fin:\n",
    "            msg = em.message_from_file(fin, policy=policy.default)\n",
    "            for part in msg.walk():\n",
    "                if part.get_content_type() == 'text/plain':\n",
    "                    data = part.get_payload(None, decode=True)\n",
    "\n",
    "            ham.append(data.decode(encoding='ISO-8859-1'))\n",
    "\n",
    "# Read in bad (spam) emails\n",
    "for root, dirs, files in os.walk(os.path.join(mypath, 'spam')):\n",
    "    for count, file in enumerate(files):\n",
    "        \n",
    "        # To control memory usage, we limit the number of files\n",
    "        if count >= max_files:\n",
    "            break\n",
    "           \n",
    "        with open(os.path.join(root, file), encoding='ISO-8859-1') as fin:\n",
    "            msg = em.message_from_file(fin, policy=policy.default)\n",
    "            for part in msg.walk():\n",
    "                if part.get_content_type() == 'text/plain':\n",
    "                    data = part.get_payload(None, decode=True)\n",
    "\n",
    "            spam.append(data.decode(encoding='ISO-8859-1'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "To apply scikit learn, we myst convert the lists of emails to numpy arrays. In the next cell we create arrays from these two email lists. We also create label arrays, before printing out the number of messages in each category and the overall storage requirements for each array. To reduce memory requirements, in the second cell we instruct the IPython kernel to delete the `ham` and `spam` python variables and to also remove their values from the IPython cache.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Good Emails, requiring  170.5 Mbs.\n",
      "500 Bad Emails, requiring   75.9 Mbs.\n"
     ]
    }
   ],
   "source": [
    "# For text analysis, we need numpy arrays.\n",
    "# Covnert the text lists to numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "pos_emails = np.array(ham)\n",
    "neg_emails = np.array(spam) \n",
    "\n",
    "# Create label arrays\n",
    "pos_labels = np.ones(pos_emails.shape[0])\n",
    "neg_labels = np.zeros(neg_emails.shape[0])\n",
    "\n",
    "# Display email counts, and memory usage\n",
    "pbytes = pos_emails.nbytes / 1024**2\n",
    "nbytes = neg_emails.nbytes / 1024**2\n",
    "\n",
    "print('{0} Good Emails, requiring {1:6.1f} Mbs.'.format(pos_emails.shape[0], pbytes))\n",
    "print('{0} Bad Emails, requiring {1:6.1f} Mbs.'.format(neg_emails.shape[0], nbytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tell Python to delete ham and spam lists\n",
    "# %xdel removes from IPYthon cache as well\n",
    "\n",
    "%xdel ham\n",
    "%xdel spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "While we could use the `test_train_split` method in scikit learn, in the next cell, we create four new arrays from our email data and labels. The following cell deletes the original two numpy email arrays to reduce memory requirements. This approach gives us greater control of our memory usage, but does limit our ability to easily rerun the pipeline with a different quantity of training data (by deleting the numpy arrays, we now need to re-run almost the entire Notebook).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We split positive/negative emails into two groups test/train each. \n",
    "# This value must be less than max_files.\n",
    "split_value = 300\n",
    "\n",
    "# We combine neg and positive into four arrays.\n",
    "x_train = np.concatenate((pos_emails[:split_value], \n",
    "                          neg_emails[:split_value]), axis = 0)\n",
    "\n",
    "x_test = np.concatenate((pos_emails[split_value:],\n",
    "                         neg_emails[split_value:]), axis = 0)\n",
    "\n",
    "y_train = np.concatenate((pos_labels[:split_value], \n",
    "                          neg_labels[:split_value]), axis = 0)\n",
    "\n",
    "y_test = np.concatenate((pos_labels[split_value:],\n",
    "                         neg_labels[split_value:]), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove the original two numpy arrays\n",
    "\n",
    "%xdel pos_emails\n",
    "%xdel neg_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We now have our training and testing data and labels. As demonstrated previously, we can create a text analysis pipeline to tokenize the data before applying a simple Naive Bayes classifier. As demonstrated by the following code cell, we obtain reasonable results, even with a limited number of training emails.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Ham       0.98      0.93      0.95       200\n",
      "       Spam       0.93      0.98      0.95       200\n",
      "\n",
      "avg / total       0.95      0.95      0.95       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nw perform classification, via pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "# Lowercase, bigrams, stop words.\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,2), \\\n",
    "                cv__lowercase=True)\n",
    "\n",
    "pclf.fit(x_train, y_train)\n",
    "y_pred = pclf.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = ['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Blind Testing\n",
    "\n",
    "While the above performance is impressive, the real test of our simple pipeline is its application to completely new emails. The github repository for this week includes a small number of emails that have been labeled by the instructors. The good emails are emails sent out by moodle, while the bad emails are spam messages that made it through the campus filters.\n",
    "\n",
    "To apply our pipeline to these emails, we first need to read these messages. Since, at least some of these messages, are multipart, we need to carefully process the text data. First, we must read the files using the appropriate encoding. Second, we must grab the text payload or else we will be classifying a message based in part on HTML markup. Finally, we directly classify each email by using the pipeline after it has been processed to simplify memory management.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham File Processing:\n",
      "--------------------\n",
      "kmdb.eml classified as spam\n",
      "prg.eml classified as ham\n",
      "pvc.eml classified as spam\n",
      "rw6.eml classified as spam\n",
      "rw7.eml classified as spam\n",
      "w7.eml classified as ham\n",
      "--------------------\n",
      "Spam File Processing:\n",
      "--------------------\n",
      "add.eml classified as spam\n",
      "cnn.eml classified as spam\n",
      "lcm.eml classified as ham\n",
      "mws.eml classified as ham\n",
      "prs.eml classified as spam\n",
      "vsfih.eml classified as ham\n"
     ]
    }
   ],
   "source": [
    "mypath = 'data'\n",
    "\n",
    "print('Ham File Processing:')\n",
    "print(20*'-')\n",
    "for root, dirs, files in os.walk(os.path.join(mypath, 'ham')):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file), encoding='ISO-8859-1') as fin:\n",
    "            msg = em.message_from_file(fin, policy=policy.default)\n",
    "            for part in msg.walk():\n",
    "                if part.get_content_type() == 'text/plain':\n",
    "                    data = part.get_payload(None, decode=True)\n",
    "                lst = []\n",
    "                lst.append(data.decode(encoding='ISO-8859-1'))\n",
    "                value = pclf.predict(np.array(lst))\n",
    "                if value[0] == 0:\n",
    "                    my_pred = 'spam'\n",
    "                else:\n",
    "                    my_pred = 'ham'\n",
    "\n",
    "            print('{0} classified as {1}'.format(file, my_pred))\n",
    "\n",
    "print(20*'-')          \n",
    "print('Spam File Processing:')\n",
    "print(20*'-')\n",
    "for root, dirs, files in os.walk(os.path.join(mypath, 'spam')):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file), encoding='ISO-8859-1') as fin:\n",
    "            msg = em.message_from_file(fin, policy=policy.default)\n",
    "            for part in msg.walk():\n",
    "                if part.get_content_type() == 'text/plain':\n",
    "                    data = part.get_payload(None, decode=True)\n",
    "                lst = []                    \n",
    "                lst.append(data.decode(encoding='ISO-8859-1'))\n",
    "                value = pclf.predict(np.array(lst))\n",
    "                if value[0] == 0:\n",
    "                    my_pred = 'spam'\n",
    "                else:\n",
    "                    my_pred = 'ham'\n",
    "\n",
    "            print('{0} classified as {1}'.format(file, my_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we developed and applied a simple Naive Bayes spam classifier. Now that you have run the Notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the split_value, both to lower and higher values. How does the validation sample perform? How does our blind data classification perform?\n",
    "2. Change parameters in the classification pipeline, for example, the n-grams, the max number of features, stemming, or parameters in the Bayesian classifier. How does the validation sample perform? How does our blind data classification perform?\n",
    "3. Try increasing the number of emails read, perhaps in increments of fifty, and see how the results change (for example, change `max_files` to 550, 600, ..., 750).  You can also modify the code to extract different emails, by using the if statement to only read in files in a specified range (like 500 to 1000).\n",
    "4. Try employing a different classifier, such as Linear SVC with regularization. Do the results change?\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
