{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Social Media: Web\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "With the explosive growth of the world wide web, a number of data sets have been _published_ by simply posting them online. In some cases, these data can be directly accessed via an API. However, in many cases, especially for small, hand-crafted data, they are presented in HTML-styled text. As a result, a data scientist is expected to be able to obtain and process web-accessible data. In this IPython Notebook, we explore web-accessible data by first programmatically acquiring web-accessible data before processing it to generate new results. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Structured Text Parsing\n",
    "\n",
    "To parse structured text, like an XML or an HTML document, we can use the Python [Beautiful Soup][bs] library. This library uses an XML/HTML parser to build a DOM tree, and Beautiful Soup then provides traversal methods to access and modify the DOM for a specific document. Beautiful Soup has been extremely popular for the ease with which it allows web scraping, for example, you can pull data out of an HTML table. But it is more powerful than this, as it allows you to easily parse and manipulate any XML document.\n",
    "\n",
    "To use Beautiful Soup, we first need to import the library, and then create a BeautifulSoup object that provides access to the parsed data. Document elements, like `body` or `table` are directly accessed from the parsed tree; and element attributes or data can be easily extracted, deleted, or replaced. If required, new data can also be added to an existing document, allowing for the dynamic creation of a new document. These capabilities are demonstrated in the following code cells.\n",
    "\n",
    "-----\n",
    "[bs]: http://www.crummy.com/software/BeautifulSoup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Webpage we wish to explore\n",
    "url = 'https://courses.illinois.edu/schedule/2016/spring/INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://courses.illinois.edu/schedule/2016/spring/INFO width=800 height=400>     </iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src={0} width=800 height=400>     </iframe>'.format(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab the webpage as HTML\n",
    "\n",
    "import requests\n",
    "page = requests.get(url)\n",
    "\n",
    "html = page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we used the `requests` library to programmatically obtain a webpage. In the rest of this Notebook, we will use this webpage, and linked webpages to build an application that processes web-accessible data. Now that you have run the Notebook, change using different URLs, either from the Course Explorer, or other website. Ensure you can retrieve the web data, and learn to explore other types of web-accessible data. \n",
    "\n",
    "We now turn to the BeautifulSoup parsing library to process the web-accessible data. We first extract part of the text content.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <title>\n",
      "   Course Explorer\n",
      "  </title>\n"
     ]
    }
   ],
   "source": [
    "# We use BeautofulSoup version 4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Now lets print out the start of the HTMl file\n",
    "print(soup.prettify()[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Most modern web browsers can provide a parsed view of a webpage, as shown below. You can use google to learn how to do this with your specific web browser.\n",
    "\n",
    "![image](images/html-table.png)\n",
    "  \n",
    "As shown in the preceding image, the Webpage is structured with specific components that can are hierarchically arranged. The specific example shown in the image is an HTML table that contains the course information. In this case, we will want to programmatically find this table, and subsequently extract the table data. We can use BeautifulSoup to do all of this. First, we find all tables, and display the beginning of the first table (since there is only one in this case) to find identifying tags to more easily find this table (which we can do by using the table `id` attribute).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document contains 1 HTML Table(s).\n"
     ]
    }
   ],
   "source": [
    "tables = soup.find_all('table')\n",
    "\n",
    "print('Document contains {0} HTML Table(s).'.format(len(tables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table class=\"table table-striped table-bordered table-condensed\" id=\"default-dt\">\n",
      " <thead>\n",
      "  <tr>\n",
      "   <th>\n",
      "    COURSE NUMBER\n",
      "   </th>\n",
      "   <th>\n",
      "    COURSE TITLE\n",
      "   </th>\n",
      "  </tr>\n",
      " </thead>\n"
     ]
    }
   ],
   "source": [
    "print(tables[0].prettify()[:185])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "We can use BeautifulSoup to find the table directly now by using the `id` attribute. This approach is recommended, since these HTML pages are almost assuredly generated by a program. As a result, the tables holding our data of interest will probably be constructed in the same manner, enabling our application to operate on any course webpage.\n",
    "\n",
    "Once we have found the correct table, we can grab every row with data (via the `tr` element) before extracting the data (via the `td` elements). BeautifulSoup allows us to find every occurrence and then iterate through the results. Finally, we generate and display a text table containing the relevant information.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tbody>\n",
      " <tr>\n",
      "  <td>\n",
      "   INFO 102\n",
      "  </td>\n",
      "  <td>\n",
      "   <a href=\"/schedule/2016/spring/INFO/102\">\n",
      "    Little Bits to Big Ideas\n",
      "   </a>\n",
      "  </td>\n",
      " </tr>\n",
      " <tr>\n",
      "  <td>\n",
      "   INFO 199\n",
      "  </td>\n",
      "  <td>\n",
      "   <a href=\"/schedule/2016/spring/INFO/199\">\n",
      "    Undergraduate Open Seminar\n",
      "   </a>\n",
      "  </td>\n",
      " </tr>\n",
      " <tr>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tb = soup.find(id='default-dt').tbody\n",
    "\n",
    "print(tb.prettify()[:290])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course  : Description\n",
      "----------------------------------------\n",
      "INFO 102: Little Bits to Big Ideas\n",
      "INFO 199: Undergraduate Open Seminar\n",
      "INFO 202: Social Aspects Info Tech\n",
      "INFO 303: Writing Across Media\n",
      "INFO 325: Social Media and Global Change\n",
      "INFO 326: New Media, Culture & Society\n",
      "INFO 390: Special Topics\n",
      "INFO 399: Individual Study\n",
      "INFO 490: Special Topics\n",
      "INFO 491: Ugrad Bioinformatics Seminar\n",
      "INFO 500: Orientation Seminar\n",
      "INFO 510: Research Practicum\n",
      "INFO 590: Advanced Special Topics\n",
      "INFO 591: Grad Bioinformatics Seminar\n",
      "INFO 597: Individual Study\n",
      "INFO 599: Thesis Research\n"
     ]
    }
   ],
   "source": [
    "print('{0:8s}: {1:s}'.format('Course', 'Description'))\n",
    "print(40*'-')\n",
    "for row in tb.find_all('tr'):\n",
    "    tdx = [val for val in row.find_all('td')]\n",
    "    course = tdx[0].contents[0].strip()\n",
    "    name = tdx[1].a.contents[0].strip()\n",
    "    print('{0:8s}: {1:s}'.format(course, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Processing Web Hierarchies\n",
    "\n",
    "Section information is listed on a separate page, for example, [INFO 102][is], which provides information about the location, instructor, and date/time for each section. A portion of this webpage is shown below:\n",
    "\n",
    "![INFO 102 Sections](images/info102-sections.png)\n",
    "\n",
    "-----\n",
    "\n",
    "Looking at this webpage, one would think the data are once again in a table. Thus to grab the data, we can employ the same approach we used for scraping the courses. Note that over the next few code cells we use a single course, INFO 102, along with its webpage to explore how to best scrape this information.\n",
    "\n",
    "-----\n",
    "[is]: https://courses.illinois.edu//schedule/2016/spring/INFO/102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# URL for our course's webpage\n",
    "course_url = 'https://courses.illinois.edu/schedule/2016/spring/INFO/102'\n",
    "\n",
    "# Grab HTML from webpage\n",
    "html = requests.get(course_url).content\n",
    "\n",
    "#Parse HTML\n",
    "new_soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document contains 2 HTML Table(s).\n"
     ]
    }
   ],
   "source": [
    "# Grab the tables\n",
    "tables = new_soup.find_all('table')\n",
    "\n",
    "print('Document contains {0} HTML Table(s).'.format(len(tables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "<thead>\n",
      "<tr>\n",
      "<th>Course</th>\n",
      "<th>Section</th>\n",
      "<th>CRN</th>\n",
      "<th>Date</th>\n",
      "<th>Day</th>\n",
      "<th>Start Time</th>\n",
      "<th>End Time</th>\n",
      "<th>Room</th>\n",
      "<th>Exam Type</th>\n",
      "</tr>\n",
      "</thead>\n",
      "--------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Only one table, so display its header and grab its body content\n",
    "print(20*'-')\n",
    "print(tables[0].thead)\n",
    "print(20*'-')\n",
    "print(tables[0].tbody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The body of the only table in this webpage is empty, thus how do we grab the data that is displayed? In this case, a more careful perusal of the webpage source (either via a browser tool or by displaying the entire parsed HTML), demonstrates that the data is encoded as a JSON document in a JavaScript variable. The JavaScript is run when the page is generated, to complete the table. For example, the following screenshot demonstrates this JavaScript variable:\n",
    "\n",
    "![JavaScript JSON variable](images/info102-javascript.png)\n",
    "\n",
    "-----\n",
    "\n",
    "To obtain these data, we will first need to parse the webpage to grab the `script` element. Next, we will parse the value of the `sectionDataObj` JavaScript variable to extract out the JSON data. Finally, we will pull the relevant information from the JSON data. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<script type=\"text/javascript\">\n",
      " var sectionDataObj = [{\"status\":\"<span class=\\\"hide\\\">5<\\/span><span class=\\\"sr-only\\\">section unknown<\\/span><img src=\\\"\\/static\\/images\\/sectionUnknown.png\\\" title=\\\"Unknown\\\" alt=\\\"Unknown\\\"\\/>\",\"crn\":\"63226\",\"type\":\"<div class=\\\"app-meeting\\\">Laboratory<\\/div>\",\"section\":\"<div class=\\\"app-meeting\\\">AB1<\\/div>\",\"time\":\"<span class=\\\"hide\\\">1600<\\/span><div class=\\\"app-meeting\\\">04:00PM - 05:50PM<\\/div>\",\"day\":\"<div class=\\\"app-meeting\\\">T      <\\/div>\",\"location\":\"<div class=\\\"app-meeting\\\">G7 Foreign Languages Building<\\/div>\",\"instructor\":\"<div class=\\\"app-meeting\\\">Padua, D<br \\/><\\/div>\",\"availability\":\"UNKNOWN\",\"credit\":null,\"sectionTitle\":null,\"sectionDescription\":null,\"courseDescription\":null,\"sectionDegreeNotes\":null,\"courseDegreeNotes\":\"Quant Reasoning I course.\",\"specialApproval\":\"\",\"approvalCode\":null,\"sectionFee\":\"\",\"sectionDateRange\":null,\"courseDateRange\":null,\"partOfTerm\":\"1\",\"info\":\"Registration will be restricted to officially declared Informatics minors until Nov. 16th at noon.\",\"corequest\":null,\"restricted\n"
     ]
    }
   ],
   "source": [
    "script_tag = new_soup.find(type='text/javascript')\n",
    "\n",
    "print(script_tag.prettify()[:1076])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Now the we have the script element, we can extract the relevant data from the contents of the `script` element. To do this, we first grab the element's contents, and use a regular expression to find the JSON data. Since the JSON data is enclosed between `{` and `}`, we build a regular expression that finds a group of characters `( ... )` between these curly braces `(\\{ ... \\})`. The simplest approach is to indicate we will match all characters except the closing curly brace by using `[^}]`. We have to escape the opening and ending curly braces, since they would be otherwise parsed differently by the regular expression engine. In the end, our final regular expression is `r'(\\{[^}]+\\})'`.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status\":\"<span class=\\\"hide\\\">5<\\/span><span class=\\\"sr-only\\\">section unknown<\\/span><img src=\\\"\\/static\\/images\\/sectionUnknown.png\\\" title=\\\"Unknown\\\" alt=\\\"Unknown\\\"\\/>\",\"crn\":\"63226\",\"type\":\"<div class=\\\"app-meeting\\\">Laboratory<\\/div>\",\"section\":\"<div class=\\\"app-meeting\\\">AB1<\\/div>\",\"time\":\"<span class=\\\"hide\\\">1600<\\/span><div class=\\\"app-meeting\\\">04:00PM - 05:50PM<\\/div>\",\"day\":\"<div class=\\\"app-meeting\\\">T      <\\/div>\",\"location\":\"<div class=\\\"app-meeting\\\">G7 Foreign Languages Building<\\/div>\",\"instructor\":\"<div class=\\\"app-meeting\\\">Padua, D<br \\/><\\/div>\",\"availability\":\"UNKNOWN\",\"credit\":null,\"sectionTitle\":null,\"sectionDescription\":null,\"courseDescription\":null,\"sectionDegreeNotes\":null,\"courseDegreeNotes\":\"Quant Reasoning I course.\",\"specialApproval\":\"\",\"approvalCode\":null,\"sectionFee\":\"\",\"sectionDateRange\":null,\"courseDateRange\":null,\"partOfTerm\":\"1\",\"info\":\"Registration will be restricted to officially declared Informatics minors until Nov. 16th at noon.\",\"corequest\":null,\"restricted\":\"\"}\n"
     ]
    }
   ],
   "source": [
    "script_txt = script_tag.contents[0]\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = re.compile(r'(\\{[^}]+\\})')\n",
    "\n",
    "match = re.search(pattern, script_txt)\n",
    "\n",
    "if match:\n",
    "    print(match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "The data extracted by the regular expression group is a string that hold the JSON data. To extract out the relevant information, we need to convert this string into a JSON document. After which we can access each element by using a dictionary key. For example, the following code cells create a JSON document, prints out the JSON keys, and extracts the `instructor` value.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['instructor', 'status', 'sectionDateRange', 'sectionFee', 'crn', 'availability', 'courseDegreeNotes', 'time', 'info', 'day', 'sectionDescription', 'courseDescription', 'type', 'location', 'section', 'courseDateRange', 'credit', 'restricted', 'corequest', 'approvalCode', 'specialApproval', 'sectionDegreeNotes', 'partOfTerm', 'sectionTitle'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_txt = json.loads(match.group(0))\n",
    "\n",
    "print(json_txt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"app-meeting\">Padua, D<br /></div>\n"
     ]
    }
   ],
   "source": [
    "print(json_txt['instructor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Unfortunately, we are not yet done, since our JSON document contains HTML markup. We could use BeautifulSoup to parse the HTML `div` element, but in this case, it will be easier to simply use an XML parser directly to extract the text contents of the HTML element. We can use the `lxml` library to create an HTML document, and apply an [_XPATH_][wx] processing directive to pull the text from the element. In this case, we tell the XML parser to find a `div` element by using the `//div/` pattern. We can extract the text contents from this element by using the text method `text()`. Combined, we have our XPATH directive: `xpath('//div/text()')`. We demonstrate this in the following code cell, where we extract the text contents and display the result.\n",
    "\n",
    "-----\n",
    "[wx]: https://en.wikipedia.org/wiki/XPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course Instructor: Padua, D\n"
     ]
    }
   ],
   "source": [
    "from lxml import html\n",
    "\n",
    "content = html.fromstring(json_txt['instructor']).xpath('//div/text()')\n",
    "print('Course Instructor: {0}'.format(content[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AB1']\n",
      "['T      ']\n",
      "['04:00PM - 05:50PM']\n",
      "['Padua, D']\n",
      "['G7 Foreign Languages Building']\n",
      "\n",
      "['AB2']\n",
      "['W      ']\n",
      "['04:00PM - 05:50PM']\n",
      "['Padua, D']\n",
      "['G7 Foreign Languages Building']\n",
      "\n",
      "['AB3']\n",
      "['R      ']\n",
      "['04:00PM - 05:50PM']\n",
      "['Padua, D']\n",
      "['G7 Foreign Languages Building']\n",
      "\n",
      "['AL1']\n",
      "['MWF    ']\n",
      "['09:00AM - 09:50AM']\n",
      "['Padua, D']\n",
      "['0216 Siebel Center for Comp Sci']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from lxml import html\n",
    "\n",
    "for match in re.finditer(pattern, script_txt):\n",
    "    data = match.group(0)\n",
    "    json_txt = json.loads(data)\n",
    "    \n",
    "    print(html.fromstring(json_txt['section']).xpath('//div/text()'))\n",
    "    print(html.fromstring(json_txt['day']).xpath('//div/text()'))\n",
    "    print(html.fromstring(json_txt['time']).xpath('//div/text()'))  \n",
    "    print(html.fromstring(json_txt['instructor']).xpath('//div/text()'))\n",
    "    print(html.fromstring(json_txt['location']).xpath('//div/text()'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Web Processing Application\n",
    "\n",
    "We can now put everything together to build a complete web parsing application. In this case, given a root page (course listings for a given department), we can traverse the course webpage hierarchy to build a list of courses, and their associated metadata such as location, days of the week, meeting times, and instructors. This same approach can be used to _scrape_ and parse data from other websites. Before doing so, however, you should always check:\n",
    "\n",
    "1. the websites _terms of use_, which may limit what you can do with any data obtained from the website, and\n",
    "2. the availability of an API for accessing data from the website, for example twitter.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course  : Description\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 102: Little Bits to Big Ideas\n",
      "  AB1  T       04:00PM - 05:50PM Padua, D             G7 Foreign Languages Building\n",
      "  AB2  W       04:00PM - 05:50PM Padua, D             G7 Foreign Languages Building\n",
      "  AB3  R       04:00PM - 05:50PM Padua, D             G7 Foreign Languages Building\n",
      "  AL1  MWF     09:00AM - 09:50AM Padua, D             0216 Siebel Center for Comp Sci\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 199: Undergraduate Open Seminar\n",
      "  N/A  n.a    ARRANGED          N/A                  n.a\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 202: Social Aspects Info Tech\n",
      "  AD1  R       02:00PM - 02:50PM Liu, C               127 English Building\n",
      "  AD2  R       03:00PM - 03:50PM Takazawa, A          1030 Foreign Languages Building\n",
      "  AD3  R       04:00PM - 04:50PM Takazawa, A          1022 Foreign Languages Building\n",
      "  AD4  R       01:00PM - 01:50PM Lupa-Lasaga, V       G48 Foreign Languages Building\n",
      "  AD5  F       10:00AM - 10:50AM Lupa-Lasaga, V       G46 Foreign Languages Building\n",
      "  AD6  F       11:00AM - 11:50AM Liu, C               1118 Foreign Languages Building\n",
      "  AE1  TR      09:30AM - 10:20AM Darch, P             126 Grad Sch of Lib & Info Science\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 303: Writing Across Media\n",
      "  A    TR      11:00AM - 12:15PM Simon, K             9 Gregory Hall\n",
      "  B    TR      12:30PM - 01:45PM Kelvie, A            9 Gregory Hall\n",
      "  C    TR      02:00PM - 03:15PM Flowers, K           9 Gregory Hall\n",
      "  D    TR      03:30PM - 04:45PM Catt, C              9 Gregory Hall\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 325: Social Media and Global Change\n",
      "  A    T       02:00PM - 03:50PM Herrera, L           312 David Kinley Hall\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 326: New Media, Culture & Society\n",
      "  B    W       03:00PM - 05:50PM Paredes, V           1028 Lincoln Hall\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 390: Special Topics\n",
      "  CC   MW      10:00AM - 11:20AM Duffy, D             126 Grad Sch of Lib & Info Science\n",
      "  W1A  WF      11:00AM - 12:20PM Ginger, J            12A Grad Sch of Lib & Info Science\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 399: Individual Study\n",
      "  N/A  n.a    ARRANGED          N/A                  n.a\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 490: Special Topics\n",
      "  GHG  M       01:00PM - 03:50PM Brooks, I            131 Grad Sch of Lib & Info Science\n",
      "  GHU  M       01:00PM - 03:50PM Brooks, I            131 Grad Sch of Lib & Info Science\n",
      "  IA3  F       01:00PM - 01:50PM Bashir, M            46 Grad Sch of Lib & Info Science\n",
      "  IA4  F       01:00PM - 01:50PM Bashir, M            46 Grad Sch of Lib & Info Science\n",
      "  RB2  n.a    ARRANGED          Brunner, R           n.a\n",
      "  RE   W       01:00PM - 03:20PM Pintar, J            G36 Foreign Languages Building\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 491: Ugrad Bioinformatics Seminar\n",
      "  A    R       12:00PM - 12:50PM Rodriguez-Zas, S     292 Animal Sciences Laboratory\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 500: Orientation Seminar\n",
      "  A    R       04:00PM - 04:50PM Gasser, L            ARR Nat Center for Suprcomp Appl\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 510: Research Practicum\n",
      "  N/A  n.a    ARRANGED          N/A                  n.a\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 590: Advanced Special Topics\n",
      "  RE   W       01:00PM - 03:20PM Pintar, J            G36 Foreign Languages Building\n",
      "  SI   R       12:00PM - 02:50PM Darch, P             109 Grad Sch of Lib & Info Science\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 591: Grad Bioinformatics Seminar\n",
      "  A    R       12:00PM - 12:50PM Rodriguez-Zas, S     292 Animal Sciences Laboratory\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 597: Individual Study\n",
      "  N/A  n.a    ARRANGED          N/A                  n.a\n",
      "-------------------------------------------------------------------------------------\n",
      "INFO 599: Thesis Research\n",
      "  N/A  n.a    ARRANGED          N/A                  n.a\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from lxml import html\n",
    "\n",
    "# Extract div element text\n",
    "def parse_xml(jt, key_str):\n",
    "    \n",
    "    html_txt  = html.fromstring(jt[key_str])\n",
    "    value = html_txt.xpath('//div/text()')\n",
    "    \n",
    "    if value:\n",
    "        return value[0]\n",
    "    else: \n",
    "        return 'N/A'\n",
    "\n",
    "# Parse course webpage\n",
    "def get_section_info(url):\n",
    "    html = requests.get(url).content\n",
    "    new_soup = BeautifulSoup(html, 'lxml')\n",
    "    script_tag = new_soup.find(type='text/javascript')\n",
    "    script_txt = script_tag.contents[0]\n",
    "    \n",
    "    for match in re.finditer(pattern, script_txt):\n",
    "        data = match.group(0)\n",
    "        json_txt = json.loads(data)\n",
    "        \n",
    "        sct = parse_xml(json_txt, 'section')\n",
    "        day = parse_xml(json_txt, 'day')\n",
    "        tme = parse_xml(json_txt, 'time')\n",
    "        ins = parse_xml(json_txt, 'instructor')\n",
    "        loc = parse_xml(json_txt, 'location')\n",
    "        \n",
    "        print('  {0:4s} {1:6s} {2:17s} {3:20s} {4}'.format(sct, day, tme, ins, loc))\n",
    "\n",
    "# Main parsing loop        \n",
    "print('{0:8s}: {1:s}'.format('Course', 'Description'))\n",
    "for row in tb.find_all('tr'):\n",
    "    tdx = [val for val in row.find_all('td')]\n",
    "    course = tdx[0].contents[0].strip()\n",
    "    c_url = 'https://courses.illinois.edu/{0}'.format(tdx[1].a['href'])\n",
    "    name = tdx[1].a.contents[0].strip()\n",
    "    print(85*'-')\n",
    "    print('{0:8s}: {1:s}'.format(course, name))\n",
    "    get_section_info(c_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Student Activity\n",
    "\n",
    "In the preceding cells, we parsed a course web page to build a list of courses and their relevant metadata. Now that you have run the Notebook, go back and make the following changes to see how the results change.\n",
    "\n",
    "1. Change the department from `INFO` to a different value, such as `ASTR` or `CS`. Does the application still work correctly?\n",
    "2. Try generating an HTML-styled version of the output. Can you display the result in this Notebook?\n",
    "3. Can you modify the application to include links to the course description in the course explorer?\n",
    "4. Modify the application to build a custom course schedule for a given set of courses\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
