{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Social Media: Twitter\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "When looking for data to use for text data processing, one of the more popular data sources is [Twitter][tw]. In this Notebook, we introduce the Twitter API, and demonstrate how to use the Twitter API from within a Python gram to acquire and process tweets, or Twitter messages. First, we review the mechanisms by which an application authenticates with Twitter. Next, we discuss different techniques for interacting with the twitter data stream by using the twitter API. Finally, we construct a tweet sentiment analysis pipeline before applying this pipeline to new tweets from a specific user.\n",
    "\n",
    "-----\n",
    "[tw]: https://www.twitter.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python and Twitter\n",
    "\n",
    "To work with the Twitter API from within a Python program, we need a Python library that wraps the official [Twitter API][twapi]. There are a number of different Python libraries that provide this capability, we will use the [tweepy][tpy] library, which is fairly popular and provides a fairly complete interface.\n",
    "\n",
    "The full Twitter API is large and robust (and continuous to evolve), for this course we will restrict our attention to several basic concepts, namely authenticating to Twitter, searching for Tweets, and digesting the messages.\n",
    "\n",
    "----\n",
    "[twapi]: https://dev.twitter.com\n",
    "[tpy]: http://www.tweepy.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy as tw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----\n",
    "\n",
    "## Reading Twitter Data\n",
    "\n",
    "To read twitter data, you need to first need to be a registered Twitter user and you need to create a new _Twitter Application_ in order to obtain credentials for connecting to Twitter and querying to the Twitter data. You create (and later manage) Twitter applications by visting the [Twitter Application Management](https://apps.twitter.com) website.\n",
    "\n",
    "![Twitter App Sign-in](images/twitter-app-signin.png)\n",
    "\n",
    "At this point you need to authenticate with Twitter, if you are already logged in to Twitter on your computer (for instance by using the Twitter website) you should already be authenticated. If you are not authenticated, click the _sign in_ link to be directed to the Twitter signin page where you can enter your credentials (if you do not have Twitter credentials, you will need to obtain a Twitter account to proceed).\n",
    "\n",
    "![Twitter Sign-in](images/twitter-signin.png)\n",
    "\n",
    "After you have been authenticated, you will be redirected to the Twitter apps page. If you have never created a Twitter application, you will have nothing listed. To create a new application, press the _Create New App_ button, as shown in the following screenshot.\n",
    "\n",
    "![Twitter Create App](images/twitter-create.png)\n",
    "\n",
    "This will open up the Twitter _Create an application_ webpage, where you need to supply some basic information for your Twitter application such as an application name, description, and website.\n",
    "\n",
    "![Twitter Application details](images/twitter-appdetails.png)\n",
    "\n",
    "Scroll to the bottom of this webpage where the **Developer Agreement** is located. Following this agreement, is a check box that you should click to signify you agree to be bound by the agreement (of course you should read this to be sure you do _agree_ with it first). Following this, press the _Create your Twitter application_ button as shown in the following screenshot.\n",
    "\n",
    "![Twitter Agree](images/twitter-agree.png)\n",
    "\n",
    "This will create your new application, and provide you with your application webpage, which will be similar to the following screenshot. \n",
    "\n",
    "![Twitter Apppage](images/twitter-apppage.png)\n",
    "\n",
    "While you can control a number of application features from this webpage, the most important tasks to complete include:\n",
    "\n",
    "1. Change your application to _read-only_ in case it is set to read-write.\n",
    "\n",
    "2. Obtain the application **Consumer Key** and **Consumer Secret**.\n",
    "\n",
    "3. Obtain your personal **Access Token** and **Access Token Secret**.\n",
    "\n",
    "You should change your application read-only to ensure you don't accidentally send data out to Twitter. You change this by selecting the _Permissions_ tab and selecting _Read only_, shown in the following screenshot. To save this setting, scroll down this webpage and click the _Update Settings_ button at the bottom of the page.\n",
    "\n",
    "![Twitter Read Only Setting](images/twitter-ro.png)\n",
    "\n",
    "These credentials can be found by selecting the _Keys and Access Tokens_ tab, and scrolling down appropriately as shown in the following two screenshots.\n",
    "\n",
    "![Twitter Consumer Application Credentials](images/twitter-consume.png)\n",
    "\n",
    "![Twitter User Credentials](images/twitter-access.png)\n",
    "\n",
    "<font color='red'> Warning: Never share these credentials with others or they will be able to fully impersonate you on Twitter! </font>\n",
    "\n",
    "You can directly copy these credentials into your Notebook, or, alternatively, save them into a file (for example by opening a terminal window and using `vim` to create a text file. In the rest of this Notebook, I demonstrate this functionality by using my credentials, which I have saved into a file called `twitter.cred`. In this empty file, which is in your github repository, I have saved the following four credentials in order:\n",
    "\n",
    "1. Access Token\n",
    "2. Access Token Secret\n",
    "3. Consumer Key\n",
    "4. Consumer Secret\n",
    "\n",
    "You can inform `git` to ignore changes in the `twitter.cred` file by using the folloqing command:\n",
    "\n",
    "```bash\n",
    "git update-index --assume-unchanged Week8/notebooks/twitter.cred \n",
    "```\n",
    "\n",
    "The following code cell demonstrates how these credentials are read from the file and used to properly authenticate our application with Twitter.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Screen Name:  ProfBrunner\n",
      "Twitter Follower Count:  140\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "# Order: Access Token, Access Token Secret, Consumer Key, Consumer SecretAccess\n",
    "\n",
    "with open(\"twitter.cred\", 'r') as fin:\n",
    "    for line in fin:\n",
    "        if line[0] != '#': # Not a comment line\n",
    "            tokens.append(line.rstrip('\\n'))\n",
    "\n",
    "auth = tw.OAuthHandler(tokens[2], tokens[3])\n",
    "auth.set_access_token(tokens[0], tokens[1])\n",
    "\n",
    "api = tw.API(auth)\n",
    "\n",
    "user = api.me()\n",
    "\n",
    "print(\"Twitter Screen Name: \", user.screen_name)\n",
    "print(\"Twitter Follower Count: \", user.followers_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "If the previous code cell runs without an error, you have successfully connected to twitter. If you are new to twitter and are not following anyone, you can instead display the user information for a different Twitter user. For example, the following code would display my Twitter information.\n",
    "\n",
    "```python\n",
    "user = api.get_user('ProfBrunner')\n",
    "```\n",
    "\n",
    "Replacing `ProfBrunner` with any valid Twitter user id will display their information. You can find examples by looking at those Twitter users you (or `ProfBrunner`) follow. Or, alternatively, you could chose a specific twitter account; for example, to analyze the _NY Times_ twitter account you would use the following statement:\n",
    "\n",
    "```python\n",
    "user = api.get_user('NYTimes')\n",
    "```\n",
    "\n",
    "This is demonstrated in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Screen Name:  nytimes\n",
      "Twitter Follower Count:  33998780\n",
      "\n",
      "This user follows:\n",
      "--------------\n",
      "fahima_haque\n",
      "homesweethome\n",
      "wirecutter\n",
      "DanaGoldstein\n",
      "PatrickKingsley\n",
      "SopanDeb\n",
      "GlennThrush\n",
      "john__corrales\n",
      "hamiltonbill\n",
      "meganspecia\n",
      "MorriganMcC\n",
      "mccanner\n",
      "qdbui\n",
      "NoahRemnick\n",
      "lancekbooth\n",
      "tgcowles\n",
      "timesinsider\n",
      "mattbpurdy\n",
      "hilsays\n",
      "NirajC\n"
     ]
    }
   ],
   "source": [
    "user = api.get_user('nytimes')\n",
    "\n",
    "print(\"Twitter Screen Name: \", user.screen_name)\n",
    "print(\"Twitter Follower Count: \", user.followers_count)\n",
    "\n",
    "print(\"\\nThis user follows:\\n--------------\")\n",
    "for friend in user.friends():\n",
    "    print(friend.screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any point, you can return to your Twitter application management webpage to view your new application. You can now view and manage your existing application, or create a new application as shown in the following screenshot.\n",
    "\n",
    "![Twitter new app management](images/twitter-manage.png)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "To run the Twitter application in the preceding cells, you will need to register your own Twitter Application. To do so, complete the following steps.\n",
    "\n",
    "1. Create a New Twitter application.\n",
    "\n",
    "2. Save your Twitter credentials and Application credentials into the provided `twitter.cred` file.\n",
    "\n",
    "3. Run the _tweepy_ sample code to connect to Twitter and display your Twitter user information.\n",
    "\n",
    "Finally, try running the preceding code, but for someone famous (if you do not know the twitter handle for someone famous, google will be your friend). \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Tweets\n",
    "\n",
    "Once you have authenticated with Twitter, you can begin to [search the Twitter stream][stw] for tweets of interest. The easiest method to get started is to being with your own (or another specific Twitter user's) own Twitter feed. To access your own Twitter feed, you can simply use your `home_timeline` to retrieve your own Tweets or Tweets from those whom you follow. This is demonstrated in the following code cell, where we display the `text` values from the ten most recent Tweets from our timeline.\n",
    "\n",
    "-----\n",
    "[stw]: https://dev.twitter.com/rest/public/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scott Adams on fake news, climate, and the Trump press ban: https://t.co/faksH76gfn via @YouTube\n",
      "'The purpose of asymptotic theory in statistics is simple: to provide usable approximations /before/ passage to the limit.' -- John Tukey\n",
      "Ooh, a new @jonathancoulton song showed up on Google Play!  \"All This Time\".  Hooray Monday!\n",
      "Video Content https://t.co/5v4SyK0wqh https://t.co/kduylUzzio https://t.co/RvMubrt5J4\n",
      "A topological sort of a DAG is a linear ordering of its vertices such that if the graph contains an edge (u, v), u appears before v.\n",
      "'Probability has reference party to our ignorance, partly to our knowledge.' -- Laplace\n",
      "What's going on inside a CNN? 6 deep learning papers that can help us to understand: https://t.co/mvzij3Dp9P https://t.co/VCk5nNS8sP\n",
      "RT @iSchoolUI: Time to celebrate a milestone birthday! #iSchoolUI has been around for 124 of those 150 years. https://t.co/JlEzVI07qt\n",
      "I hope everyone has a lot more of the first panel this week than the second panel. Happy Sciencing! https://t.co/u3nEBi7yz2\n",
      "My steak take is that you should almost never order the filet. Go with the ribeye or maybe the strip.\n"
     ]
    }
   ],
   "source": [
    "for tweet in tw.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    print(tweet.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Searching\n",
    "\n",
    "Twitter also provides the capability to search for specific tweets by using the Tweepy [`search` method][twse]. In this method, you supply a query string (and optional arguments) and are returned a list of Tweets. The query string should follow the [Twitter Search API][tsa], but basically you can search for specific text in a string by using the text of interest, you can search for a person by using the `@` character followed by their Twitter username, and hashtags by using the `#` character followed by the tag text.\n",
    "\n",
    "-----\n",
    "[twse]: http://docs.tweepy.org/en/stable/api.html#API.search\n",
    "[tsa]: https://dev.twitter.com/rest/public/search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet ID: 836285991962959872\n",
      "Tweeted by  JamiatMSDF\n",
      "Created at  2017-02-27 18:44:52\n",
      "Location:  Twitter Web Client\n",
      "Tweet Text:  RT @mbaur: It's not rocket science... but it is computer science. :-) https://t.co/H4dcl554Js\n",
      "-------------------------\n",
      "Tweet ID: 836226368899698688\n",
      "Tweeted by  mbaur\n",
      "Created at  2017-02-27 14:47:57\n",
      "Location:  LinkedIn\n",
      "Tweet Text:  It's not rocket science... but it is computer science. :-) https://t.co/H4dcl554Js\n",
      "-------------------------\n",
      "Tweet ID: 836151528377430016\n",
      "Tweeted by  mahjooob\n",
      "Created at  2017-02-27 09:50:33\n",
      "Location:  Twitter for iPhone\n",
      "Tweet Text:  RT @3alaKivy: التغريدة الأولى :)\n",
      "مقال يشرح إستخدام #البايثون في تنقيب وتحليل البيانات\n",
      "https://t.co/f367aHeKcr\n",
      "#بايثون\n",
      "#Python\n",
      "-------------------------\n",
      "Tweet ID: 836008520948596736\n",
      "Tweeted by  SarahnChile\n",
      "Created at  2017-02-27 00:22:18\n",
      "Location:  Twitter Web Client\n",
      "Tweet Text:  @MarisolAcuna Holii! Va asistir este viernes? :) \"Women in Data Science\" https://t.co/rVisY8jeuC @Eventbrite\n",
      "-------------------------\n",
      "Tweet ID: 835951874754895876\n",
      "Tweeted by  WhatIsWAVES\n",
      "Created at  2017-02-26 20:37:12\n",
      "Location:  IFTTT\n",
      "Tweet Text:  Thanks Big Data Science for joining the Blockchain Revolution :) https://t.co/8Gu8TqG5bI\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Hash Tage search: term = '#python'\n",
    "# User search: term = '@nytimes'\n",
    "# Keyword search: term = 'data science'\n",
    "# Keyword and Sentiment: term ='data science :)' # Positive attitute\n",
    "\n",
    "term ='data science :)'\n",
    "num_tweets = 5\n",
    "\n",
    "for tweet in tw.Cursor(api.search, q=term).items(num_tweets):\n",
    "    # Process a single status\n",
    "    print(\"Tweet ID:\", tweet.id)\n",
    "    print('Tweeted by ', tweet.user.screen_name)\n",
    "    print(\"Created at \",tweet.created_at)\n",
    "    print(\"Location: \",tweet.source)\n",
    "    print('Tweet Text: ', tweet.text)\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can view the available attributes to display by using Python `dir` method to perform introspection. In the following code cell we explicitly remove _class_ methods to minimize the display list and focus on the items of interest. After this, we display the Tweet in its raw JSON format by accessing the `_json` attribute.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '_max_id', '_since_id', 'append', 'clear', 'completed_in', 'copy', 'count',\n",
      "  'extend', 'ids', 'index', 'insert', 'max_id', 'next_results', 'parse', 'pop',\n",
      "  'query', 'refresh_url', 'remove', 'reverse', 'since_id', 'sort']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "\n",
    "tweets = api.search(q='ProfBrunner', rpp=1)\n",
    "\n",
    "pp.pprint([att for att in dir(tweets) if '__' not in att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ '_api', '_json', 'author', 'contributors', 'coordinates', 'created_at',\n",
      "  'destroy', 'entities', 'favorite', 'favorite_count', 'favorited', 'geo', 'id',\n",
      "  'id_str', 'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
      "  'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
      "  'is_quote_status', 'lang', 'metadata', 'parse', 'parse_list', 'place',\n",
      "  'possibly_sensitive', 'quoted_status', 'quoted_status_id',\n",
      "  'quoted_status_id_str', 'retweet', 'retweet_count', 'retweeted', 'retweets',\n",
      "  'source', 'source_url', 'text', 'truncated', 'user']\n"
     ]
    }
   ],
   "source": [
    "# Pick a single tweet to analyze\n",
    "\n",
    "tweet = tweets[1]\n",
    "pp.pprint([att for att in dir(tweet) if '__' not in att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'contributors': None,\n",
      "  'coordinates': None,\n",
      "  'created_at': 'Wed Feb 22 18:14:58 +0000 2017',\n",
      "  'entities': { 'hashtags': [],\n",
      "                'symbols': [],\n",
      "                'urls': [...],\n",
      "                'user_mentions': [...]},\n",
      "  'favorite_count': 1,\n",
      "  'favorited': False,\n",
      "  'geo': None,\n",
      "  'id': 834466526627500032,\n",
      "  'id_str': '834466526627500032',\n",
      "  'in_reply_to_screen_name': 'ProfBrunner',\n",
      "  'in_reply_to_status_id': None,\n",
      "  'in_reply_to_status_id_str': None,\n",
      "  'in_reply_to_user_id': 2230450843,\n",
      "  'in_reply_to_user_id_str': '2230450843',\n",
      "  'is_quote_status': True,\n",
      "  'lang': 'und',\n",
      "  'metadata': {'iso_language_code': 'und', 'result_type': 'recent'},\n",
      "  'place': None,\n",
      "  'possibly_sensitive': False,\n",
      "  'quoted_status': { 'contributors': None,\n",
      "                     'coordinates': None,\n",
      "                     'created_at': 'Wed Feb 22 18:14:22 +0000 2017',\n",
      "                     'entities': {...},\n",
      "                     'favorite_count': 179694,\n",
      "                     'favorited': False,\n",
      "                     'geo': None,\n",
      "                     'id': 834466377201307652,\n",
      "                     'id_str': '834466377201307652',\n",
      "                     'in_reply_to_screen_name': None,\n",
      "                     'in_reply_to_status_id': None,\n",
      "                     'in_reply_to_status_id_str': None,\n",
      "                     'in_reply_to_user_id': None,\n",
      "                     'in_reply_to_user_id_str': None,\n",
      "                     'is_quote_status': False,\n",
      "                     'lang': 'en',\n",
      "                     'metadata': {...},\n",
      "                     'place': None,\n",
      "                     'possibly_sensitive': False,\n",
      "                     'retweet_count': 128435,\n",
      "                     'retweeted': False,\n",
      "                     'source': '<a href=\"http://twitter.com\" '\n",
      "                               'rel=\"nofollow\">Twitter Web Client</a>',\n",
      "                     'text': 'New record! We’ve found 7 Earth-sized planets '\n",
      "                             'around a single star outside our solar system; 3 '\n",
      "                             'in habitable zone:… https://t.co/5q0kNqE5sw',\n",
      "                     'truncated': True,\n",
      "                     'user': {...}},\n",
      "  'quoted_status_id': 834466377201307652,\n",
      "  'quoted_status_id_str': '834466377201307652',\n",
      "  'retweet_count': 0,\n",
      "  'retweeted': False,\n",
      "  'source': '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web '\n",
      "            'Client</a>',\n",
      "  'text': '@ProfBrunner 😀👽 https://t.co/XznPryPk4R',\n",
      "  'truncated': False,\n",
      "  'user': { 'contributors_enabled': False,\n",
      "            'created_at': 'Tue Feb 21 18:39:11 +0000 2012',\n",
      "            'default_profile': True,\n",
      "            'default_profile_image': False,\n",
      "            'description': 'MTS @VMware, working on https://t.co/wfxvK1gHFm. '\n",
      "                           \"@IllinoisCS '16. I like cars, hockey and Docker \"\n",
      "                           \"containers. Let's go Hawks!\",\n",
      "            'entities': {...},\n",
      "            'favourites_count': 1074,\n",
      "            'follow_request_sent': False,\n",
      "            'followers_count': 224,\n",
      "            'following': False,\n",
      "            'friends_count': 611,\n",
      "            'geo_enabled': True,\n",
      "            'has_extended_profile': False,\n",
      "            'id': 499057485,\n",
      "            'id_str': '499057485',\n",
      "            'is_translation_enabled': False,\n",
      "            'is_translator': False,\n",
      "            'lang': 'en',\n",
      "            'listed_count': 8,\n",
      "            'location': 'Austin, TX',\n",
      "            'name': 'Anchal Agrawal',\n",
      "            'notifications': False,\n",
      "            'profile_background_color': 'C0DEED',\n",
      "            'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "            'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "            'profile_background_tile': False,\n",
      "            'profile_banner_url': 'https://pbs.twimg.com/profile_banners/499057485/1433829779',\n",
      "            'profile_image_url': 'http://pbs.twimg.com/profile_images/469368218100834305/pa4XCBfo_normal.jpeg',\n",
      "            'profile_image_url_https': 'https://pbs.twimg.com/profile_images/469368218100834305/pa4XCBfo_normal.jpeg',\n",
      "            'profile_link_color': '1DA1F2',\n",
      "            'profile_sidebar_border_color': 'C0DEED',\n",
      "            'profile_sidebar_fill_color': 'DDEEF6',\n",
      "            'profile_text_color': '333333',\n",
      "            'profile_use_background_image': True,\n",
      "            'protected': False,\n",
      "            'screen_name': 'AgrawalAnchal',\n",
      "            'statuses_count': 1115,\n",
      "            'time_zone': 'Central America',\n",
      "            'translator_type': 'none',\n",
      "            'url': None,\n",
      "            'utc_offset': -21600,\n",
      "            'verified': False}}\n"
     ]
    }
   ],
   "source": [
    "# We can display the message data in JSON format\n",
    "\n",
    "pp.pprint(tweet._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Trending\n",
    "\n",
    "Twitter tracks tweet data to identify topics or people that are being frequently mentioned, which is known as [_trending_][twtr]. The Twitter API enables an application to obtain a list of currently trending topics. These topics include metadata that can be used to learn more about trending topics. One component of the metadata is the physical location of the trending topic. This location is encoded as a [**WOEID**][woeid], which is a Yahoo developed standard that is short for _where on the earth ID_. In the first code cell below, we demonstrate obtaining the locations of currently trending topics before displaying these physical locations. In the second code cell, we display the complete metadata for one location, which can be used to obtain a list of trending topics for a particular location on Earth, via the WOEID. Note that since trending topics change, this Notebook will provide different results when run at different times.\n",
    "\n",
    "----\n",
    "[twtr]: https://dev.twitter.com/rest/reference/get/trends/available\n",
    "[woeid]: https://developer.yahoo.com/geo/geoplanet/guide/concepts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOEID Code (2972): Winnipeg, Canada\n",
      "WOEID Code (3369): Ottawa, Canada\n",
      "WOEID Code (3444): Quebec, Canada\n",
      "WOEID Code (3534): Montreal, Canada\n",
      "WOEID Code (4118): Toronto, Canada\n",
      "WOEID Code (8676): Edmonton, Canada\n",
      "WOEID Code (8775): Calgary, Canada\n",
      "WOEID Code (9807): Vancouver, Canada\n",
      "WOEID Code (12723): Birmingham, United Kingdom\n",
      "WOEID Code (12903): Blackpool, United Kingdom\n",
      "WOEID Code (13383): Bournemouth, United Kingdom\n",
      "WOEID Code (13911): Brighton, United Kingdom\n",
      "WOEID Code (13963): Bristol, United Kingdom\n",
      "WOEID Code (15127): Cardiff, United Kingdom\n",
      "WOEID Code (17044): Coventry, United Kingdom\n",
      "WOEID Code (18114): Derby, United Kingdom\n",
      "WOEID Code (19344): Edinburgh, United Kingdom\n",
      "WOEID Code (21125): Glasgow, United Kingdom\n",
      "WOEID Code (25211): Hull, United Kingdom\n"
     ]
    }
   ],
   "source": [
    "# Returns a JSON object that contains (a large number of) locations \n",
    "# that are currently trending.\n",
    "\n",
    "top_display = 20\n",
    "trending = api.trends_available()\n",
    "\n",
    "# We skip first value, which is entry for the World in JSON.\n",
    "for trend in trending[1:top_display]:\n",
    "    print('WOEID Code ({2:d}): {0}, {1}'.format(trend['name'], \\\n",
    "                                                trend['country'], trend['woeid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'country': 'United Kingdom',\n",
      "  'countryCode': 'GB',\n",
      "  'name': 'Blackpool',\n",
      "  'parentid': 23424975,\n",
      "  'placeType': {'code': 7, 'name': 'Town'},\n",
      "  'url': 'http://where.yahooapis.com/v1/place/12903',\n",
      "  'woeid': 12903}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(trending[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK Trends\n",
      "----------\n",
      "  #UniversityChallenge\n",
      "  #LeiLiv\n",
      "  John Major\n",
      "  Mignolet\n",
      "  #Broadchurch\n",
      "  Lucas\n",
      "  #ElectricalFilmTitles\n",
      "  #dorsethour\n",
      "  Coutinho\n",
      "  Mane\n"
     ]
    }
   ],
   "source": [
    "# We can use a WOEID to find location specific trends.\n",
    "# Here we use the WOEID for the UK (from previous example)\n",
    "\n",
    "top_display = 10\n",
    "\n",
    "print(\"UK Trends\")\n",
    "print(10*'-')\n",
    "\n",
    "for trends in api.trends_place(id = 23424975):\n",
    "    for trend in trends[\"trends\"][:top_display]:\n",
    "        print(\"  {0:s}\".format(trend[\"name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used the twitter API to obtain tweets and to identify trending topics. Now that you have run the Notebook, try making the following changes.\n",
    "\n",
    "1. Pick a particular twitter user and search their twitter stream.\n",
    "2. Pick a different location from the trending topics location list, and identify trending topics from a different WOEID.\n",
    "\n",
    "-----\n",
    "## Twitter text analysis\n",
    "\n",
    "We can now develop a text analysis project that uses twitter data. To simplify the application, we will use the NLTK twitter corpus. Otherwise, we would need a separate notebook to obtain the necessary tweets (because of the twitter rate limitation). The [NLTK twitter corpus][ntw] includes thirty thousand tweets retrieved from the twitter streaming API, the data have been cached on our course JupyterHub server. The tweets were explicitly selected from a recent election in the United Kingdom and one-third of them have been classified into positive or negative (with equal numbers of each). With these tweets, we can build a classification pipeline to perform sentiment analysis on twitter data.\n",
    "\n",
    "In the following code cells, we first obtain the tweets, build the numpy arrays for our classification pipeline, before constructing and testing this simple sentiment analysis text analysis application.\n",
    "\n",
    "-----\n",
    "[ntw]: http://www.nltk.org/howto/twitter.html#Using-a-Tweet-Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 Positive Tweets\n",
      "5000 Negative Tweets\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "tws = nltk.corpus.twitter_samples\n",
    "\n",
    "pos_tweets = np.array(tws.strings('positive_tweets.json'))\n",
    "neg_tweets = np.array(tws.strings('negative_tweets.json'))\n",
    "\n",
    "pos_labels = np.ones(pos_tweets.shape[0])\n",
    "neg_labels = np.zeros(neg_tweets.shape[0])\n",
    "\n",
    "targets = np.concatenate((pos_labels, neg_labels), axis=0)\n",
    "data = np.concatenate((pos_tweets, neg_tweets), axis = 0)\n",
    "\n",
    "print('{0} Positive Tweets'.format(pos_tweets.shape[0]))\n",
    "print('{0} Negative Tweets'.format(neg_tweets.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We will employ 75% of the data for training, with 25% held out for validation. The classification pipeline will use a simply tokenizer to build a document-term matrix before applying a Naive Bayes classifier. The tokenizer will use _English_ stop words, will convert the text to all lowercase, and includes both unigrams and bigrams. Overall, this simple classification pipeline gives reasonable results.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, targets, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Positive       0.73      0.79      0.76      1240\n",
      "   Negative       0.78      0.71      0.74      1260\n",
      "\n",
      "avg / total       0.75      0.75      0.75      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "\n",
    "# Lowercase, English Stop Words, and unigrams and bigrams.\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,2), \\\n",
    "                cv__lowercase=True)\n",
    "\n",
    "pclf.fit(x_train, y_train)\n",
    "y_pred = pclf.predict(x_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = ['Positive', 'Negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Blind Testing\n",
    "\n",
    "We can use the remaining twenty thousand tweets in the NLTK corpus for blind testing. We first obtain the tweets as a numpy array, before applying our sentiment analysis pipeline. Finally, we display the relative numbers of positive and negative classifications and we display examples of both positive and negative classified tweets.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unknown_tweets = np.array(tws.strings('tweets.20150430-223406.json'))\n",
    "unknown_pred = pclf.predict(unknown_tweets)\n",
    "\n",
    "unknown_pos = unknown_tweets[unknown_pred == 1]\n",
    "unknown_neg = unknown_tweets[unknown_pred == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 tweets to classify.\n",
      "8508 tweets classified as positive.\n",
      "11492 tweets classified as negative.\n",
      "---------------------------------------------------------------------------\n",
      "Sample Positve Tweet:\n",
      "---------------------------------------------------------------------------\n",
      "\"David Cameron: smooth, smiley but unconvincing\" #bbcqt http://t.co/mJ2ZkX1TjB\n",
      "---------------------------------------------------------------------------\n",
      "Sample Negatve Tweet:\n",
      "---------------------------------------------------------------------------\n",
      "RT @DouglasDaniel: Miliband's new line 'if you don't vote Labour in Scotland I will punish you by letting the Tories in'.\n"
     ]
    }
   ],
   "source": [
    "tweet_idx = 101\n",
    "\n",
    "print('{0} tweets to classify.'.format(unknown_tweets.shape[0]))\n",
    "print('{0} tweets classified as positive.'.format(unknown_pos.shape[0]))\n",
    "print('{0} tweets classified as negative.'.format(unknown_neg.shape[0]))\n",
    "\n",
    "print(75*'-')\n",
    "print('Sample Positve Tweet:')\n",
    "print(75*'-')\n",
    "print(unknown_pos[tweet_idx])\n",
    "\n",
    "print(75*'-')\n",
    "print('Sample Negatve Tweet:')\n",
    "print(75*'-')\n",
    "print(unknown_neg[tweet_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Classifying new Tweets\n",
    "\n",
    "We can now combine everything covered in this notebook in order to apply our trained sentiment analysis pipeline on new twitter data. For this, we pick _random_ user, in this case CNN Political Twitter Feed (note this wasn't completely random. The training data was obtained from a similar feeds). We first obtain tweets from this user before creating the numpy arrays to use with our scikit learn classifier. Finally, we display the results of this sentiment analysis classifier. Note, since the tweets will change over time, the results presented in this Notebook will also change.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We obtained a sample of 100 tweets\n"
     ]
    }
   ],
   "source": [
    "newtweets = api.user_timeline(screen_name='@CNNPolitics', include_rts=False, count=100)\n",
    "                           \n",
    "print('We obtained a sample of {0} tweets'.format(len(newtweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "for tweet in newtweets:\n",
    "    messages.append(tweet.text)\n",
    "    \n",
    "new_tweets = np.array(messages)\n",
    "new_pred = pclf.predict(new_tweets)\n",
    "\n",
    "new_pos = new_tweets[new_pred == 1]\n",
    "new_neg = new_tweets[new_pred == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tweets to classify.\n",
      "73 tweets classified as positive.\n",
      "27 tweets classified as negative.\n",
      "---------------------------------------------------------------------------\n",
      "Sample Positve Tweet:\n",
      "---------------------------------------------------------------------------\n",
      "Former President George W. Bush Bush offers muted criticism of President Trump https://t.co/lrBIv1Tqcx https://t.co/XFhUOsgmpw\n",
      "---------------------------------------------------------------------------\n",
      "Sample Negatve Tweet:\n",
      "---------------------------------------------------------------------------\n",
      "The #Oscars got political https://t.co/IcgHyaioU9 https://t.co/TlQhlSp5ID\n"
     ]
    }
   ],
   "source": [
    "tweet_idx = 13\n",
    "\n",
    "print('{0} tweets to classify.'.format(new_tweets.shape[0]))\n",
    "print('{0} tweets classified as positive.'.format(new_pos.shape[0]))\n",
    "print('{0} tweets classified as negative.'.format(new_neg.shape[0]))\n",
    "\n",
    "print(75*'-')\n",
    "print('Sample Positve Tweet:')\n",
    "print(75*'-')\n",
    "print(new_pos[tweet_idx])\n",
    "\n",
    "print(75*'-')\n",
    "print('Sample Negatve Tweet:')\n",
    "print(75*'-')\n",
    "print(new_neg[tweet_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we build a sentiment analysis classification pipeline by using the NLTK corpus before applying it to new tweets. Now that you have run the Notebook, try making the following changes.\n",
    "\n",
    "1. Modify the pipeline parameters, for example, apply stemming, change the `max_features`, or the number of n-grams. Can you improve the classification results on the validation data?\n",
    "\n",
    "2. Change the type of classification algorithm (e.g., random forest or SVC with regularization). Can you improve the classification results on the validation data?\n",
    "\n",
    "3. Using your new classification pipeline, examine the performance on the current twitter user. Look at other tweets, does the performance improve?\n",
    "\n",
    "4. Try classifying tweets from a different user, either an _election_ type feed or a popular figure. By looking at select tweets and their classification, comment on how your classifier performs?\n",
    "\n",
    "Finally, why do you think the classification pipeline performs in the manner that is does (i.e., why are some tweets classified negative/positive)? Feel free to use the class forums.\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
