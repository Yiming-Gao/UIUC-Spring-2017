{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not using the `Assignments` tab on the course JupyterHub server to read this notebook, read [Activating the assignments tab](https://github.com/lcdm-uiuc/info490-sp17/blob/master/help/act_assign_tab.md).\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_  â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded.\n",
    "\n",
    "# Problem 9.1: NLP: Basic Concepts\n",
    "\n",
    "For this problem, we will be delving into part of speech tagging and some basic text analysis.  You will be analyzing text from Monty Python and the Holy Grail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6f48c5eeba3eeda1f6ca2a1bd0e57e03",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "import pprint\n",
    "import collections\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures, TrigramCollocationFinder, TrigramAssocMeasures\n",
    "from nltk.tag import DefaultTagger, UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance, assert_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monty = webtext.raw('grail.txt')\n",
    "assert_is_instance(monty, str)\n",
    "assert_equal(len(monty), 65003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "In this function, you will tokenize the given input text.  The function word_tokenize() might prove helpful in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b24fe2622b2b768a2772868b4cfcaafc",
     "grade": false,
     "grade_id": "combine",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text_str):\n",
    "    '''\n",
    "    Tokenizes the text string by words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: A string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    tokens = word_tokenize(text_str)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "09b5f78206500aa7a276fa9fc55813d8",
     "grade": true,
     "grade_id": "combine_test",
     "locked": true,
     "points": 13,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tok = tokenize(monty)\n",
    "assert_is_instance(tok,list)\n",
    "assert_true(all(isinstance(t, str) for t in tok))\n",
    "assert_equal(len(tok), 16450)\n",
    "assert_equal(tok[:10], ['SCENE', '1', ':', '[', 'wind', ']', '[', 'clop', 'clop', 'clop'])\n",
    "assert_equal(tok[51:55], ['King', 'of', 'the', 'Britons'])\n",
    "assert_equal(tok[507:511], ['African', 'swallows', 'are', 'non-migratory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations: bigrams\n",
    "\n",
    "Here you will make a function that will use NLTK to grab the x best bi-grams, where x is a positive integer.  You should be using pointwise mutual information in order to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "62ae40fde60f59ade1a6483c0509d69f",
     "grade": false,
     "grade_id": "pivot",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def x_bigrams(tokens, x):\n",
    "    '''\n",
    "    Find the x best bi-grams given tokens (a list of strings) and x which will \n",
    "    tell you how many bi-grams to return.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: A list of strings\n",
    "    x: An integer\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ls_bigram: A list of tuples, with the tuples being of the form (str, str).\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    ls_bigram = finder.nbest(bigram_measures.pmi, x)\n",
    "    \n",
    "    return ls_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "74dd08448afc661cb989a9b73de59a48",
     "grade": true,
     "grade_id": "pivot_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "bigrams = x_bigrams(tok, 20)\n",
    "assert_is_instance(bigrams, list)\n",
    "assert_true(all(isinstance(b, tuple) for b in bigrams))\n",
    "assert_true(len(bigrams), 20)\n",
    "assert_equal(bigrams, [(\"'Til\", 'Recently'), (\"'To\", 'whoever'),\n",
    "                       ('Anybody', 'armed'), ('Attila', 'raised'),\n",
    "                       ('Badon', 'Hill'), ('Bon', 'magne'), ('Chapter', 'Two'),\n",
    "                       ('Clark', 'Gable'), ('Divine', 'Providence'),\n",
    "                       ('Great', 'scott'), ('Most', 'kind'),\n",
    "                       ('Olfin', 'Bedwere'), ('Recently', 'Said'),\n",
    "                       ('Thou', 'hast'), ('Thy', 'mer'), ('Too', 'late'),\n",
    "                       ('Uther', 'Pendragon'), ('absolutely', 'necessary'),\n",
    "                       ('advancing', 'behaviour'),\n",
    "                       ('anarcho-syndicalist', 'commune')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations: trigrams\n",
    "\n",
    "Now you will repeat the previous function, but instead of bi-grams, you will be finding the x best tri-grams, again using PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3fedf339ba3473900fea98897fe14382",
     "grade": false,
     "grade_id": "similar_user",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def x_trigrams(tokens, x):\n",
    "    '''\n",
    "    Find the x best tri-grams given tokens (a list of strings) and x which will \n",
    "    tell you how many tri-grams to return.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: A list of strings\n",
    "    x: An integer\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tri_list: A list of tuples, with the tuples being of the \n",
    "    form (str, str, str).\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    trigram_measures = TrigramAssocMeasures()\n",
    "    finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    tri_list = finder.nbest(trigram_measures.pmi, x)\n",
    "    \n",
    "    return tri_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "23b31bbb5ea3b831c185419a28d3e424",
     "grade": true,
     "grade_id": "sim_user_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trigrams = x_trigrams(tok, 5)\n",
    "assert_is_instance(trigrams, list)\n",
    "assert_true(all(isinstance(t, tuple) for t in trigrams))\n",
    "assert_true(len(trigrams), 5)\n",
    "assert_equal(trigrams, [(\"'Til\", 'Recently', 'Said'),\n",
    "                        (\"'To\", 'whoever', 'finds'), \n",
    "                        ('Thou', 'hast', 'vouchsafed'),\n",
    "                        ('basic', 'medical', 'training'),\n",
    "                        ('dorsal', 'guiding', 'feathers')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "Now that we have a good handle on our best bi- and tri-grams, let's change gears and try to do some POS tagging.  For this function, we will only do rudimentary POS tagging.  You should find the use of `pos_tag` to be helpful here.  You should write your code so that if default is true, you use the default tagger, but if it is false, then the Universal tagger should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "0fa9ba019fb8ac63e483f7bfc3989b2d",
     "grade": false,
     "grade_id": "find_books",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tagging(tokens, default = True):\n",
    "    '''\n",
    "    Performs POS tagging with the tagger determined by the boolean 'default'.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: a list of strings\n",
    "    default: a boolean \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tagged: a list of tuples, with the tuples taking the form of (str, str)\n",
    "    '''\n",
    "    # default tagger\n",
    "    if default == True:\n",
    "        tagged = pos_tag(tokens)\n",
    "\n",
    "    # universal tagger\n",
    "    elif default == False:\n",
    "        tagged = pos_tag(tokens, tagset = 'universal')\n",
    "    \n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7b4ef28dfc74cfeab969dcbb531fc6d8",
     "grade": true,
     "grade_id": "list_check",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "uni = tagging(tok, default = False)\n",
    "assert_is_instance(uni, list)\n",
    "assert_true(all(isinstance(u, tuple) for u in uni))\n",
    "assert_true(len(uni), 16450)\n",
    "assert_equal(uni[745:760], [('DEAD', 'NOUN'), ('PERSON', 'NOUN'),\n",
    "                            (':', '.'), ('I', 'PRON'), (\"'m\", 'VERB'),\n",
    "                            ('not', 'ADV'), ('dead', 'ADJ'), ('!', '.'),\n",
    "                            ('CART-MASTER', 'NOUN'), (':', '.'),\n",
    "                            ('What', 'PRON'), ('?', '.'), ('CUSTOMER', 'NOUN'),\n",
    "                            (':', '.'), ('Nothing', 'NOUN')])\n",
    "\n",
    "not_uni = tagging(tok)\n",
    "assert_is_instance(not_uni, list)\n",
    "assert_true(all(isinstance(n, tuple) for n in not_uni))\n",
    "assert_true(len(not_uni), 16450)\n",
    "assert_equal(not_uni[1503:1525], [('We', 'PRP'), (\"'re\", 'VBP'), ('an', 'DT'),\n",
    "                                  ('anarcho-syndicalist', 'JJ'),\n",
    "                                  ('commune', 'NN'), ('.', '.'), ('We', 'PRP'),\n",
    "                                  ('take', 'VBP'), ('it', 'PRP'), ('in', 'IN'),\n",
    "                                  ('turns', 'VBZ'), ('to', 'TO'), ('act', 'VB'),\n",
    "                                  ('as', 'IN'), ('a', 'DT'), ('sort', 'NN'),\n",
    "                                  ('of', 'IN'), ('executive', 'JJ'),\n",
    "                                  ('officer', 'NN'), ('for', 'IN'),\n",
    "                                  ('the', 'DT'), ('week', 'NN')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagged Text extraction\n",
    "\n",
    "Finally, we will create a function that will only return the Nouns or Adjectives from our tokens.  It will be helpful to use regular expressions in this case.  Also, you should utilize the \"tagging\" function that you just made above. Additionally, your function should return the \"n\" most common words (and their occurances) in ext_tag. In order to find the most common words and their occurances, please consider using Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "51463e7aae707b99e430386340e57473",
     "grade": false,
     "grade_id": "long_words",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tag_tx_ext(tokens, n):\n",
    "    '''\n",
    "    Takes in tokens and returns a list of tokens that are either nouns\n",
    "    or adjectives as well as a list of tuples of the most common adjectives\n",
    "    or nouns with their number of occurances.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: A list of strings.\n",
    "    n: An integer.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of ext_tag and common where these two arguments have the following\n",
    "    structure:\n",
    "    ext_tag: A list of strings.\n",
    "    common: A list of tuples of the form (str, int)\n",
    "    '''\n",
    "\n",
    "    # filter nouns and adjectives\n",
    "    rgxs = re.compile(r\"(NOUN|ADJ)\")\n",
    "    pos_tags = tagging(tokens, default = False)\n",
    "    ext_tag = [tkn[0] for tkn in pos_tags if re.match(rgxs, tkn[1])]\n",
    "    \n",
    "    # count occurances\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "\n",
    "    for tag in ext_tag:    \n",
    "        cnt[tag] += 1\n",
    "    # most common words\n",
    "    common = cnt.most_common(n)\n",
    "    \n",
    "    return ext_tag, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "fdd157c2ebe4862470f28b845c397ecf",
     "grade": true,
     "grade_id": "long_words_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ex_tags, com = tag_tx_ext(tok, 13)\n",
    "assert_is_instance(ex_tags, list)\n",
    "assert_true(all(isinstance(ex, str) for ex in ex_tags))\n",
    "assert_true(len(ex_tags), 5323)\n",
    "assert_equal(ex_tags[603:620], ['BLACK', 'KNIGHT', 'Aagh', 'GREEN', 'KNIGHT',\n",
    "                                '[', 'King', 'Arthur', 'music', ']', 'Ooh',\n",
    "                                '[', 'music', ']', 'BLACK','KNIGHT', 'Aaagh'])\n",
    "\n",
    "assert_equal(ex_tags[1000:1010], ['Burn', 'BEDEVERE', 'Quiet', 'Quiet', 'Quiet',\n",
    "                                  'Quiet', 'ways', 'witch', 'VILLAGER', 'Are'])\n",
    "\n",
    "assert_is_instance(com, list)\n",
    "assert_true(all(isinstance(c, tuple) for c in com))\n",
    "assert_true(len(com), 13)\n",
    "assert_equal(com, [(']', 296), ('[', 285), ('ARTHUR', 220), ('LAUNCELOT', 71),\n",
    "                   ('KNIGHT', 68), ('GALAHAD', 67), ('FATHER', 63),\n",
    "                   ('BEDEVERE', 60), ('HEAD', 54), ('GUARD', 53),\n",
    "                   ('Sir', 51), ('VILLAGER', 47), ('boom', 45)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
