# Week 3 Lesson 1 #
## Supervised Learning: Nearest Neighbor ##

In this lesson, you will gain an introduction to Supervised Learning, specifically via the nearest neighbor algorithm and a discussion of classification and regression metrics such as precision, recall, and f1 score. The nearest neighbor algorithm, more generally known as k-nearest neighbor is one of the simplest machine learning algorithms to learn, and often provides a quick and easy benchmark comparison for more advanced techniques. 

###Objectives ###

By the end of this lesson, you will be able to:

- Understand the basic concepts behind the k-nearest neighbor algorithm
- Be able to apply k-nn by using the scikit learn library
- Understand the class of problems where this algorithm can be successfully applied.

### Time Estimate ###

Approximately 2 hours.

### Readings ####

_Course Notebook_

- Explore the course [knn][l1nb] IPython Notebook on the course JupyterHub server.

_Other Material_

- Dato Blog post on [classification metrics][bcm]
- Dato Blog post on [regression metrics][brm]
- Wikipedia Article on [classification metrics][wcm]
- A discussion of [using k-nn][yknn]
- An introduction to [k-nearest neighbors][knnb]

_Video_

[Week Three, Lesson 1 video][lv]

#### Optional Additional Readings

- Scikit Learn [knn][sknn] documentation
- Discussion of [using knn in scikit learn][dknn]
- Kaggle video blog post on [Using k-NN][kknnb]

- Section 2.2 (KNN subsection) from [Introduction to Statistical Learning][isl]  by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani
- Section 13.3 from [Elements of Statistical Learning][esl] by Trevor Hastie, Robert Tibshirani, and Jerome Friedman. Note this text is rather mathematical in nature.

_Safari Online Books_

- **Chapter 12: k-Nearest Neighbors** from _Data Science from Scratch_, by Joel Grus.
- First three sections in **Chapter 6. Similarity, Neighbors, and Clusters** from Data Science for Business_ by Foster Provost and Tom Fawcett.

### Assessment ###

When you have completed and worked through the above readings, please take the [Week 3 Lesson 1 Assessment][la]

[l1nb]: ../notebooks/intro2knn.ipynb
[lv]: https://mediaspace.illinois.edu/media/W3l1/1_j3328uq8/63153661
[la]: https://learn.illinois.edu/mod/quiz/view.php?id=1844388

[bcm]: https://web.archive.org/web/20160430205320/http://blog.dato.com/how-to-evaluate-machine-learning-models-part-2a-classification-metrics
[brm]: https://web.archive.org/web/20160304134518/http://blog.dato.com/how-to-evaluate-machine-learning-models-part-2b-ranking-and-regression-metrics
[wcm]: https://en.wikipedia.org/wiki/Precision_and_recall
[sknn]: http://scikit-learn.org/stable/modules/neighbors.html
[yknn]: http://blog.yhat.com/posts/classification-using-knn-and-python.html
[knnb]: http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/
[kknnb]: http://blog.kaggle.com/2015/04/30/scikit-learn-video-4-model-training-and-prediction-with-k-nearest-neighbors/

[dknn]: http://bigdataexaminer.com/uncategorized/k-nearest-neighbors-and-curse-of-dimensionality-in-python-scikit-learn/
[isl]: http://www-bcf.usc.edu/~gareth/ISL/
[esl]: http://statweb.stanford.edu/~tibs/ElemStatLearn/
