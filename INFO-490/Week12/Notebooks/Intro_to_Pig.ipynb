{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Pig\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "In this IPython Notebook, we introduce Pig, a tool that simplifies\n",
    "writing MapReduce programs. We will run Pig from within our Jupyter\n",
    "Notebook, but of course Pig can also be run at the Unix command line\n",
    "when Hadoop has been properly installed. In this course, we have a\n",
    "Docker container that has Hadoop installed as a single-node system. You\n",
    "can open a terminal window from within the JupyterHub environment to\n",
    "execute Pig commands at the Unix prompt, or simply use a Jupyter Notebook\n",
    "as demonstrated below.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Pig\n",
    "\n",
    "Pig is a tool in the Hadoop ecosystem to simplify the creation of\n",
    "MapReduce programs. The language used to create these MapReduce programs\n",
    "is called _Pig Latin_, which is processed by the Pig platform. We can\n",
    "invoke the Pig platform at the command line, or in this case within a\n",
    "Jupyter Cell, and display the platform usage with the `-help` flag. We\n",
    "can run Pig in local mode or in Hadoop mode, which simplifies\n",
    "development and testing. in the rest of this Notebook, we will develop\n",
    "two Pig applications, and run both in local and Hadoop modes.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Apache Pig version 0.15.0 (r1682971) \r\n",
      "compiled Jun 01 2015, 11:44:35\r\n",
      "\r\n",
      "USAGE: Pig [options] [-] : Run interactively in grunt shell.\r\n",
      "       Pig [options] -e[xecute] cmd [cmd ...] : Run cmd(s).\r\n",
      "       Pig [options] [-f[ile]] file : Run cmds found in file.\r\n",
      "  options include:\r\n",
      "    -4, -log4jconf - Log4j configuration file, overrides log conf\r\n",
      "    -b, -brief - Brief logging (no timestamps)\r\n",
      "    -c, -check - Syntax check\r\n",
      "    -d, -debug - Debug level, INFO is default\r\n",
      "    -e, -execute - Commands to execute (within quotes)\r\n",
      "    -f, -file - Path to the script to execute\r\n",
      "    -g, -embedded - ScriptEngine classname or keyword for the ScriptEngine\r\n",
      "    -h, -help - Display this message. You can specify topic to get help for that topic.\r\n",
      "        properties is the only topic currently supported: -h properties.\r\n",
      "    -i, -version - Display version information\r\n",
      "    -l, -logfile - Path to client side log file; default is current working directory.\r\n",
      "    -m, -param_file - Path to the parameter file\r\n",
      "    -p, -param - Key value pair of the form param=val\r\n",
      "    -r, -dryrun - Produces script with substituted parameters. Script is not executed.\r\n",
      "    -t, -optimizer_off - Turn optimizations off. The following values are supported:\r\n",
      "            ConstantCalculator - Calculate constants at compile time\r\n",
      "            SplitFilter - Split filter conditions\r\n",
      "            PushUpFilter - Filter as early as possible\r\n",
      "            MergeFilter - Merge filter conditions\r\n",
      "            PushDownForeachFlatten - Join or explode as late as possible\r\n",
      "            LimitOptimizer - Limit as early as possible\r\n",
      "            ColumnMapKeyPrune - Remove unused data\r\n",
      "            AddForEach - Add ForEach to remove unneeded columns\r\n",
      "            MergeForEach - Merge adjacent ForEach\r\n",
      "            GroupByConstParallelSetter - Force parallel 1 for \"group all\" statement\r\n",
      "            PartitionFilterOptimizer - Pushdown partition filter conditions to loader implementing LoadMetaData\r\n",
      "            PredicatePushdownOptimizer - Pushdown filter predicates to loader implementing LoadPredicatePushDown\r\n",
      "            All - Disable all optimizations\r\n",
      "        All optimizations listed here are enabled by default. Optimization values are case insensitive.\r\n",
      "    -v, -verbose - Print all error messages to screen\r\n",
      "    -w, -warning - Turn warning logging on; also turns warning aggregation off\r\n",
      "    -x, -exectype - Set execution mode: local|mapreduce|tez, default is mapreduce.\r\n",
      "    -F, -stop_on_failure - Aborts execution on the first failed job; default is off\r\n",
      "    -M, -no_multiquery - Turn multiquery optimization off; default is on\r\n",
      "    -N, -no_fetch - Turn fetch optimization off; default is on\r\n",
      "    -P, -propertyFile - Path to property file\r\n",
      "    -printCmdDebug - Overrides anything else and prints the actual command used to run Pig, including\r\n",
      "                     any environment variables that are set by the pig command.\r\n",
      "17/04/01 21:09:45 INFO pig.Main: Pig script completed in 137 milliseconds (137 ms)\r\n"
     ]
    }
   ],
   "source": [
    "!pig -help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Local Pig Processing\n",
    "\n",
    "We can now use Pig to repeat the Word Count example. In this simple example, we will need to load the data, tokenize the data, group the tokens, and count the occurrences of each token. For simplify, we also will only display the top occurring tokens, which means we need to order the data set by the number of occurrences (in descending order) and limit the output in some manner. Finally, we should save our result and display only the top occurring tokens. To complete these steps, we will employ the following Pig operations:\n",
    "\n",
    "- `Load`: to load a file into a Pig data set.\n",
    "\n",
    "- `FOREACH`: to iterate through each item in a data set. We first step through each line in the file, before we step through each item in a group of tokens.\n",
    "\n",
    "- `TOKENIZE`: to generate tokens from each line in the file.\n",
    "\n",
    "- `GENERATE FLATTEN`: to generate a list of words from the tokens in the line.\n",
    "\n",
    "- `GROUP ... By`: to group all identical tokens together\n",
    "\n",
    "- `COUNT`: to compute the number of times each token occurs in the grouped token data set\n",
    "\n",
    "- `ORDER BY DESC`: to order the data set in descending order by a column. In this case, we use `$1` to refer to the second column in the grouping, which is the counts for each token.\n",
    "\n",
    "- `LIMIT`: to limit the number of items in the data set.\n",
    "\n",
    "- `STORE ... INTO`: to save the results into a named file.\n",
    "\n",
    "- `DUMP`: to display the data to the screen.\n",
    "\n",
    "Once we have created this Pig script, we run Pig in local mode (via the `-x local` flag) and send the STDERR to `/dev/null`, which hides all informational messages displayed by Pig (or Hadoop). This results in a cleaner display, but will potentially hide useful messages, so use with care.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/wordcount-local.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/wordcount-local.pig\n",
    "\n",
    "Lines = LOAD 'book.txt' AS (Line:chararray) ;\n",
    "Words = FOREACH Lines GENERATE FLATTEN (TOKENIZE (Line)) AS Word ;\n",
    "Groups = GROUP Words BY Word ;\n",
    "Counts = FOREACH Groups GENERATE group, COUNT (Words) ;\n",
    "Results = ORDER Counts BY $1 DESC ;\n",
    "Top_Results = LIMIT Results 10 ;\n",
    "STORE Results INTO 'top_words' ;\n",
    "DUMP Top_Results ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(the,13634)\n",
      "(of,8141)\n",
      "(and,6691)\n",
      "(a,5868)\n",
      "(to,4819)\n",
      "(in,4663)\n",
      "(his,3051)\n",
      "(he,2791)\n",
      "(I,2450)\n",
      "(with,2399)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# We run locally, and send pig/hadoop messages to nowhere\n",
    "pig -x local -f wordcount-local.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "For reference, the output of our Python based map-reduce was as follows:\n",
    "\n",
    "  Word | Count  | - | Word | Count\n",
    " :----: | ----: |   | :----: | ----:\n",
    "the |  13600 | | in | 4606\n",
    "of | 8127 | | his\t|  3035\n",
    "and | 6542 | | he  | 2712\n",
    "a  | 5842 | | I  | 2432\n",
    "to | 4787 | | with | 2391\n",
    "\n",
    "As seen in the output of the previous cell, the counts are different.\n",
    "The reason is simply the method of tokenization performed by Pig is\n",
    "different than our original tokenization on white space (if you did the\n",
    "Student assignment in the Map/Reduce Notebook, you will have seen the\n",
    "effect of removing punctuation during the tokenization process).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 468\n",
      "drwxr-xr-x 2 data_scientist users   4096 Apr  1 21:09 .\n",
      "drwxr-xr-x 3 data_scientist users   4096 Apr  1 21:09 ..\n",
      "-rw-r--r-- 1 data_scientist users 459046 Apr  1 21:09 part-r-00000\n",
      "-rw-r--r-- 1 data_scientist users   3596 Apr  1 21:09 .part-r-00000.crc\n",
      "-rw-r--r-- 1 data_scientist users      0 Apr  1 21:09 _SUCCESS\n",
      "-rw-r--r-- 1 data_scientist users      8 Apr  1 21:09 ._SUCCESS.crc\n",
      "\n",
      "Top Words\n",
      "the\t13634\n",
      "of\t8141\n",
      "and\t6691\n",
      "a\t5868\n",
      "to\t4819\n",
      "in\t4663\n",
      "his\t3051\n",
      "he\t2791\n",
      "I\t2450\n",
      "with\t2399\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# Show the contents of the local output\n",
    "ls -la top_words\n",
    "\n",
    "# Now display top words from output\n",
    "echo\n",
    "echo 'Top Words'\n",
    "head -10 top_words/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Setup Local Hadoop Environment\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "\n",
      "Starting namenodes on [eeacbe10081d]\n",
      "eeacbe10081d: starting namenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-namenode-eeacbe10081d.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-datanode-eeacbe10081d.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-secondarynamenode-eeacbe10081d.out\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-eeacbe10081d.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-data_scientist-nodemanager-eeacbe10081d.out\n",
      "\n",
      "========================================\n",
      "\n",
      "Safe mode is OFF\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n",
    "\n",
    "!echo\n",
    "!echo '========================================'\n",
    "!echo\n",
    "\n",
    "# Restart namenode and datanodes\n",
    "!$HADOOP_PREFIX/sbin/start-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/start-yarn.sh\n",
    "\n",
    "!echo\n",
    "!echo '========================================'\n",
    "!echo\n",
    "\n",
    "# Sometimes when the namenode is restarted, it enteres Safe Mode, \n",
    "# not allowing any changes to the file system. \n",
    "# We do want to make changes, so we manually leave Safe Mode.\n",
    "\n",
    "!$HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Hadoop Pig\n",
    "\n",
    "We can easily run our Pig script in Hadoop, the main change is that we\n",
    "need to specify the location of our input and output data sets within\n",
    "our Hadoop HDFS filesystem. In this case, we change the location of the\n",
    "_book.txt_ file to be `wc/in/book.txt`, and we now use the Pig `STORE`\n",
    "operation to save the final Pig data set into the `wc/out/top_words`\n",
    "directory. Once we have made these changes to our Pig script, we can\n",
    "simply run Pig in Hadoop mode (the default) to process the book and\n",
    "compute the top words, by total count in the selected text.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/wordcount.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/wordcount.pig\n",
    "\n",
    "Lines = LOAD 'wc/in/book.txt' AS (Line:chararray) ;\n",
    "Words = FOREACH Lines GENERATE FLATTEN (TOKENIZE (Line)) AS Word ;\n",
    "Groups = GROUP Words BY Word ;\n",
    "Counts = FOREACH Groups GENERATE group, COUNT (Words) ;\n",
    "Results = ORDER Counts BY $1 DESC ;\n",
    "Top_Results = LIMIT Results 10 ;\n",
    "STORE Results INTO 'wc/out/top_words' ;\n",
    "DUMP Top_Results ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wc/out\n",
      "(the,13634)\n",
      "(of,8141)\n",
      "(and,6691)\n",
      "(a,5868)\n",
      "(to,4819)\n",
      "(in,4663)\n",
      "(his,3051)\n",
      "(he,2791)\n",
      "(I,2450)\n",
      "(with,2399)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# We remove old output if it exists and create output directory\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f wc/out 2> /dev/null\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p wc/out 2> /dev/null\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# We run remotely, and send pig/hadoop messages to nowhere\n",
    "pig -f wordcount.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can use HDFS commands to show the contents of our HDFS file system\n",
    "related to this Pig task, before we display the output of our Pig\n",
    "script. In this case, we have the `book.txt` file in the input\n",
    "directory, and the `_SUCCESS` and `part-r-00000` files to indicate the\n",
    "result of and output from the Pig Hadoop task. We display the contents\n",
    "of the `part-r-00000` file, to show the _top words_ in the selected book.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Directory\n",
      "Found 1 items\n",
      "-rw-r--r--   1 data_scientist supergroup    1580927 2017-04-01 21:07 wc/in/book.txt\n",
      "\n",
      "Output Directory\n",
      "Found 2 items\n",
      "-rw-r--r--   1 data_scientist supergroup          0 2017-04-01 21:11 wc/out/top_words/_SUCCESS\n",
      "-rw-r--r--   1 data_scientist supergroup     459046 2017-04-01 21:11 wc/out/top_words/part-r-00000\n",
      "\n",
      "Top Words\n",
      "the\t13634\n",
      "of\t8141\n",
      "and\t6691\n",
      "a\t5868\n",
      "to\t4819\n",
      "in\t4663\n",
      "his\t3051\n",
      "he\t2791\n",
      "I\t2450\n",
      "with\t2399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HADOOP_PREFIX\n",
    "# Display directory contents\n",
    "\n",
    "echo\n",
    "echo 'Input Directory'\n",
    "bin/hdfs dfs -ls wc/in\n",
    "\n",
    "echo\n",
    "echo 'Output Directory'\n",
    "bin/hdfs dfs -ls wc/out/top_words\n",
    "\n",
    "# Write output\n",
    "\n",
    "echo\n",
    "echo 'Top Words'\n",
    "\n",
    "bin/hdfs dfs -cat wc/out/top_words/part-r-00000 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Movie Lens Data Analysis\n",
    "\n",
    "Earlier in this course, we downloaded the _small_ MovieLens data set to\n",
    "learn about recommender systems. In the rest of this Notebook, we will\n",
    "revisit that data to provide a second sample data set to learn about\n",
    "using Pig. In particular, we will learn how to join data sets and how to\n",
    "develop summary results by grouping similar rows. in the following\n",
    "several code cells, we first name our new data directory, after which we\n",
    "download and expand the MovieLens data. Finally, we display the first\n",
    "few rows of one of the data files.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Name of the directory holding the Small MovieLens data\n",
    "data_dir = '/home/data_scientist/hadoop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-04-01 21:12:52--  http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.34.146\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.34.146|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 918269 (897K) [application/zip]\n",
      "Saving to: ‘/home/data_scientist/hadoop/ml-latest-small.zip’\n",
      "\n",
      "100%[======================================>] 918,269     2.62MB/s   in 0.3s   \n",
      "\n",
      "2017-04-01 21:12:53 (2.62 MB/s) - ‘/home/data_scientist/hadoop/ml-latest-small.zip’ saved [918269/918269]\n",
      "\n",
      "Archive:  /home/data_scientist/hadoop/ml-latest-small.zip\n",
      "   creating: /home/data_scientist/hadoop/ml-latest-small/\n",
      "  inflating: /home/data_scientist/hadoop/ml-latest-small/links.csv  \n",
      "  inflating: /home/data_scientist/hadoop/ml-latest-small/movies.csv  \n",
      "  inflating: /home/data_scientist/hadoop/ml-latest-small/ratings.csv  \n",
      "  inflating: /home/data_scientist/hadoop/ml-latest-small/README.txt  \n",
      "  inflating: /home/data_scientist/hadoop/ml-latest-small/tags.csv  \n"
     ]
    }
   ],
   "source": [
    "# Grab a book to process\n",
    "!wget --output-document=$data_dir/ml-latest-small.zip \\\n",
    "    http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "\n",
    "!unzip -o $data_dir/ml-latest-small.zip -d $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\r",
      "\r\n",
      "1,31,2.5,1260759144\r",
      "\r\n",
      "1,1029,3.0,1260759179\r",
      "\r\n",
      "1,1061,3.0,1260759182\r",
      "\r\n",
      "1,1129,2.0,1260759185\r",
      "\r\n",
      "1,1172,4.0,1260759205\r",
      "\r\n",
      "1,1263,2.0,1260759151\r",
      "\r\n",
      "1,1287,2.0,1260759187\r",
      "\r\n",
      "1,1293,2.0,1260759148\r",
      "\r\n",
      "1,1339,3.5,1260759125\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 $data_dir/ml-latest-small/ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "For our first Pig task, we will load the `ratings.csv` file into Pig and\n",
    "display the first ten lines, in order to compare with the output of the\n",
    "Unix `head` command. For this, we will use the Pig `STREAM` operator,\n",
    "which allows us to stream data into a Unix command. For simplicity, we\n",
    "will simply use the Unix `head` command. In the next two code cells, we\n",
    "first save these Pig commands into a script file, after which we execute\n",
    "this Pig script in local mode, which display the results, which should\n",
    "match, other than the first row, which is our header row, processed as a\n",
    "normal data row.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/head.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/head.pig\n",
    "\n",
    "ratings = LOAD 'ml-latest-small/ratings.csv' USING PigStorage(',') ;\n",
    "tr = STREAM ratings THROUGH `head -10` AS (userID, mnovieID, rating, timestamp) ;\n",
    "DUMP tr ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(userId,movieId,rating,timestamp)\n",
      "(1,31,2.5,1260759144)\n",
      "(1,1029,3.0,1260759179)\n",
      "(1,1061,3.0,1260759182)\n",
      "(1,1129,2.0,1260759185)\n",
      "(1,1172,4.0,1260759205)\n",
      "(1,1263,2.0,1260759151)\n",
      "(1,1287,2.0,1260759187)\n",
      "(1,1293,2.0,1260759148)\n",
      "(1,1339,3.5,1260759125)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "pig -x local -b -f head.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Pig provides a powerful mechanisms for processing data within a Hadoop\n",
    "framework. However, one aspect that is not simple in the standard Hadoop\n",
    "installation is removing Header information. Since each data file in the\n",
    "MovieLens includes a header row, we employ the following Bash script to\n",
    "make copies of the data files we will be using, remove the first line\n",
    "from each file, before we display the first tow lines of these two data\n",
    "files to demonstrate the files now have no header information. Note the\n",
    "use of `sed -i '1d'`, which deletes the first line _in-place_ of the\n",
    "named file.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 data_scientist users  183372 Oct 17 11:12 ml-latest-small/links.csv\n",
      "-rw-r--r-- 1 data_scientist users  458368 Apr  1 21:12 ml-latest-small/movies.csv\n",
      "-rw-r--r-- 1 data_scientist users  458390 Apr  1 21:12 ml-latest-small/original-movies.csv\n",
      "-rw-r--r-- 1 data_scientist users 2438266 Apr  1 21:12 ml-latest-small/original-ratings.csv\n",
      "-rw-r--r-- 1 data_scientist users 2438233 Apr  1 21:12 ml-latest-small/ratings.csv\n",
      "-rw-r--r-- 1 data_scientist users   41902 Oct 17 11:12 ml-latest-small/tags.csv\n",
      "\n",
      "***** Ratings File *****\n",
      "1,31,2.5,1260759144\r\n",
      "1,1029,3.0,1260759179\r\n",
      "\n",
      "***** Movies File *****\n",
      "1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\r\n",
      "2,Jumanji (1995),Adventure|Children|Fantasy\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# Copy original files to new name\n",
    "cp ml-latest-small/ratings.csv ml-latest-small/original-ratings.csv\n",
    "cp ml-latest-small/movies.csv ml-latest-small/original-movies.csv\n",
    "\n",
    "# GNU SED allows inline editing, here we delete the first line from the file\n",
    "sed -i '1d' ml-latest-small/ratings.csv\n",
    "sed -i '1d' ml-latest-small/movies.csv\n",
    "\n",
    "# List CSV files\n",
    "ls -la ml-latest-small/*.csv\n",
    "\n",
    "echo\n",
    "echo '***** Ratings File *****'\n",
    "head -2 ml-latest-small/ratings.csv\n",
    "\n",
    "echo\n",
    "echo '***** Movies File *****'\n",
    "head -2 ml-latest-small/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Pig: Data Processing\n",
    "\n",
    "In the rest of this Notebook, we will use Pig to process the MovieLens\n",
    "data set. First, we will read the data into Pig with a defined schema.\n",
    "This includes naming the columns and specifying the actual data types\n",
    "for each column. This first example will use the `DESCRIBE` and\n",
    "`Illustrate` Pig commands to display the schema for the new Pig data set\n",
    "and a graphical representation of the schema with one actual row.\n",
    "Finally, we use the `LIMIT` and `DUMP` Pig commands to only display the\n",
    "values for a limited number of rows in the new Pig data set. \n",
    "\n",
    "In the following two code cells, we first create the pig script to\n",
    "perform these operations, after which we run this script by using Pig in\n",
    "local mode. Note that we could also employ the Pig `SAMPLE` command,\n",
    "which randomly samples from the data set to achieve the same result.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/ratings.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/ratings.pig\n",
    "\n",
    "ratings = LOAD 'ml-latest-small/ratings.csv' USING PigStorage(',')\n",
    "    AS (userID:int, mnovieID:int, rating:double, timestamp:int) ;\n",
    "DESCRIBE ratings ;\n",
    "ILLUSTRATE ratings ;\n",
    "top_rows = LIMIT ratings 10 ;\n",
    "DUMP top_rows ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings: {userID: int,mnovieID: int,rating: double,timestamp: int}\n",
      "(5,30707,4.5,1163374340)\n",
      "-----------------------------------------------------------------------------------\n",
      "| ratings     | userID:int   | mnovieID:int   | rating:double   | timestamp:int   | \n",
      "-----------------------------------------------------------------------------------\n",
      "|             | 5            | 30707          | 4.5             | 1163374340      | \n",
      "-----------------------------------------------------------------------------------\n",
      "\n",
      "(1,31,2.5,1260759144)\n",
      "(1,1029,3.0,1260759179)\n",
      "(1,1061,3.0,1260759182)\n",
      "(1,1129,2.0,1260759185)\n",
      "(1,1172,4.0,1260759205)\n",
      "(1,1263,2.0,1260759151)\n",
      "(1,1287,2.0,1260759187)\n",
      "(1,1293,2.0,1260759148)\n",
      "(1,1339,3.5,1260759125)\n",
      "(1,1343,2.0,1260759131)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "pig -x local -f ratings.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Pig: Join\n",
    "\n",
    "We can use a _join_ operation with Pig to combine rows from multiple\n",
    "tables, in the same manner as a SQL Join. While the full Pig join syntax\n",
    "can be complicated, especially if outer joins are involved, we\n",
    "demonstrate a simple inner join in the following two code cells. The\n",
    "_join_ operation syntax is straightforward, we specify the first data\n",
    "set and the common column followed by the second data set and the common\n",
    "column. For example, to join two data sets on the _movie id_ column, we\n",
    "employ the following Pig command:\n",
    "\n",
    "```pig\n",
    "movie_ratings = JOIN ratings by movieID, movies by movieID ;\n",
    "```\n",
    "\n",
    "In the following two code cells, we first create the pig script to\n",
    "perform this join operation, after which we run this script by using Pig\n",
    "in local mode. We also use the `DESCRIBE` and `DUMP` commands to display\n",
    "the schema for the new joined data set, as well as the first few rows of\n",
    "the joined data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/join.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/join.pig\n",
    "\n",
    "ratings = LOAD 'ml-latest-small/ratings.csv' USING PigStorage(',')\n",
    "    AS (userID:int, movieID:int, rating:double, timestamp:int) ;\n",
    "\n",
    "movies = LOAD 'ml-latest-small/movies.csv' USING PigStorage(',')\n",
    "    AS (movieID:int, title:chararray, genre:chararray) ;\n",
    "\n",
    "movie_ratings = JOIN ratings by movieID, movies by movieID ;\n",
    "\n",
    "DESCRIBE movie_ratings ;\n",
    "top_rows = LIMIT movie_ratings 10 ;\n",
    "DUMP top_rows ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_ratings: {ratings::userID: int,ratings::movieID: int,ratings::rating: double,ratings::timestamp: int,movies::movieID: int,movies::title: chararray,movies::genre: chararray}\n",
      "(136,1,4.5,1405977265,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(138,1,2.0,1440379001,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(185,1,5.0,1003523268,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(272,1,4.0,1453587590,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(273,1,4.5,1466945564,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(275,1,5.0,1350254117,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(447,1,3.0,832492847,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(520,1,3.0,1142028487,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(538,1,5.0,831835670,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n",
      "(562,1,4.5,1167426662,1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "pig -x local -b -f join.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Pig: Group By\n",
    "\n",
    "Pig allows us to perform a _group by_ operation, which creates a summary\n",
    "data set. In this example, we use `GROUP ... BY` to group all reviews\n",
    "for a specific movie, before we join with the movie titles. The syntax\n",
    "of this command is simple, we group a data set by a specific column, for\n",
    "example:\n",
    "\n",
    "```pig\n",
    "hr_group = GROUP high_ratings BY movieID ;\n",
    "```\n",
    "\n",
    "creates a new data set that the highly rated movies group by movie id.\n",
    "In the following two code cells, we first create the pig script to\n",
    "perform this full operation, after which we run this script by using Pig\n",
    "in local mode.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/join-group.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/join-group.pig\n",
    "\n",
    "ratings = LOAD 'ml-latest-small/ratings.csv' USING PigStorage(',')\n",
    "    AS (userID:int, movieID:int, rating:double, timestamp:int) ;\n",
    "\n",
    "high_ratings = FILTER ratings BY rating > 3 ;\n",
    "\n",
    "hr_group = GROUP high_ratings BY movieID ;\n",
    "\n",
    "hr_count = FOREACH hr_group GENERATE group AS mvID, COUNT(high_ratings) AS cnt ;\n",
    "\n",
    "movies = LOAD 'ml-latest-small/movies.csv' USING PigStorage(',')\n",
    "    AS (movieID:int, title:chararray, genre:chararray) ;\n",
    "\n",
    "movie_ratings = JOIN hr_count by mvID, movies by movieID ;\n",
    "\n",
    "ordered_movies = ORDER movie_ratings BY cnt DESC ;\n",
    "\n",
    "top_movies = LIMIT ordered_movies 10 ;\n",
    "\n",
    "DESCRIBE top_movies ;\n",
    "\n",
    "DUMP top_movies ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_movies: {hr_count::mvID: int,hr_count::cnt: long,movies::movieID: int,movies::title: chararray,movies::genre: chararray}\n",
      "(318,286,318,\"Shawshank Redemption, The (1994)\")\n",
      "(356,273,356,Forrest Gump (1994),Comedy|Drama|Romance|War)\n",
      "(296,269,296,Pulp Fiction (1994),Comedy|Crime|Drama|Thriller)\n",
      "(593,259,593,\"Silence of the Lambs, The (1991)\")\n",
      "(260,247,260,Star Wars: Episode IV - A New Hope (1977),Action|Adventure|Sci-Fi)\n",
      "(2571,219,2571,\"Matrix, The (1999)\")\n",
      "(527,214,527,Schindler's List (1993),Drama|War)\n",
      "(1196,201,1196,Star Wars: Episode V - The Empire Strikes Back (1980),Action|Adventure|Sci-Fi)\n",
      "(1198,198,1198,Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981),Action|Adventure)\n",
      "(608,195,608,Fargo (1996),Comedy|Crime|Drama|Thriller)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd $HOME/hadoop\n",
    "\n",
    "pig -x local -b -f join-group.pig 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Pig Cleanup\n",
    "\n",
    "After running these examples, you will have a number of files that\n",
    "should be cleaned up, especially if you wish to rerun this Notebook.\n",
    "These files include Pig log files, our Pig scripts, the MovieLens\n",
    "data, and finally, the Pig output. The following code cell deletes all\n",
    "of these files; you can modify this cell to leave some or all of these\n",
    "files if you wish to analyze anything in more detail.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1560\n",
      "drwxr-xr-x 2 data_scientist users    4096 Apr  1 21:13 .\n",
      "drwxr-xr-x 1 data_scientist users    4096 Apr  1 21:05 ..\n",
      "-rw-r--r-- 1 data_scientist users 1580927 Nov 21 11:09 book.txt\n",
      "-rwxr--r-- 1 data_scientist users     849 Apr  1 21:06 mapper.py\n",
      "-rwxr--r-- 1 data_scientist users    1496 Apr  1 21:06 reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Clean up the working directory (Don't run to save data for later analysis)\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# Remove pig log files\n",
    "rm -f pig*.log\n",
    "\n",
    "# Remove our pig scripts\n",
    "rm -f *.pig\n",
    "\n",
    "# Remove the movieLens Data\n",
    "rm -f ml-latest-small.zip\n",
    "rm -rf ml-latest-small\n",
    "\n",
    "# Remove our output file.\n",
    "rm -rf top_words\n",
    "\n",
    "# Display cleaned directory contents\n",
    "ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [eeacbe10081d]\n",
      "eeacbe10081d: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n"
     ]
    }
   ],
   "source": [
    "# Having the namenode and datanodes running in the background consumes quite a bit of memory. \n",
    "# So we should shut down the nodes at the end of the notebook.\n",
    "\n",
    "!$HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced Hadoop Pig. Now that you have run\n",
    "the Notebook, go back and run it a second time. Notice how the data and\n",
    "thus model fits have changed.\n",
    "\n",
    "1. Change the first Pig example to compute bi-grams as opposed to\n",
    "unigrams.\n",
    "2. Change the MovieLens example to compute the average rating for each\n",
    "Movie.\n",
    "3. Generate a Recommendation system (similar to our example in a\n",
    "previous Notebook) by using Pig.\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
