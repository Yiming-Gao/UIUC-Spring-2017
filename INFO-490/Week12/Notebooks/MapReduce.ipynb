{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Map/Reduce\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we introduce the map/reduce programming\n",
    "paradigm. Simply put, this approach to computing breaks tasks down into\n",
    "a map phase (where an algorithm is mapped onto data) and a reduce phase,\n",
    "where the outputs of the map phase are aggregated into a concise output.\n",
    "The map phase is designed to be parallel, and to move the computation to\n",
    "the data, which, when using HDFS, can be widely distributed. In this\n",
    "case, a map phase can be executed against a large quantity of data very\n",
    "quickly. The map phase identifies keys and associates with them a value.\n",
    "The reduce phase collects keys and aggregates their values. The standard\n",
    "example used to demonstrate this programming approach is a word count\n",
    "problem, where words (or tokens) are the keys) and the number of\n",
    "occurrences of each word (or token) is the value.\n",
    "\n",
    "As this technique was popularized by large web search companies like\n",
    "Google and Yahoo who were processing large quantities of unstructured\n",
    "text data, this approach quickly became popular for a wide range of\n",
    "problems.  Of course, not every problem can be transformed into a\n",
    "map-reduce approach, which is why we will explore Spark in several\n",
    "weeks. The standard MapReduce approach uses Hadoop, which was built\n",
    "using Java. Rather than switching to a new language, however, we will\n",
    "use Hadoop Streaming to execute Python code. In the rest of this\n",
    "notebook, we introduce a simple Python WordCount example code. We first\n",
    "demonstrate this code running at the Unix command line, before switching\n",
    "to running the code by using Hadoop Streaming.\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Mapper: Word Count\n",
    "\n",
    "The first Python code we will write is the map Python program. This\n",
    "program simply reads data from STDIN, tokenizes each line into words and\n",
    "outputs each word on a separate line along with a count of one. Thus our\n",
    "map program generates a list of word tokens as the keys and the value is\n",
    "always one.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# These examples are based off the blog post by Michale Noll:\n",
    "# \n",
    "# http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\n",
    "#\n",
    "\n",
    "import sys\n",
    "\n",
    "# We explicitly define the word/count separator token.\n",
    "sep = '\\t'\n",
    "\n",
    "# We open STDIN and STDOUT\n",
    "with sys.stdin as fin:\n",
    "    with sys.stdout as fout:\n",
    "    \n",
    "        # For every line in STDIN\n",
    "        for line in fin:\n",
    "        \n",
    "            # Strip off leading and trailing whitespace\n",
    "            line = line.strip()\n",
    "            \n",
    "            # We split the line into word tokens. Use whitespace to split.\n",
    "            # Note we don't deal with punctuation.\n",
    "            \n",
    "            words = line.split()\n",
    "            \n",
    "            # Now loop through all words in the line and output\n",
    "\n",
    "            for word in words:\n",
    "                fout.write(\"{0}{1}1\\n\".format(word, sep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Reducer: Word Count\n",
    "\n",
    "The second Python program we write is our reduce program. In this code,\n",
    "we read key-value pairs from STDIN and use the fact that the Hadoop\n",
    "process first sorts all key-value pairs before sending the map output to\n",
    "the reduce process to accumulate the cumulative count of each word. The\n",
    "following code could easily be made more sophisticated by using `yield`\n",
    "statements and iterators, but for clarity we use the simple approach of\n",
    "tracking when the current word becomes different than the previous word\n",
    "to output the key-cumulative count pairs.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /home/data_scientist/hadoop/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/data_scientist/hadoop/reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# We explicitly define the word/count separator token.\n",
    "sep = '\\t'\n",
    "\n",
    "# We open STDIN and STDOUT\n",
    "with sys.stdin as fin:\n",
    "    with sys.stdout as fout:\n",
    "    \n",
    "        # Keep track of current word and count\n",
    "        cword = None\n",
    "        ccount = 0\n",
    "        word = None\n",
    "   \n",
    "        # For every line in STDIN\n",
    "        for line in fin:\n",
    "        \n",
    "            # Strip off leading and trailing whitespace\n",
    "            # Note by construction, we should have no leading white space\n",
    "            line = line.strip()\n",
    "            \n",
    "            # We split the line into a word and count, based on predefined\n",
    "            # separator token.\n",
    "            #\n",
    "            # Note we haven't dealt with punctuation.\n",
    "            \n",
    "            word, scount = line.split('\\t', 1)\n",
    "            \n",
    "            # We will assume count is always an integer value\n",
    "            \n",
    "            count = int(scount)\n",
    "            \n",
    "            # word is either repeated or new\n",
    "            \n",
    "            if cword == word:\n",
    "                ccount += count\n",
    "            else:\n",
    "                # We have to handle first word explicitly\n",
    "                if cword != None:\n",
    "                    fout.write(\"{0:s}{1:s}{2:d}\\n\".format(cword, sep, ccount))\n",
    "                \n",
    "                # New word, so reset variables\n",
    "                cword = word\n",
    "                ccount = count\n",
    "        else:\n",
    "            # Output final word count\n",
    "            if cword == word:\n",
    "                fout.write(\"{0:s}{1:s}{2:d}\\n\".format(word, sep, ccount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Testing Python Map-Reduce\n",
    "\n",
    "Before we begin using Hadoop, we should first test our Python codes out\n",
    "to ensure they work as expected. First, we should change the permissions\n",
    "of the two programs to be executable, which we can do with the Unix\n",
    "`chmod` command.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1560\n",
      "drwxr-xr-x 2 data_scientist users    4096 Apr  1 21:06 .\n",
      "drwxr-xr-x 1 data_scientist users    4096 Apr  1 21:05 ..\n",
      "-rw-r--r-- 1 data_scientist users 1580927 Nov 21 11:09 book.txt\n",
      "-rwxr--r-- 1 data_scientist users     849 Apr  1 21:06 mapper.py\n",
      "-rwxr--r-- 1 data_scientist users    1496 Apr  1 21:06 reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "chmod u+x /home/data_scientist/hadoop/mapper.py\n",
    "chmod u+x /home/data_scientist/hadoop/reducer.py\n",
    "\n",
    "ls -la /home/data_scientist/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Testing Mapper.py\n",
    "\n",
    "To test out the map Python code, we can run the Python `mapper.py` code\n",
    "and specify that the code should redirect STDIN to read the book text\n",
    "data. This is done in the following code cell, we pipe the output into\n",
    "the Unix `head` command in order to restrict the output, which would be\n",
    "one line per word found in the book text file. In the second code cell,\n",
    "we next pipe the output of  `mapper.py` into the Unix `sort` command,\n",
    "which is done automatically by Hadoop. To see the result of this\n",
    "operation, we next pipe the result into the Unix `uniq` command to count\n",
    "duplicates, pipe this result into a new sort routine to sort the output\n",
    "by the number of occurrences of a word, and finally display the last few\n",
    "lines with the Unix `tail` command to verify the program is operating\n",
    "correctly.\n",
    "\n",
    "With these sequence of Unix commands, we have (in a single-node)\n",
    "replicated the steps performed by Hadoop MapReduce: Map, Sort, and\n",
    "Reduce.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267949\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2391 with\t1\n",
      "   2430 I\t1\n",
      "   2712 he\t1\n",
      "   3035 his\t1\n",
      "   4619 in\t1\n",
      "   4790 to\t1\n",
      "   5841 a\t1\n",
      "   6551 and\t1\n",
      "   8134 of\t1\n",
      "  13608 the\t1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | sort -n -k 1 | \\\n",
    " uniq -c -d | sort -n -k 1 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "#### Testing Reducer.py\n",
    "\n",
    "To test out the reduce Python code, we run the previous code cell, but\n",
    "rather than piping the result into the Unix `tail` command, we pipe the\n",
    "result of the sort command into the Python `reducer.py` code. This\n",
    "simulates the Hadoop model, where the map output is key sorted before\n",
    "being passed into the reduce process. First, we will simply count the\n",
    "number of lines displayed by the reduce process, which will indicate the\n",
    "number of  unique _word tokens_ in the book. Next, we will sort the\n",
    "output by the number of times each word token appears and display the\n",
    "last few lines to compare with the previous results.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49316\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | sort -n -k 1 | \\\n",
    "./reducer.py | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with\t2391\n",
      "I\t2430\n",
      "he\t2712\n",
      "his\t3035\n",
      "in\t4619\n",
      "to\t4790\n",
      "a\t5841\n",
      "and\t6551\n",
      "of\t8134\n",
      "the\t13608\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/data_scientist/hadoop\n",
    "\n",
    "./mapper.py <  book.txt | sort -n -k 1 | \\\n",
    "./reducer.py | sort -n -k 2 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Setup Local Hadoop Environment\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [eeacbe10081d]\n",
      "eeacbe10081d: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: no secondarynamenode to stop\n",
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n",
      "rm: cannot remove ‘/tmp/hsperfdata_root’: Operation not permitted\n",
      "Formatting using clusterid: CID-a0918add-0e43-4739-a0e0-e574759bd7ed\n"
     ]
    }
   ],
   "source": [
    "# make sure we stop the namenode and datanodes if there are any running from previous run\n",
    "!$HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/stop-yarn.sh\n",
    "\n",
    "# Clean up temp files if there are any created during the previous Hadoop operation.\n",
    "!rm -rf /tmp/*\n",
    "\n",
    "# Format the namenode and delete all files in our HDFS.\n",
    "!echo \"Y\" | $HADOOP_PREFIX/bin/hdfs namenode -format 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [eeacbe10081d]\n",
      "eeacbe10081d: starting namenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-namenode-eeacbe10081d.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-datanode-eeacbe10081d.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-secondarynamenode-eeacbe10081d.out\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-eeacbe10081d.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-data_scientist-nodemanager-eeacbe10081d.out\n"
     ]
    }
   ],
   "source": [
    "# Restart namenode and datanodes\n",
    "!$HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n",
    "!$HADOOP_PREFIX/sbin/start-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe mode is OFF\r\n"
     ]
    }
   ],
   "source": [
    "# Sometimes when the namenode is restarted, it enteres Safe Mode, \n",
    "# not allowing any changes to the file system. \n",
    "# We do want to make changes, so we manually leave Safe Mode.\n",
    "\n",
    "!$HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave\n",
    "\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/$NB_USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Python Hadoop Streaming\n",
    "\n",
    "We are now ready to actually run our Python codes via Hadoop Streaming.\n",
    "The main command to perform this task is `$HADOOP_PREFIX/bin/hadoop jar\n",
    "hadoop-streaming-X.X.X.jar`, where the current version of the streaming\n",
    "jar file is `hadoop-streaming-2.7.2.jar` as shown in the following code\n",
    "cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "Running this Hadoop command by supplying the `-help` flag will provide\n",
    "a useful summary of the different options. Note that `jar` is short for\n",
    "Java Archive, which is a compressed archive of compiled Java code that\n",
    "can be executed to perform different operations. In this case, we will\n",
    "run the Java Hadoop streaming jar file to enable our Python code to work\n",
    "within Hadoop.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]\r\n",
      "  CLASSNAME            run the class named CLASSNAME\r\n",
      " or\r\n",
      "  where COMMAND is one of:\r\n",
      "  fs                   run a generic filesystem user client\r\n",
      "  version              print the version\r\n",
      "  jar <jar>            run a jar file\r\n",
      "                       note: please use \"yarn jar\" to launch\r\n",
      "                             YARN applications, not this command.\r\n",
      "  checknative [-a|-h]  check native hadoop and compression libraries availability\r\n",
      "  distcp <srcurl> <desturl> copy file or directories recursively\r\n",
      "  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\r\n",
      "  classpath            prints the class path needed to get the\r\n",
      "  credential           interact with credential providers\r\n",
      "                       Hadoop jar and the required libraries\r\n",
      "  daemonlog            get/set the log level for each daemon\r\n",
      "  trace                view and modify Hadoop tracing settings\r\n",
      "\r\n",
      "Most commands print help when invoked w/o parameters.\r\n"
     ]
    }
   ],
   "source": [
    "# Run the Map Reduce task within Hadoop\n",
    "!$HADOOP_PREFIX/bin/hadoop --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "For our map/reduce Python example to\n",
    "run successfully, we will need to specify five flags:\n",
    "\n",
    "1. `-files`: a comma separated list of files to be copied to the Hadoop cluster.\n",
    "2. `-input`: the HDFS input file(s) to be used for the map task.\n",
    "3. `-output`: the HDFS output directory, used for the reduce task.\n",
    "4. `-mapper`: the command to run for the map task.\n",
    "5. `-reducer`: the command to run for the reduce task.\n",
    "\n",
    "Given our previous setup, we will run the full command as follows:\n",
    "\n",
    "    $HADOOP_PREFIX/bin/hadoop jar hs.jar -files mapper.py,reducer.py -input wc/in \\\n",
    "        -output wc/out -mapper mapper.py -reducer reducer.py \n",
    "\n",
    "When this command is run, a series of messages will be displayed to the\n",
    "screen (via STDERR) showing the progress of our Hadoop Streaming task.\n",
    "At the end of the stream of information messages will be a statement\n",
    "indicating the location of the output directory as shown below. Note, we\n",
    "can append Bash redirection to ignore the Hadoop messages, simply by\n",
    "appending `2> /dev/null` to the end of any Hadoop command, which sends\n",
    "all STDERR messages to a non-existent Unix device, which is akin to\n",
    "nothing. \n",
    "\n",
    "For example, to ignore any messages from the `hdfs dfs -rm -r -f wc/out`\n",
    "command, we would use the following syntax:\n",
    "\n",
    "```bash\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f wc/out 2> /dev/null\n",
    "```\n",
    "\n",
    "Doing this, however, does hide all messages, which can make debugging\n",
    "problems more difficult. As a result, you should only do this when your\n",
    "commands work correctly and you want to improve the appearance of your\n",
    "Notebook.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f wc\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p wc/in\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -put $HOME/hadoop/book.txt wc/in/book.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar5345246585859287398/] [] /tmp/streamjob7104143533387876812.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17/04/01 21:07:38 INFO client.RMProxy: Connecting to ResourceManager at eeacbe10081d/172.17.0.2:8032\n",
      "17/04/01 21:07:38 INFO client.RMProxy: Connecting to ResourceManager at eeacbe10081d/172.17.0.2:8032\n",
      "17/04/01 21:07:39 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/04/01 21:07:39 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "17/04/01 21:07:39 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1491080842837_0001\n",
      "17/04/01 21:07:39 INFO impl.YarnClientImpl: Submitted application application_1491080842837_0001\n",
      "17/04/01 21:07:39 INFO mapreduce.Job: The url to track the job: http://eeacbe10081d:8088/proxy/application_1491080842837_0001/\n",
      "17/04/01 21:07:39 INFO mapreduce.Job: Running job: job_1491080842837_0001\n",
      "17/04/01 21:07:46 INFO mapreduce.Job: Job job_1491080842837_0001 running in uber mode : false\n",
      "17/04/01 21:07:46 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/04/01 21:07:51 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/04/01 21:07:57 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/04/01 21:07:57 INFO mapreduce.Job: Job job_1491080842837_0001 completed successfully\n",
      "17/04/01 21:07:57 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2609527\n",
      "\t\tFILE: Number of bytes written=5583853\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1585245\n",
      "\t\tHDFS: Number of bytes written=522427\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6274\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2754\n",
      "\t\tTotal time spent by all map tasks (ms)=6274\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2754\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6274\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2754\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6424576\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2820096\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=32577\n",
      "\t\tMap output records=267948\n",
      "\t\tMap output bytes=2073625\n",
      "\t\tMap output materialized bytes=2609533\n",
      "\t\tInput split bytes=222\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=49315\n",
      "\t\tReduce shuffle bytes=2609533\n",
      "\t\tReduce input records=267948\n",
      "\t\tReduce output records=49315\n",
      "\t\tSpilled Records=535896\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=181\n",
      "\t\tCPU time spent (ms)=3950\n",
      "\t\tPhysical memory (bytes) snapshot=680509440\n",
      "\t\tVirtual memory (bytes) snapshot=5874495488\n",
      "\t\tTotal committed heap usage (bytes)=486539264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1585023\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=522427\n",
      "17/04/01 21:07:57 INFO streaming.StreamJob: Output directory: wc/out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Change into correct working directory\n",
    "cd $HOME/hadoop\n",
    "\n",
    "# Delete output directory (if it exists)\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f wc/out\n",
    "\n",
    "# Grab current streaming lib jar filename\n",
    "streaming_file=$(ls $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming*)\n",
    "\n",
    "# Run the Map Reduce task within Hadoop\n",
    "$HADOOP_PREFIX/bin/hadoop jar $streaming_file \\\n",
    "    -files mapper.py,reducer.py -input wc/in \\\n",
    "    -output wc/out -mapper mapper.py -reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Hadoop Results\n",
    "\n",
    "In order to view the results of our Hadoop Streaming task, we must use\n",
    "HDFS DFS commands to examine the directory and files generated by our\n",
    "Python Map/Reduce programs. The following list of DFS commands might\n",
    "prove useful to view the results of this map/reduce job.\n",
    "\n",
    "```bash\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -ls wc\n",
    "\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -ls wc/out\n",
    "\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -count -h wc/out/part-00000\n",
    "\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -tail wc/out/part-00000\n",
    "```\n",
    "\n",
    "We demonstrate using several of these commands below. Note that these\n",
    "Hadoop HDFS commands can be intermixed with Unix commands to perform\n",
    "additional text processing. The important point is that direct file I/O\n",
    "operations must use HDFS commands to work with the HDFS file system.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 data_scientist supergroup          0 2017-04-01 21:07 wc/out/_SUCCESS\r\n",
      "-rw-r--r--   1 data_scientist supergroup     522427 2017-04-01 21:07 wc/out/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -ls wc/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0            1            510.2 K wc/out/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -count -h wc/out/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Come\t1\r\n",
      "“Defects,”\t1\r\n",
      "“I\t1\r\n",
      "“Information\t1\r\n",
      "“J”\t1\r\n",
      "“Plain\t2\r\n",
      "“Project\t5\r\n",
      "“Right\t1\r\n",
      "“Viator”\t1\r\n",
      "•\t1\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -tail wc/out/part-00000 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "To compare this map/reduce Hadoop Streaming task output to our previous\n",
    "python only output, we must apply several Unix commands as follows:\n",
    "\n",
    "```bash\n",
    "`$HADOOP_PREFIX/bin/hdfs dfs -cat wc/out/part-00000 | sort -n -k 2 | tail -10\n",
    "\n",
    "```\n",
    "\n",
    "This is demonstrated below, where the output should match the Python\n",
    "only map-reduce approach.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with\t2391\r\n",
      "I\t2430\r\n",
      "he\t2712\r\n",
      "his\t3035\r\n",
      "in\t4619\r\n",
      "to\t4790\r\n",
      "a\t5841\r\n",
      "and\t6551\r\n",
      "of\t8134\r\n",
      "the\t13608\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/bin/hdfs dfs -cat wc/out/part-00000 | sort -n -k 2 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop Cleanup\n",
    "\n",
    "Following the successful run of our map/reduce Python programs, we have\n",
    "created a new directory `wc/out`, which contains two files. If we wish\n",
    "to rerun this Hadoop Streaming map/reduce task, we must either specify a\n",
    "different output directory, or else we must clean up the results of the\n",
    "previous run. To remove the output directory, we can simply use the HDFS\n",
    "`-rm -r -f wc/out` command, which will immediately delete the `wc/out`\n",
    "directory. The successful completion of this command is indicated by\n",
    "Hadoop, and this can also be verified by listing the contents of the\n",
    "`wc` directory.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [eeacbe10081d]\n",
      "eeacbe10081d: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n"
     ]
    }
   ],
   "source": [
    "# !$HADOOP_PREFIX/bin/hdfs dfs -r -f wc/out\n",
    "\n",
    "# Having the namenode and datanodes running in the background consumes quite a bit of memory. \n",
    "# So I think we should shut down the nodes at the end of the notebook:\n",
    "\n",
    "!$HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced Hadoop map/reduce by using a\n",
    "simple word count task. Now that you have run the Notebook, go back and\n",
    "make the following changes to see how the results change.\n",
    "\n",
    "1. We ignored punctuation, modify the original mapper Python code to\n",
    "token on white space or punctuation. How does this change the Python\n",
    "map-reduce output?\n",
    "2. Try downloading a different text from Project Gutenberg. Can you make\n",
    "your map-reduce application run across multiple texts?\n",
    "3. Can you make your map-reduce code compute bi-grams instead of\n",
    "unigrams?\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
