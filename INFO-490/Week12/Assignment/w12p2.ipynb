{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1891bdcd87c6c960df75b6e90b02b234",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If you are not using the `Assignments` tab on the course JupyterHub server to read this notebook, read [Activating the assignments tab](https://github.com/lcdm-uiuc/info490-sp17/blob/master/help/act_assign_tab.md).\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "197039b9b8899dc1d4ed57d5a26aee17",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Problem 12.2. MapReduce.\n",
    "\n",
    "In this problem, we will use Hadoop Streaming to execute a MapReduce code written in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "f031162e2abc7fcf51f9dd275606ff76",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nose.tools import assert_equal, assert_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54f56796e10423a3f721bb427b1abd5c",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will use the [airline on-time performance data](http://stat-computing.org/dataexpo/2009/), but before proceeding, recall that the data set is encoded in `latin-1`. However, the Python 3 interpreter expects the standard input and output to be in `utf-8` encoding. Thus, we have to explicitly state that the Python interpreter should use `latin-1` for all IO operations, which we can do by setting the Python environment variable `PYTHONIOENCODING` equal to `latin-1`. We can set the environment variables of the current IPython kernel by modifying the `os.environ` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3e7906fde5d4b7f27e9927d261b3f089",
     "grade": false,
     "grade_id": "os_environ",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "os.environ['PYTHONIOENCODING'] = 'latin-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c65ffad078dda3747245ec34b0791730",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Let's use the shell to check if the variable is set correctly. If you are not familiar with the following syntax (i.e., Python variable = ! shell command), [this notebook](https://github.com/UI-DataScience/info490-fa15/blob/master/Week4/assignment/unix_ipython.ipynb) from the previous semester might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8de6d2887a1792f2183e070174de9b84",
     "grade": false,
     "grade_id": "pythonioencoding",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "python_io_encoding = ! echo $PYTHONIOENCODING\n",
    "assert_equal(python_io_encoding.s, 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "56bc8908e5a07f21657ea08cc9d4ad20",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Mapper\n",
    "\n",
    "Write a Python script that\n",
    "  - Reads data from `STDIN`,\n",
    "  - Skips the first line (The first line of `2001.csv` is the header that has the column titles.)\n",
    "  - Outputs to `STDOUT` the `Origin` and `AirTime` columns separated with a tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5462e16b875b4b848820898cc6083cb3",
     "grade": false,
     "grade_id": "mapper_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# YOUR CODE HERE\n",
    "with sys.stdin as fin:\n",
    "    next(fin)  # skip header\n",
    "    with sys.stdout as fout:\n",
    "        \n",
    "        # for every line in STDIN\n",
    "        for line in fin:    \n",
    "            \n",
    "            # Strip off leading and trailing whitespace\n",
    "            line = line.strip()\n",
    "            \n",
    "            # We split the line into word tokens. Use whitespace to split.\n",
    "            # Note we don't deal with punctuation.\n",
    "            words = line.split(',')\n",
    "            \n",
    "            # origin(16), Airtime(13)\n",
    "            fout.write(\"{0}\\t{1}\\n\".format(words[16], words[13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "74b6b924b70d3777c7d46159dc6b3e8f",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We need make the file executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "45db6ad323b7869347842777aaadb992",
     "grade": false,
     "grade_id": "chmod_mapper",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! chmod u+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4438b3a411c0254c27b392517414a511",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Before testing the mapper code on the entire data set, let's first create a small file and test our code on this small data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "06147b916a7f5c463db26b61271f3166",
     "grade": false,
     "grade_id": "head_csv",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWI\t60\n",
      "BWI\t64\n",
      "BWI\t80\n",
      "BWI\t66\n",
      "BWI\t62\n",
      "BWI\t61\n",
      "BWI\t61\n",
      "BWI\t60\n",
      "BWI\t52\n",
      "BWI\t62\n",
      "BWI\t62\n",
      "BWI\t55\n",
      "BWI\t60\n",
      "BWI\t61\n",
      "BWI\t63\n",
      "PHL\t53\n",
      "PHL\t54\n",
      "PHL\t55\n",
      "PHL\t53\n",
      "PHL\t50\n",
      "PHL\tNA\n",
      "PHL\t57\n",
      "PHL\t48\n",
      "PHL\t56\n",
      "PHL\t55\n",
      "PHL\t55\n",
      "PHL\t55\n",
      "PHL\t55\n",
      "PHL\t49\n",
      "PHL\t75\n",
      "PHL\t49\n",
      "PHL\t50\n",
      "PHL\t49\n",
      "PHL\tNA\n",
      "PHL\t46\n",
      "PHL\tNA\n",
      "PHL\t51\n",
      "PHL\t53\n",
      "PHL\t52\n",
      "PHL\t52\n",
      "PHL\t54\n",
      "PHL\t56\n",
      "PHL\t55\n",
      "PHL\t51\n",
      "PHL\t49\n",
      "PHL\t49\n",
      "CLT\t82\n",
      "CLT\t82\n",
      "CLT\t78\n"
     ]
    }
   ],
   "source": [
    "! head -n 50 $HOME/data/2001.csv > 2001.csv.head\n",
    "map_out_head = ! ./mapper.py < 2001.csv.head\n",
    "print('\\n'.join(map_out_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "fa9dc67a4280d4e19f4aaae48d38554f",
     "grade": true,
     "grade_id": "mapper_test",
     "locked": true,
     "points": 6,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(\n",
    "    map_out_head,\n",
    "    ['BWI\\t60','BWI\\t64','BWI\\t80','BWI\\t66','BWI\\t62','BWI\\t61',\n",
    "     'BWI\\t61','BWI\\t60','BWI\\t52','BWI\\t62','BWI\\t62','BWI\\t55',\n",
    "     'BWI\\t60','BWI\\t61','BWI\\t63','PHL\\t53','PHL\\t54','PHL\\t55',\n",
    "     'PHL\\t53','PHL\\t50','PHL\\tNA','PHL\\t57','PHL\\t48','PHL\\t56',\n",
    "     'PHL\\t55','PHL\\t55','PHL\\t55','PHL\\t55','PHL\\t49','PHL\\t75',\n",
    "     'PHL\\t49','PHL\\t50','PHL\\t49','PHL\\tNA','PHL\\t46','PHL\\tNA',\n",
    "     'PHL\\t51','PHL\\t53','PHL\\t52','PHL\\t52','PHL\\t54','PHL\\t56',\n",
    "     'PHL\\t55','PHL\\t51','PHL\\t49','PHL\\t49','CLT\\t82','CLT\\t82',\n",
    "     'CLT\\t78']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a7b16d35034aff5d24ab9c626d05920f",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Reducer\n",
    "\n",
    "Write a Python script that\n",
    "\n",
    "  - Reads key-value pairs from `STDIN`,\n",
    "  - Computes the minimum and maximum air time for flights, with respect to each origin airport,\n",
    "  - Outputs to `STDOUT` the airports and the minimum and maximum air time for flights at each airport, separated with tabs.\n",
    "  \n",
    "For example,\n",
    "\n",
    "```shell\n",
    "$ ./mapper.py < 2001.csv.head | sort -n -k 1 | ./reducer.py\n",
    "```\n",
    "\n",
    "should give\n",
    "\n",
    "```\n",
    "BWI\t52\t80\n",
    "CLT\t78\t82\n",
    "PHL\t46\t75\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "577e94ce77f20e9fcab4548473e6068b",
     "grade": false,
     "grade_id": "reducer_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# YOUR CODE HERE\n",
    "with sys.stdin as fin:\n",
    "    with sys.stdout as fout:\n",
    "        \n",
    "        # Keep track of current word and count\n",
    "        cword = None\n",
    "        cmin = None\n",
    "        cmax = None\n",
    "        word = None\n",
    "        \n",
    "        # For every line in STDIN\n",
    "        for line in fin:\n",
    "            \n",
    "            # We split the line into a word and count, based on predefined\n",
    "            # separator token.\n",
    "            word = line.split('\\t')[0]\n",
    "            airtime = line.split('\\t')[1]\n",
    "            \n",
    "            if airtime != \"NA\\n\":\n",
    "                \n",
    "                # We will assume airtime is always an integer value\n",
    "                airtime = int(airtime)\n",
    "                \n",
    "                if cword == None:\n",
    "                    cmin = cmax = airtime\n",
    "                    cword = word\n",
    "                    \n",
    "                elif word == cword:\n",
    "                    # compare current value with new value\n",
    "                    cmin = min(cmin, airtime)\n",
    "                    cmax = max(cmax, airtime)\n",
    "                    \n",
    "                else:\n",
    "                    fout.write('%s\\t%d\\t%d\\n' % (cword, cmin, cmax))\n",
    "                    cmin = cmax = airtime\n",
    "                    cword = word\n",
    "        else:\n",
    "            if cword == word:\n",
    "                fout.write('%s\\t%d\\t%d\\n' % (cword, cmin, cmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "116d84eeb57b5821803b1c5dd6079cdd",
     "grade": false,
     "grade_id": "chmod_reducer",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! chmod u+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c32d9b3eb5c3cdd12d43453646c4a41e",
     "grade": false,
     "grade_id": "print_red_head_out",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWI\t52\t80\n",
      "CLT\t78\t82\n",
      "PHL\t46\t75\n"
     ]
    }
   ],
   "source": [
    "red_head_out = ! ./mapper.py < 2001.csv.head | sort -n -k 1 | ./reducer.py\n",
    "print('\\n'.join(red_head_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a4d522b73ff355b7ba4f081572853df1",
     "grade": true,
     "grade_id": "reducer_test",
     "locked": true,
     "points": 6,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(red_head_out, ['BWI\\t52\\t80','CLT\\t78\\t82','PHL\\t46\\t75'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "acacae226783a3ccc0f58e89b87f26c8",
     "grade": false,
     "grade_id": "markdown_7",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If the previous tests on the smaller data set were successful, we can run the mapreduce on the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1b626b777b93117328d274074ae30df2",
     "grade": false,
     "grade_id": "print_mapred_out",
     "locked": true,
     "points": 15,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABE\t16\t180\n",
      "ABI\t28\t85\n",
      "ABQ\t15\t264\n",
      "ACT\t19\t81\n",
      "ACY\t33\t33\n",
      "ADQ\t32\t67\n",
      "AKN\t12\t54\n",
      "ALB\t23\t360\n",
      "AMA\t30\t130\n",
      "ANC\t23\t428\n"
     ]
    }
   ],
   "source": [
    "mapred_out = ! ./mapper.py < $HOME/data/2001.csv | sort -n -k 1 | ./reducer.py\n",
    "print('\\n'.join(mapred_out[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "707e0440f71ecf50fd25df796055a7ed",
     "grade": true,
     "grade_id": "mapred_test",
     "locked": true,
     "points": 15,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(mapred_out), 231)\n",
    "assert_equal(mapred_out[:5], ['ABE\\t16\\t180', 'ABI\\t28\\t85', 'ABQ\\t15\\t264', 'ACT\\t19\\t81', 'ACY\\t33\\t33'])\n",
    "assert_equal(mapred_out[-5:], ['TYS\\t11\\t177', 'VPS\\t28\\t123', 'WRG\\t5\\t38', 'XNA\\t33\\t195', 'YAK\\t28\\t72'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5494cd4c175f076e94595e7cf322f36",
     "grade": false,
     "grade_id": "markdown_8",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## HDFS: Reset\n",
    "\n",
    "We will do some cleaning up before we run Hadoop streaming. Let's first stop the [namenode and datanodes](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9d80b0ad0ee1800ef4bd250fb25d0834",
     "grade": false,
     "grade_id": "stop_dfs",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [info490rb.studentspace.cs.illinois.edu]\n",
      "info490rb.studentspace.cs.illinois.edu: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: no secondarynamenode to stop\n",
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n"
     ]
    }
   ],
   "source": [
    "! $HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "! $HADOOP_PREFIX/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "58b6012aa82d530ffe36420eaf5e8b0d",
     "grade": false,
     "grade_id": "markdown_9",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If there are any temporary files created during the previous Hadoop operation, we want to clean them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d6c4b22eb88a2b1b047ffd31c5f0dee7",
     "grade": false,
     "grade_id": "rm_tmp",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove â€˜/tmp/hsperfdata_rootâ€™: Operation not permitted\r\n"
     ]
    }
   ],
   "source": [
    "! rm -rf /tmp/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cb4cc641b35d74ac705e9e510076acd0",
     "grade": false,
     "grade_id": "markdown_10",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will simply [format the namenode](https://wiki.apache.org/hadoop/GettingStartedWithHadoop#Formatting_the_Namenode) and delete all files in our HDFS. Note that our HDFS is in an ephemeral Docker container, so all data will be lost anyway when the Docker container is shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f45a70c893a0b3bc36054ce691b8b77b",
     "grade": false,
     "grade_id": "format_namenode",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting using clusterid: CID-c1e260a2-131b-408f-b32b-5434d78b5487\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"Y\" | $HADOOP_PREFIX/bin/hdfs namenode -format 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e25ae4525c6108ddefb363a864f53810",
     "grade": false,
     "grade_id": "markdown_11",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "After formatting the namenode, we restart the namenode and datanodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d125ebad14694cb0db192ed38fa992b8",
     "grade": false,
     "grade_id": "start_dfs",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [info490rb.studentspace.cs.illinois.edu]\n",
      "info490rb.studentspace.cs.illinois.edu: starting namenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-namenode-info490rb.studentspace.cs.illinois.edu.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-datanode-info490rb.studentspace.cs.illinois.edu.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-secondarynamenode-info490rb.studentspace.cs.illinois.edu.out\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-info490rb.studentspace.cs.illinois.edu.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-data_scientist-nodemanager-info490rb.studentspace.cs.illinois.edu.out\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n",
    "!$HADOOP_PREFIX/sbin/start-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d27f137ba10b75f650a95f8cdeda3359",
     "grade": false,
     "grade_id": "markdown_12",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Sometimes when the namenode is restarted, it enteres Safe Mode, not allowing any changes to the file system. We do want to make changes, so we manually leave Safe Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "414927d448a62d71d8a59e3660a35a71",
     "grade": false,
     "grade_id": "leave_safemode",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe mode is OFF\r\n"
     ]
    }
   ],
   "source": [
    "! $HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "52d35ba08290de85a816749e7d802358",
     "grade": false,
     "grade_id": "markdown_13",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## HDFS: Create directory\n",
    "\n",
    "- Create a new directory in HDFS at `/user/data_scientist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "87678b3b8b0718be86659d2c63226ef0",
     "grade": false,
     "grade_id": "mkdir_user_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a new directory in HDFS at /user/data_scientist.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/data_scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8ff7f2449db429a227393b7116d5429e",
     "grade": false,
     "grade_id": "print_user_dir",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2017-04-13 08:46 /user/data_scientist\n"
     ]
    }
   ],
   "source": [
    "ls_user = ! $HADOOP_PREFIX/bin/hdfs dfs -ls /user/\n",
    "print('\\n'.join(ls_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "66bd1e3aa00b79ffb656972b7aa35033",
     "grade": true,
     "grade_id": "mkdir_user_test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('/user/data_scientist' in ls_user.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "271d5fc842bc695a6a5e5f4c5ed1e3f4",
     "grade": false,
     "grade_id": "markdown_14",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Create a new directory in HDFS at `/user/data_scientist/wc/in`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d942e42ca13f85183f1f3e60e4ab66e4",
     "grade": false,
     "grade_id": "mkdir_wc_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a new directory in HDFS at `/user/data_scientist/wc/in`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/data_scientist/wc/in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2bb267029f1cdcec08bd2729637d7f7c",
     "grade": false,
     "grade_id": "print_wc_1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2017-04-13 08:46 wc/in\n"
     ]
    }
   ],
   "source": [
    "ls_wc = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc\n",
    "print('\\n'.join(ls_wc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3e2b6c77978895a2c0403c1e2fcc55ac",
     "grade": true,
     "grade_id": "mkdir_wc_test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('wc/in' in ls_wc.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "13625f0e5ff95d5a949eec08a7553a6a",
     "grade": false,
     "grade_id": "markdown_15",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## HDFS: Copy\n",
    "\n",
    "- Copy `/home/data_scientist/data/2001.csv` from local file system into our new HDFS directory `wc/in`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "987b0deca0e75ca26456ff1b7b1a6468",
     "grade": false,
     "grade_id": "put_csv_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Copy `/home/data_scientist/data/2001.csv` from local file system into our new HDFS directory `wc/in`.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -put /home/data_scientist/data/2001.csv /user/data_scientist/wc/in/2001.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1acd1a962a4241fe999e828e68e9041f",
     "grade": false,
     "grade_id": "print_wc_2",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 data_scientist supergroup  600411462 2017-04-13 08:46 wc/in/2001.csv\n"
     ]
    }
   ],
   "source": [
    "ls_wc_in = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc/in\n",
    "print('\\n'.join(ls_wc_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "aade508e65d6b4ae41475d0e7fb5a4d0",
     "grade": true,
     "grade_id": "put_csv_test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('wc/in/2001.csv' in ls_wc_in.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9e6cc3ff68930ecfa168bdda54d6a0d0",
     "grade": false,
     "grade_id": "markdown_16",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Python Hadoop Streaming\n",
    "\n",
    "- Run `mapper.py` and `reducer.py` via Hadoop Streaming.\n",
    "- Use `/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar`.\n",
    "- We need to pass the `PYTHONIOENCODING` environment variable to our Hadoop streaming task. To find out how to set `PYTHONIOENCODING` to `latin-1` in a Hadoop streaming task, use the `--help` and `-info` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a81465f086561001174b6537a2b423c4",
     "grade": false,
     "grade_id": "stream_answer",
     "locked": false,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar1420468990086584161/] [] /tmp/streamjob8339076673355595733.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 4: !ls: command not found\n",
      "17/04/13 08:46:42 INFO client.RMProxy: Connecting to ResourceManager at info490rb.studentspace.cs.illinois.edu/192.168.4.3:8032\n",
      "17/04/13 08:46:43 INFO client.RMProxy: Connecting to ResourceManager at info490rb.studentspace.cs.illinois.edu/192.168.4.3:8032\n",
      "17/04/13 08:46:43 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/04/13 08:46:43 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "17/04/13 08:46:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1492091166032_0001\n",
      "17/04/13 08:46:44 INFO impl.YarnClientImpl: Submitted application application_1492091166032_0001\n",
      "17/04/13 08:46:44 INFO mapreduce.Job: The url to track the job: http://info490rb.studentspace.cs.illinois.edu:8088/proxy/application_1492091166032_0001/\n",
      "17/04/13 08:46:44 INFO mapreduce.Job: Running job: job_1492091166032_0001\n",
      "17/04/13 08:46:51 INFO mapreduce.Job: Job job_1492091166032_0001 running in uber mode : false\n",
      "17/04/13 08:46:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "17/04/13 08:47:00 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "17/04/13 08:47:02 INFO mapreduce.Job:  map 80% reduce 0%\n",
      "17/04/13 08:47:03 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "17/04/13 08:47:10 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "17/04/13 08:47:13 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "17/04/13 08:47:16 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "17/04/13 08:47:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "17/04/13 08:47:20 INFO mapreduce.Job: Job job_1492091166032_0001 completed successfully\n",
      "17/04/13 08:47:20 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=56042349\n",
      "\t\tFILE: Number of bytes written=112815935\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=600428531\n",
      "\t\tHDFS: Number of bytes written=2464\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=5\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=45748\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=16373\n",
      "\t\tTotal time spent by all map tasks (ms)=45748\n",
      "\t\tTotal time spent by all reduce tasks (ms)=16373\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=45748\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=16373\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=46845952\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16765952\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5967781\n",
      "\t\tMap output records=5967776\n",
      "\t\tMap output bytes=44106791\n",
      "\t\tMap output materialized bytes=56042373\n",
      "\t\tInput split bytes=685\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=231\n",
      "\t\tReduce shuffle bytes=56042373\n",
      "\t\tReduce input records=5967776\n",
      "\t\tReduce output records=231\n",
      "\t\tSpilled Records=11935552\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=909\n",
      "\t\tCPU time spent (ms)=30930\n",
      "\t\tPhysical memory (bytes) snapshot=1503657984\n",
      "\t\tVirtual memory (bytes) snapshot=11750506496\n",
      "\t\tTotal committed heap usage (bytes)=1138753536\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=600427846\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2464\n",
      "17/04/13 08:47:20 INFO streaming.StreamJob: Output directory: wc/out\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run Python code via Hadoop streaming\n",
    "\n",
    "# current version of the streaming jar file\n",
    "!ls $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming*\n",
    "\n",
    "\n",
    "# Delete output directory (if it exists)\n",
    "$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f wc/out\n",
    "\n",
    "$HADOOP_PREFIX/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -input wc/in \\\n",
    "    -output wc/out \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -cmdenv PYTHONIOENCODING=latin-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d36e4eb945e5a26ee0d7d2a2e1f67416",
     "grade": false,
     "grade_id": "print_wc_out",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 data_scientist supergroup          0 2017-04-13 08:47 wc/out/_SUCCESS\n",
      "-rw-r--r--   1 data_scientist supergroup       2464 2017-04-13 08:47 wc/out/part-00000\n"
     ]
    }
   ],
   "source": [
    "ls_wc_out = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc/out\n",
    "print('\\n'.join(ls_wc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "df117688327be68ad0fbe76c8a1d0531",
     "grade": true,
     "grade_id": "stream_test_1",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('wc/out/_SUCCESS' in ls_wc_out.s)\n",
    "assert_true('wc/out/part-00000' in ls_wc_out.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "cc22509b73fa2029b07f12dd8c2c7f11",
     "grade": false,
     "grade_id": "print_wc_out_part",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABE\t16\t180\n",
      "ABI\t28\t85\n",
      "ABQ\t15\t264\n",
      "ACT\t19\t81\n",
      "ACY\t33\t33\n",
      "ADQ\t32\t67\n",
      "AKN\t12\t54\n",
      "ALB\t23\t360\n",
      "AMA\t30\t130\n",
      "ANC\t23\t428\n"
     ]
    }
   ],
   "source": [
    "stream_out = ! $HADOOP_PREFIX/bin/hdfs dfs -cat wc/out/part-00000\n",
    "print('\\n'.join(stream_out[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c8bc6f9e83bafd7a8f559a7d995da5f0",
     "grade": true,
     "grade_id": "stream_test_2",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(mapred_out, stream_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "27c6c2b5c6026ed783833b4018ae4730",
     "grade": false,
     "grade_id": "markdown_17",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "aaba4bec2c85276e5400a1e9929d286c",
     "grade": false,
     "grade_id": "cleanup",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted wc/out\r\n"
     ]
    }
   ],
   "source": [
    "! $HADOOP_PREFIX/bin/hdfs dfs -rm -r -f -skipTrash wc/out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
