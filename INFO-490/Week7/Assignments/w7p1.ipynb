{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not using the `Assignments` tab on the course JupyterHub server to read this notebook, read [Activating the assignments tab](https://github.com/lcdm-uiuc/info490-sp17/blob/master/help/act_assign_tab.md).\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_  â†’ _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded.\n",
    "\n",
    "# Problem 7.1. Text Analysis.\n",
    "\n",
    "For this problem, we will be performing text analysis on NLTK's Webtext corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2471218a03d1b40b0734edce23cbd811",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "from nose.tools import (\n",
    "    assert_equal,\n",
    "    assert_is_instance,\n",
    "    assert_almost_equal,\n",
    "    assert_true,\n",
    "    assert_false\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text\n",
    "Before we can really do some analysis on this corpus, we must first tokenize it.  In the `tokenize` function, you will be extracting the raw text from our NLTK corpus for one certain file within, identified by the fileID; you should reference [NLTK](http://www.nltk.org/book/ch02.html#reuters-corpus) for help.  After you have accessed this raw text, you should tokenize the text, making sure to cast everything to lower case and stripping out any errant punctuation marks and spaces.  Additionally, this should only return words that are made of one or more alphanumerical characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8e77a7c04e8180918c97dab4e2b6e716",
     "grade": false,
     "grade_id": "combine",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(corpus, fileID):\n",
    "    '''\n",
    "    Tokenizes the, casting all words to lower case, stripping out punctuation marks, spaces,\n",
    "    and words not made of one or more alphanumerical characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus\n",
    "    fileID: A string\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    words: a list of strings\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # The raw() function gives us the contents of the file without any linguistic processing. \n",
    "    pattern = re.compile(r'[^\\w\\s]')\n",
    "    text = corpus.raw(fileID)\n",
    "    words = [word.lower() for word in nltk.word_tokenize(re.sub(pattern, ' ', text))]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "40fd81aa3a99485bcbfe18259989eec9",
     "grade": true,
     "grade_id": "combine_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "monty = tokenize(webtext, 'grail.txt')\n",
    "assert_is_instance(monty, list)\n",
    "assert_equal(len(monty), 11608)\n",
    "\n",
    "assert_true(all(isinstance(w, str) for w in monty))\n",
    "assert_true(all(all(not c.isupper() for c in w) for w in monty))\n",
    "assert_true(all(any(c.isalnum() for c in w) for w in monty))\n",
    "\n",
    "assert_equal(monty[8:13], ['whoa', 'there', 'clop', 'clop', 'clop'])\n",
    "assert_equal(monty[20:45], ['it', 'is', 'i', 'arthur', 'son', 'of', 'uther', 'pendragon',\\\n",
    "                            'from', 'the', 'castle', 'of', 'camelot', 'king', 'of', 'the',\\\n",
    "                            'britons', 'defeator', 'of', 'the', 'saxons', 'sovereign', 'of', 'all', 'england'])\n",
    "\n",
    "pirates= tokenize(webtext, 'pirates.txt')\n",
    "assert_is_instance(pirates, list)\n",
    "assert_equal(len(pirates), 17153)\n",
    "\n",
    "assert_true(all(isinstance(w, str) for w in pirates))\n",
    "assert_true(all(all(not c.isupper() for c in w) for w in pirates))\n",
    "assert_true(all(any(c.isalnum() for c in w) for w in pirates))\n",
    "\n",
    "assert_equal(pirates[100:110], ['the', 'left', 'in', 'the', 'barn', 'where', 'the', 'marines', 'enter', 'liz'])\n",
    "assert_equal(pirates[-10:], ['left', 'shoulder', 'faces', 'the', 'camera', 'and', 'snarls', 'scene', 'end', 'credits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count words\n",
    "Here, we want to find the number of tokens, the number of words and lexical diversity of one of the list of strings found with the previous function definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "60b94ef2362c81571e02b8303e2daf91",
     "grade": false,
     "grade_id": "pivot",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def count_words(word_ls):\n",
    "    '''\n",
    "    Computes the the number of token, number of words, and lexical diversity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word_ls: A list of of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 3-tuple of (num_tokens, num_words, lex_div) called tup\n",
    "    num_tokens: An int. The number of tokens in \"words\".\n",
    "    num_words: An int. The number of words in \"words\".\n",
    "    lex_div: A float. The lexical diversity of \"words\".\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    counts = nltk.FreqDist(word_ls)\n",
    "    \n",
    "    num_tokens = len(counts)\n",
    "    num_words = len(word_ls)\n",
    "    lex_div = num_words / num_tokens\n",
    "    tup = num_tokens, num_words, lex_div\n",
    "    \n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a3d94817527fcc5dbeb0d2e55a511f90",
     "grade": true,
     "grade_id": "pivot_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "monty_tokens, monty_words, mld = count_words(monty)\n",
    "assert_is_instance(monty_tokens, int)\n",
    "assert_is_instance(monty_words, int)\n",
    "assert_is_instance(mld, float)\n",
    "assert_equal(monty_tokens, 1823)\n",
    "assert_equal(monty_words, 11608)\n",
    "assert_almost_equal(mld, 6.3675260559517275)\n",
    "\n",
    "pirate_tokens, pirate_words, pld = count_words(pirates)\n",
    "assert_is_instance(pirate_tokens, int)\n",
    "assert_is_instance(pirate_words, int)\n",
    "assert_is_instance(pld, float)\n",
    "assert_equal(pirate_tokens, 2731)\n",
    "assert_equal(pirate_words, 17153)\n",
    "assert_almost_equal(pld, 6.280849505675577)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common words\n",
    "Now that we have tokens, we can find the most common words used in each list of strings found with `tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b9c559eea976d680efebea2e38abaeae",
     "grade": false,
     "grade_id": "similar_user",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def most_common(words, num_top_words):\n",
    "    '''\n",
    "    Takes the output of tokenize and find the most common words within that list,\n",
    "    returning a list of tuples containing the most common words and their number \n",
    "    of occurances.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: A list of strings\n",
    "    num_top_words:  An int. The number of most common words (and tuples) \n",
    "                    that will be returned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    top_words:  A list of tuples, where each tuple contains a word and\n",
    "                its number of occurances.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    counts = nltk.FreqDist(words)\n",
    "    top_words = counts.most_common(num_top_words)\n",
    "    \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "ccdee79429b6fb1fd79d6ae165c095aa",
     "grade": true,
     "grade_id": "sim_user_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "yarr = most_common(pirates, 5)\n",
    "\n",
    "assert_is_instance(yarr, list)\n",
    "assert_true(all(isinstance(t, tuple) for t in yarr))\n",
    "assert_true(all(isinstance(t, str) for t, f in yarr))\n",
    "assert_true(all(isinstance(f, int) for t, f in yarr))\n",
    "\n",
    "assert_equal(len(most_common(pirates, 10)), 10)\n",
    "assert_equal(len(most_common(pirates, 20)), 20)\n",
    "assert_equal(yarr, [('the', 1073), ('jack', 470), ('a', 434), ('to', 372), ('of', 285)])\n",
    "\n",
    "shrubbery = most_common(monty, 5)\n",
    "assert_is_instance(shrubbery, list)\n",
    "assert_true(all(isinstance(t, tuple) for t in shrubbery))\n",
    "assert_true(all(isinstance(t, str) for t, f in shrubbery))\n",
    "assert_true(all(isinstance(f, int) for t, f in shrubbery))\n",
    "\n",
    "assert_equal(len(most_common(monty, 15)), 15)\n",
    "assert_equal(len(most_common(monty, 37)), 37)\n",
    "assert_equal(shrubbery, [('the', 334), ('you', 265), ('arthur', 261), ('i', 260), ('a', 238)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hapaxes\n",
    "Now, we find the hapaxes from the list of strings we made with `tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "499353ee10e29606bd5dec0903d0f70d",
     "grade": false,
     "grade_id": "find_books",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def hapax(words):\n",
    "    '''\n",
    "    Finds all hapaxes from the \"words\" list of strings.    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: A list of strings\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    hapax: A list of strings\n",
    "    \n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    counts = nltk.FreqDist(words)\n",
    "    hapax = counts.hapaxes()\n",
    "    \n",
    "    return hapax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "ce0aa8396015d4e9a0e03ef4a6b7e9de",
     "grade": true,
     "grade_id": "list_check",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(hapax(monty), list)\n",
    "assert_true(all(isinstance(w, str) for w in hapax(monty)))\n",
    "assert_equal(len(hapax(monty)), 977)\n",
    "assert_equal(sorted(hapax(monty))[-5:],['zhiv', 'zone', 'zoo', 'zoop', 'zoosh'])\n",
    "\n",
    "assert_is_instance(hapax(pirates), list)\n",
    "assert_true(all(isinstance(w, str) for w in hapax(pirates)))\n",
    "assert_equal(len(hapax(pirates)), 1433)\n",
    "assert_equal(sorted(hapax(pirates))[-5:],['yeah', 'yep', 'yours', 'yourselves', 'zooming'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long words\n",
    "\n",
    "Finally, we find the words within the output of `tokenize` longer than a given length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "b55a90a6af2daf63c4b19ef45343327f",
     "grade": false,
     "grade_id": "long_words",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def long_words(words, length=10):\n",
    "    '''\n",
    "    Finds all words in \"words\" longer than \"length\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An list of strings.\n",
    "    length: An int. Default: 10\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    long_words = [word for word in words if len(word) > length]\n",
    "    \n",
    "    return long_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0f3d005f0b8bfdf5792fa5a105d0cf04",
     "grade": true,
     "grade_id": "long_words_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "monty_l = long_words(monty, 12)\n",
    "assert_is_instance(monty_l, list)\n",
    "assert_true(all(isinstance(w, str) for w in monty_l))    \n",
    "assert_equal(len(monty_l), 6)\n",
    "assert_equal(\n",
    "    set(monty_l),\n",
    "    set(['unfortunately', 'understanding', 'oooohoohohooo', 'indefatigable', 'camaaaaaargue', 'automatically'])\n",
    "    )\n",
    "assert_equal(len(long_words(monty,10)), 68)\n",
    "assert_equal(len(long_words(monty,11)), 37)\n",
    "\n",
    "\n",
    "pirate_l = long_words(pirates, 13)\n",
    "assert_is_instance(pirate_l, list)\n",
    "assert_true(all(isinstance(w, str) for w in monty_l))    \n",
    "assert_equal(len(pirate_l), 5)\n",
    "assert_equal(\n",
    "    set(pirate_l),\n",
    "    set(['simultanenously', 'responsibility', 'reconciliatory', 'incapacitorially', 'enthusiastically']))\n",
    "assert_equal(len(long_words(pirates,10)), 107)\n",
    "assert_equal(len(long_words(pirates,12)), 29)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
