---
title: "Lab: Trees, Random Forests, Boosting"
author: "STAT 430, Spring 2017"
date: ''
output:
  html_document: default
---

For this lab you will need the following packages:

```{r, message = FALSE, warning = FALSE}
library(ISLR)
library(tree)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(caret)
library(plyr)
```

You will investigate the use of trees, random forests, and boosting on the `OJ` data from the `ISLR` package. The categorical response is `Purchase`, while all other variables are used as predictors. Below we create a test-train split of the data.

```{r}
set.seed(42)
oj_idx = createDataPartition(OJ$Purchase, p = 0.5, list = FALSE)
oj_trn = OJ[oj_idx,]
oj_tst = OJ[-oj_idx,]
```

Since we are performing classification, we will calculate several accuracies.

```{r}
accuracy = function(actual, predicted) {
  mean(actual == predicted)
}
```

> Fit a tree to the training data using the `tree()` function, with `Purchase` as the response and the other variables as predictors. Use the  `summary()` function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have? What variables are used?

```{r}
oj_tree = tree(Purchase ~ ., data = oj_trn)
summary(oj_tree)
```

This tree has **9** terminal nodes and a training error rate of 0.1437. It contains `LoyalCH`, `PriceDiff`, `StoreID` and `ListPriceDiff`.

> Type in the name of the tree object in order to get a detailed text output. Pick one of the **terminal nodes**, and interpret the information displayed.

```{r}
oj_tree
```

Line with stars are terminal nodes. 13 not pure nodes.

> Create a plot of the tree you fit. Interpret the results.

```{r}
plot(oj_tree)
text(oj_tree, pretty = 0)
title(main = "Unpruned Classification Tree")
```

> Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test accuracy?

```{r}
oj_tst_pred = predict(oj_tree, oj_tst, type = "class")

# confusion matrix
table(predicted = oj_tst_pred, actual = oj_tst$Purchase)

# test accuracy
accuracy(predicted = oj_tst_pred, actual = oj_tst$Purchase)
```

The code below trains a single tree using 5-fold cross-validation. (It will consider three different complexity parameters.) Also, instead of `tree()`, we use `rpart()` together with the `caret` package.

```{r}
cv_5 = trainControl(method = "cv", number = 5)
oj_tree_cv  = train(Purchase ~ .,
                    data = oj_trn,
                    trControl = cv_5,
                    method = "rpart") 
```

We also create a plot of the resulting tree.

```{r}
plot(oj_tree_cv)
prp(oj_tree_cv$finalModel)
```

> Predict the response on the test data using this new tree. What is the test accuracy? Does this perform better than the previous (unpruned) tree? Is this tree smaller or larger?

```{r}
oj_tst_cv_pred = predict(oj_tree_cv, newdata = oj_tst)

# test accuracy
accuracy(predicted = oj_tst_cv_pred, actual = oj_tst$Purchase)
```
Compared to the results of unpruned tree gives, this one gives higher test accuracy which is `r round(accuracy(predicted = oj_tst_cv_pred, actual = oj_tst$Purchase), 4)`.

> Now we'll consider ensemble methods. The following defines OOB resampling for use when tuning a random forest.

```{r}
oob = trainControl(method = "oob")
```

> Tune a random forest using OOB resampling and **all** possible values of `mtry`. What value of `mtry` is found to be the best?

`mtry` can take all possible values of number of predictors.

```{r}
# your code here
rf_grid = expand.grid(mtry = c(1: (ncol(oj_trn)-1)))

oj_rf_oob  = train(
    Purchase ~ .,
    data = oj_trn,
    trControl = oob,
    method = "rf",
    tuneGrid = rf_grid)

oj_rf_oob$bestTune
```

> Report the OOB classification accuracy for a bagged tree model.

```{r}
# your code here
oj_rf_oob$results[ncol(oj_trn) - 1, ]
```

OOB classification accuracy for a bagged tree model is `r round(oj_rf_oob$results$Accuracy[17], 4)`.

> Create a variable importance plot for the random forest you tuned.

```{r}
######
# your code here
varImpPlot(oj_rf_oob$finalModel)
```

> Report the test accuracy for the tuned random forest.

```{r}
accuracy(predict(oj_rf_oob, newdata = oj_tst), oj_tst$Purchase)
```

> Use the following tuning grid to tune a boosted tree model using 5-fold cross-validation.

```{r}
gbm_grid_small = expand.grid(interaction.depth = c(1, 2),
                             n.trees = c(500, 1000, 1500),
                             shrinkage = c(0.01, 0.1),
                             n.minobsinnode = 10)
```

```{r}
oj_gbm_small_cv = train(
  Purchase ~ ., data = oj_trn,
  trControl = cv_5,
  method = "gbm",
  verbose = FALSE,
  tuneGrid = gbm_grid_small
)
```

> Report the **two** most important variables according to the tuned boosted model. (Attempt to suppress the plot that is usually created.) 

```{r}
# two most important variables
summary(oj_gbm_small_cv, plotit = FALSE)[1:2, ]
```

> What were the tuning parameters of the chosen boosted model?

```{r}
oj_gbm_small_cv$bestTune
```

> Calculate the test accuracy for the chosen boosted model.

```{r}
accuracy(predict(oj_gbm_small_cv, oj_tst), oj_tst$Purchase)
```

> Create a larger tuning grid and fit a new boosted model. Report the tuning parameters and new test accuracy. Is this new model better? Is this model different?

```{r}
gbm_grid_large = expand.grid(interaction.depth = c(1, 2, 3),
                             n.trees = c(500, 1000, 1500),
                             shrinkage = c(0.001, 0.01, 0.1),
                             n.minobsinnode = 10)

oj_gbm_large_cv  = train(Purchase ~ .,
                         data = oj_trn,
                         trControl = cv_5,
                         method = "gbm",
                         verbose = FALSE,
                         tuneGrid = gbm_grid_large)
oj_gbm_large_cv$bestTune

# test accuracy
accuracy(predict(oj_gbm_large_cv, oj_tst), oj_tst$Purchase)
```

> Of all the models you have fit, which is best?

Almost same.